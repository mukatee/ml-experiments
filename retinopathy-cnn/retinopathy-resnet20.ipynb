{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TBD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_2015 = 1\n",
    "epochs_2015 = 3\n",
    "warmup_2019 = 1\n",
    "epochs_2019 = 5\n",
    "lr_decay = 0\n",
    "warmup_enabled = True\n",
    "batch_size = 32\n",
    "model_img_size = 448\n",
    "tta = True\n",
    "n_classes = 5\n",
    "resnet_depth = 20\n",
    "resnet_version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers, applications\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import backend as K \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxr-xr-x 4 root root 4096 Jun 28 02:06 aptos2019-blindness-detection\r\n",
      "drwxr-xr-x 3 root root 4096 Jul  1 02:43 retinopathy-train-2015\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of files in train vs test vs the 2015 training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3663\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../input/aptos2019-blindness-detection/train_images | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../input/aptos2019-blindness-detection/test_images | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35127\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../input/retinopathy-train-2015/rescaled_train_896/rescaled_train_896 | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_2015 = \"../input/retinopathy-train-2015/rescaled_train_896/rescaled_train_896/\"\n",
    "train_path_2019 = \"../input/aptos2019-blindness-detection/train_images/\"\n",
    "test_path_2019 = \"../input/aptos2019-blindness-detection/test_images/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  000c1434d8d7          2\n",
       "1  001639a390f0          4\n",
       "2  0024cdab0c1e          1\n",
       "3  002c21358ce6          0\n",
       "4  005b95c28852          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_2019 = pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\n",
    "df_train_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code\n",
       "0  0005cfc8afb6\n",
       "1  003f0afdcd15\n",
       "2  006efc72b638\n",
       "3  00836aaacf06\n",
       "4  009245722fa4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_2019 = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "df_test_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image  level\n",
       "0   10_left      0\n",
       "1  10_right      0\n",
       "2   13_left      0\n",
       "3  13_right      0\n",
       "4   15_left      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_2015 = pd.read_csv(\"../input/retinopathy-train-2015/rescaled_train_896/trainLabels.csv\")\n",
    "df_train_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all metadata to single dataframe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows_2019 = df_train_2019.shape[0]\n",
    "n_rows_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "      <td>000c1434d8d7.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "      <td>001639a390f0.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "      <td>0024cdab0c1e.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "      <td>002c21358ce6.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "      <td>005b95c28852.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  ...   year\n",
       "0  000c1434d8d7  ...   2019\n",
       "1  001639a390f0  ...   2019\n",
       "2  0024cdab0c1e  ...   2019\n",
       "3  002c21358ce6  ...   2019\n",
       "4  005b95c28852  ...   2019\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_2019[\"filename\"] = df_train_2019[\"id_code\"]+\".png\"\n",
    "df_train_2019[\"path\"] = [train_path_2019]*n_rows_2019\n",
    "#the year is just to be able to easily separate the past and present datasets later\n",
    "df_train_2019[\"year\"] = [2019]*n_rows_2019\n",
    "df_train_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35126"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows_2015 = df_train_2015.shape[0]\n",
    "n_rows_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>level</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>0</td>\n",
       "      <td>10_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>0</td>\n",
       "      <td>10_right.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>0</td>\n",
       "      <td>13_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>0</td>\n",
       "      <td>13_right.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_left</td>\n",
       "      <td>1</td>\n",
       "      <td>15_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image  ...   year\n",
       "0   10_left  ...   2015\n",
       "1  10_right  ...   2015\n",
       "2   13_left  ...   2015\n",
       "3  13_right  ...   2015\n",
       "4   15_left  ...   2015\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_2015[\"filename\"] = df_train_2015[\"image\"]+\".png\"\n",
    "df_train_2015[\"path\"] = [train_path_2015]*n_rows_2015\n",
    "df_train_2015[\"year\"] = [2015]*n_rows_2015\n",
    "df_train_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows_test = df_test_2019.shape[0]\n",
    "n_rows_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_code'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0005cfc8afb6.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>-1</td>\n",
       "      <td>003f0afdcd15.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>-1</td>\n",
       "      <td>006efc72b638.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>-1</td>\n",
       "      <td>00836aaacf06.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>-1</td>\n",
       "      <td>009245722fa4.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  ...   year\n",
       "0  0005cfc8afb6  ...   2050\n",
       "1  003f0afdcd15  ...   2050\n",
       "2  006efc72b638  ...   2050\n",
       "3  00836aaacf06  ...   2050\n",
       "4  009245722fa4  ...   2050\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_2019[\"filename\"] = df_test_2019[\"id_code\"]+\".png\"\n",
    "df_test_2019[\"path\"] = [test_path_2019]*n_rows_test\n",
    "df_test_2019[\"year\"] = [2050]*n_rows_test\n",
    "df_test_2019[\"diagnosis\"] = -1\n",
    "df_test_2019 = df_test_2019[[\"id_code\", \"diagnosis\", \"filename\", \"path\", \"year\"]]\n",
    "df_test_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>0</td>\n",
       "      <td>10_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>0</td>\n",
       "      <td>10_right.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>0</td>\n",
       "      <td>13_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>0</td>\n",
       "      <td>13_right.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_left</td>\n",
       "      <td>1</td>\n",
       "      <td>15_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_code  ...   year\n",
       "0   10_left  ...   2015\n",
       "1  10_right  ...   2015\n",
       "2   13_left  ...   2015\n",
       "3  13_right  ...   2015\n",
       "4   15_left  ...   2015\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_2015.columns = [\"id_code\", \"diagnosis\", \"filename\", \"path\", \"year\"]\n",
    "df_train_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "      <td>000c1434d8d7.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "      <td>001639a390f0.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "      <td>0024cdab0c1e.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "      <td>002c21358ce6.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "      <td>005b95c28852.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ...   year\n",
       "0      0  ...   2019\n",
       "1      1  ...   2019\n",
       "2      2  ...   2019\n",
       "3      3  ...   2019\n",
       "4      4  ...   2019\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all = pd.concat([df_train_2019,df_train_2015, df_test_2019], axis=0, sort=False).reset_index()\n",
    "df_train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40711</th>\n",
       "      <td>1923</td>\n",
       "      <td>ff2fd94448de</td>\n",
       "      <td>-1</td>\n",
       "      <td>ff2fd94448de.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40712</th>\n",
       "      <td>1924</td>\n",
       "      <td>ff4c945d9b17</td>\n",
       "      <td>-1</td>\n",
       "      <td>ff4c945d9b17.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40713</th>\n",
       "      <td>1925</td>\n",
       "      <td>ff64897ac0d8</td>\n",
       "      <td>-1</td>\n",
       "      <td>ff64897ac0d8.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40714</th>\n",
       "      <td>1926</td>\n",
       "      <td>ffa73465b705</td>\n",
       "      <td>-1</td>\n",
       "      <td>ffa73465b705.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40715</th>\n",
       "      <td>1927</td>\n",
       "      <td>ffdc2152d455</td>\n",
       "      <td>-1</td>\n",
       "      <td>ffdc2152d455.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/test_im...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  ...   year\n",
       "40711   1923  ...   2050\n",
       "40712   1924  ...   2050\n",
       "40713   1925  ...   2050\n",
       "40714   1926  ...   2050\n",
       "40715   1927  ...   2050\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing df_train with the full set to calculate features and do visualizations all at once, keeping the original (present) just in case\n",
    "df_train = df_train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Aspect Ratios etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3327b2f768f1474293620bcbc7068ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40716), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 16.2 s, sys: 16.3 s, total: 32.6 s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img_sizes = []\n",
    "widths = []\n",
    "heights = []\n",
    "aspect_ratios = []\n",
    "\n",
    "for index, row in tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "    filename = row[\"filename\"]\n",
    "    path = row[\"path\"]\n",
    "    img_path = os.path.join(path, filename)\n",
    "    with open(img_path, 'rb') as f:\n",
    "        img = PIL.Image.open(f)\n",
    "        img_size = img.size\n",
    "        img_sizes.append(img_size)\n",
    "        widths.append(img_size[0])\n",
    "        heights.append(img_size[1])\n",
    "        aspect_ratios.append(img_size[0]/img_size[1])\n",
    "\n",
    "df_train[\"width\"] = widths\n",
    "df_train[\"height\"] = heights\n",
    "df_train[\"aspect_ratio\"] = aspect_ratios\n",
    "df_train[\"size\"] = img_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "      <td>000c1434d8d7.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "      <td>3216</td>\n",
       "      <td>2136</td>\n",
       "      <td>1.505618</td>\n",
       "      <td>(3216, 2136)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "      <td>001639a390f0.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "      <td>3216</td>\n",
       "      <td>2136</td>\n",
       "      <td>1.505618</td>\n",
       "      <td>(3216, 2136)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "      <td>0024cdab0c1e.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2416</td>\n",
       "      <td>1736</td>\n",
       "      <td>1.391705</td>\n",
       "      <td>(2416, 1736)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "      <td>002c21358ce6.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(1050, 1050)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "      <td>005b95c28852.png</td>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>(2048, 1536)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       id_code      ...       aspect_ratio          size\n",
       "0      0  000c1434d8d7      ...           1.505618  (3216, 2136)\n",
       "1      1  001639a390f0      ...           1.505618  (3216, 2136)\n",
       "2      2  0024cdab0c1e      ...           1.391705  (2416, 1736)\n",
       "3      3  002c21358ce6      ...           1.000000  (1050, 1050)\n",
       "4      4  005b95c28852      ...           1.333333  (2048, 1536)\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2019 = df_train[df_train[\"year\"] == 2019]\n",
    "df_train_2015 = df_train[df_train[\"year\"] == 2015]\n",
    "df_test = df_train[df_train[\"year\"] == 2050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This just shows a single image in the notebook\n",
    "def show_img(filename, path):\n",
    "        img = PIL.Image.open(f\"{path}/{filename}\")\n",
    "        npa = np.array(img)\n",
    "        print(npa.shape)\n",
    "        #https://stackoverflow.com/questions/35902302/discarding-alpha-channel-from-images-stored-as-numpy-arrays\n",
    "#        npa3 = npa[ :, :, :3]\n",
    "        print(filename)\n",
    "        plt.imshow(npa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ben_color(data, img_size, sigmaX=10):\n",
    "    if data.ndim == 4:  # array of images\n",
    "        for i in range(len(data)):\n",
    "            image = cv2.cvtColor(data[i], cv2.COLOR_BGR2RGB)\n",
    "            image = crop_image_from_gray(image)\n",
    "            image = cv2.resize(image, (img_size, img_size))\n",
    "            data[i] = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX), -4 ,128)\n",
    "    elif data.ndim == 3:  # just a single image\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n",
    "        data = crop_image_from_gray(data)\n",
    "        data = cv2.resize(data, (img_size, img_size))\n",
    "        data = cv2.addWeighted(data, 4, cv2.GaussianBlur(data, (0,0), sigmaX), -4 , 128)\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(data, img_size, sigmaX=10):\n",
    "    # cropping & Ben Graham's preprocessing method\n",
    "    data = load_ben_color(data, img_size, sigmaX)\n",
    "    \n",
    "    # normalization (rescaling between 0 and 1)\n",
    "    data = data.astype('float32')\n",
    "    for i in range(len(data)):\n",
    "        cv2.normalize(data[i],  data[i], 0, 1, cv2.NORM_MINMAX)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "def img_augment(img):\n",
    "    img = load_ben_color(img, model_img_size, 10)\n",
    "    #fifty_chance = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    import random\n",
    "    if random.randint(1,101) > 90:\n",
    "        #print(\"original\")\n",
    "        #one in 10 return original image\n",
    "        return img\n",
    "    seq = iaa.SomeOf(3, [\n",
    "        iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "        #iaa.GaussianBlur(sigma=(0, 1.0)), # blur images with a sigma of 0 to 1.0 TODO: test for good values\n",
    "        iaa.Affine(\n",
    "            #scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "            scale=(0.8, 1.2),\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "        iaa.Affine(\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": 0}, # translate by -20 to +20 percent (per axis)\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "        iaa.Affine(\n",
    "            rotate=(-10, 10), # rotate by -10 to +10 degrees\n",
    "            #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "        iaa.Affine(\n",
    "            shear=(-5, 5), # shear by -5 to +5 degrees\n",
    "            #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "    ])\n",
    "    img = seq.augment_image(img)\n",
    "    return img\n",
    "\n",
    "#https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up\n",
    "#https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "import random\n",
    "\n",
    "def img_pad_resize(df, filename, input_path, augment):\n",
    "    img_meta = df[df[\"filename\"] == filename].iloc[0]\n",
    "    #print(img_meta)\n",
    "    img = PIL.Image.open(f'{input_path}/{filename}')\n",
    "\n",
    "    cropped = False\n",
    "    if random.randint(1,101) > 50:\n",
    "        aspect_ratio = img_meta[\"aspect_ratio\"]\n",
    "        if aspect_ratio > 2 or aspect_ratio < 0.5:\n",
    "            img_w = img_meta[\"width\"]\n",
    "            img_h = img_meta[\"height\"]\n",
    "            diff = abs(img_w-img_h)\n",
    "            if img_w > img_h:\n",
    "                shorter = \"height\"\n",
    "                larger = \"width\"\n",
    "                crop_y = 0\n",
    "                crop_x = random.randint(0, diff)\n",
    "            else:\n",
    "                shorter = \"width\"\n",
    "                larger = \"height\"\n",
    "                crop_x = 0\n",
    "                crop_y = random.randint(0, diff)\n",
    "            crop_size = img_meta[shorter]\n",
    "            img_cropped = img.crop((crop_x, crop_y, crop_x+crop_size, crop_y+crop_size))\n",
    "            img.close()\n",
    "            img = img_cropped\n",
    "            cropped = True\n",
    "            #print(\"cropped:\"+filename)\n",
    "       \n",
    "    w = img.size[0]\n",
    "    h = img.size[1]\n",
    "    pad_size = np.abs(h-w)\n",
    "    wm = hm = 1\n",
    "    pw = ph = 0\n",
    "    if w < h:\n",
    "        wm = h / w\n",
    "        pw = pad_size / 2\n",
    "    else:\n",
    "        hm = w / h\n",
    "        ph = pad_size / 2\n",
    "    w *= wm\n",
    "    h *= hm\n",
    "    h = int(h)\n",
    "    w = int(w)\n",
    "    pw = int(pw)\n",
    "    ph = int(ph)\n",
    "    padding = (pw, ph, pw, ph)\n",
    "    padded = ImageOps.expand(img, padding)\n",
    "    resized = padded.resize((model_img_size, model_img_size))\n",
    "    np_img = np.array(resized)\n",
    "    img.close()\n",
    "    padded.close()\n",
    "    del img\n",
    "    del padded\n",
    "    if augment:\n",
    "        np_img = img_augment(np_img)\n",
    "    #np_img = resnet50.preprocess_input(np_img)\n",
    "    return np_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a.iloc[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras.utils import Sequence\n",
    "\n",
    "#https://github.com/sdcubber/Keras-Sequence-boilerplate/blob/master/Keras-Sequence.ipynb\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class MySequence(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, mode=\"train\", augment=True):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.max_idx = math.ceil(len(x_set)/batch_size)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx2 = idx % self.max_idx\n",
    "        #i guess the len method above stops the generator from being called too much\n",
    "        #print(f\"idx:{idx}, idx2:{idx2}\")\n",
    "        start = idx * self.batch_size\n",
    "        end = min(start + batch_size, len(self.x))\n",
    "        batch_x = self.x.iloc[start : end]\n",
    "        batch_y = self.y[start : end]\n",
    "        \n",
    "        next_batch = []\n",
    "        for index, row in batch_x.iterrows():\n",
    "            file_name = row[\"filename\"]\n",
    "            file_path = row[\"path\"]\n",
    "            padded = img_pad_resize(self.x, file_name, file_path, self.augment)\n",
    "            if padded.shape != (model_img_size, model_img_size, 3):\n",
    "                print(f\"shape mismatch {file_name}, {file_path}, {padded.shape}\")\n",
    "                print()\n",
    "                #the image has alpha channel, drop it\n",
    "                padded = padded[ :, :, :3]\n",
    "            \n",
    "            next_batch.append(padded)\n",
    "        np_y = np.array(batch_y)\n",
    "        result = np.array(next_batch), np_y\n",
    "\n",
    "        #print(result[0].shape)\n",
    "        return result\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.x, self.y = unison_shuffled_copies(self.x, self.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>year</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>0</td>\n",
       "      <td>10_left</td>\n",
       "      <td>0</td>\n",
       "      <td>10_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "      <td>896</td>\n",
       "      <td>597</td>\n",
       "      <td>1.500838</td>\n",
       "      <td>(896, 597)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>1</td>\n",
       "      <td>10_right</td>\n",
       "      <td>0</td>\n",
       "      <td>10_right.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "      <td>896</td>\n",
       "      <td>597</td>\n",
       "      <td>1.500838</td>\n",
       "      <td>(896, 597)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>2</td>\n",
       "      <td>13_left</td>\n",
       "      <td>0</td>\n",
       "      <td>13_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "      <td>896</td>\n",
       "      <td>672</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>(896, 672)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>3</td>\n",
       "      <td>13_right</td>\n",
       "      <td>0</td>\n",
       "      <td>13_right.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "      <td>896</td>\n",
       "      <td>672</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>(896, 672)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>4</td>\n",
       "      <td>15_left</td>\n",
       "      <td>1</td>\n",
       "      <td>15_left.png</td>\n",
       "      <td>../input/retinopathy-train-2015/rescaled_train...</td>\n",
       "      <td>2015</td>\n",
       "      <td>896</td>\n",
       "      <td>593</td>\n",
       "      <td>1.510961</td>\n",
       "      <td>(896, 593)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index   id_code  diagnosis     ...     height aspect_ratio        size\n",
       "3662      0   10_left          0     ...        597     1.500838  (896, 597)\n",
       "3663      1  10_right          0     ...        597     1.500838  (896, 597)\n",
       "3664      2   13_left          0     ...        672     1.333333  (896, 672)\n",
       "3665      3  13_right          0     ...        672     1.333333  (896, 672)\n",
       "3666      4   15_left          1     ...        593     1.510961  (896, 593)\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2015 = df_train_2015[\"diagnosis\"]\n",
    "y_2019 = df_train_2019[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2015 = np.zeros((n_rows_2015, n_classes))\n",
    "idx = 0\n",
    "for diagnosis in df_train_2015[\"diagnosis\"]:\n",
    "    y_2015[idx][int(diagnosis)] = 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2019 = np.zeros((n_rows_2019, n_classes))\n",
    "idx = 0\n",
    "for diagnosis in df_train_2019[\"diagnosis\"]:\n",
    "    y_2019[idx][int(diagnosis)] = 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_generators(df_train, y):\n",
    "    indexes = np.arange(df_train.shape[0])\n",
    "    train_indices, valid_indices = train_test_split(indexes, test_size=0.10, random_state=8, stratify=y)\n",
    "    train_sub_df = df_train.iloc[train_indices]\n",
    "    train_sub_y = y[train_indices]\n",
    "    valid_sub_df = df_train.iloc[valid_indices]\n",
    "    valid_sub_y = y[valid_indices]\n",
    "    warmup_gen = MySequence(train_sub_df, train_sub_y, batch_size, augment=False)\n",
    "    train_gen = MySequence(train_sub_df, train_sub_y, batch_size, augment=True)\n",
    "    valid_gen = MySequence(valid_sub_df, valid_sub_y, batch_size, augment=False)\n",
    "    return train_indices, valid_indices, warmup_gen, train_gen, valid_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_2015, valid_indices_2015, warmup_gen_2015, train_gen_2015, valid_gen_2015 = create_generators(df_train_2015, y_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_2019, valid_indices_2019, warmup_gen_2019, train_gen_2019, valid_gen_2019 = create_generators(df_train_2019, y_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up\n",
    "indexes = np.arange(df_train_2015.shape[0])\n",
    "train_indices, valid_indices = train_test_split(indexes, test_size=0.10, random_state=8, stratify=y_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_sub_df = df_train_2015.iloc[train_indices]\n",
    "train_sub_y = y_2015[train_indices]\n",
    "valid_sub_df = df_train_2015.iloc[valid_indices]\n",
    "valid_sub_y = y_2015[valid_indices]\n",
    "#train_gen = MySequence(train_sub_df, train_sub_df[\"breed_label\"], batch_size)\n",
    "#valid_gen = MySequence(valid_sub_df, valid_sub_df[\"breed_label\"], batch_size)\n",
    "warmup_gen_2015 = MySequence(train_sub_df, train_sub_y, batch_size, augment=False)\n",
    "#TODO: remove probability of no augment from generator?\n",
    "train_gen_2015 = MySequence(train_sub_df, train_sub_y, batch_size, augment=True)\n",
    "valid_gen_2015 = MySequence(valid_sub_df, valid_sub_y, batch_size, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "                             \n",
    "checkpoint = ModelCheckpoint('../working/Resnet20_best_{epoch:03d}_{val_loss:.2f}.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3,\n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=7)\n",
    "\n",
    "csv_logger = CSVLogger(filename='../working/training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen_batch(idx):\n",
    "    # configure batch size and retrieve one batch of images\n",
    "    plt.clf() #clears matplotlib data and axes\n",
    "    #for batch in train_generator:\n",
    "    rows = (batch_size / 3)+1\n",
    "    plt.figure(figsize=[30,10*rows])\n",
    "    batch = train_gen_2019.__getitem__(idx)\n",
    "    for x in range(0,batch_size-1):\n",
    "    #    print(train_generator.filenames[x])\n",
    "        plt.subplot(rows, 3, x+1)\n",
    "        plt.imshow(batch[0][x], interpolation='nearest')\n",
    "\n",
    "        diagnosis = batch[1][x]\n",
    "        print(diagnosis)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_gen_batch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, AveragePooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "def create_model(trainable_layer_count):\n",
    "    input_tensor = Input(shape=(model_img_size, model_img_size, 3))\n",
    "    base_model = ResNet50(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor)\n",
    "    if load_weights:\n",
    "        base_model.load_weights('../input/retinopathy_weights/resnet50_best.h5')\n",
    "\n",
    "#    base_model = ResNet50(include_top=False,\n",
    "#                          #the weights value can apparently also be a file path..\n",
    "#                   weights=None, #loading weights from dataset, avoiding need for internet conn\n",
    "#                   input_tensor=input_tensor)\n",
    "#    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    if trainable_layer_count == \"all\":\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-trainable_layer_count:]:\n",
    "            layer.trainable = True\n",
    "    print(\"base model has {} layers\".format(len(base_model.layers)))\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu')(base_model.output)\n",
    "#     x = Flatten()(x)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(5e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #predict individual probability of each category\n",
    "    final_output = Dense(n_classes, activation='softmax', name='final_output')(x)\n",
    "#    final_output = Dense(n_classes, activation='sigmoid', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_model(train_depth)\n",
    "input_shape = (model_img_size, model_img_size, 3)\n",
    "depth = resnet_depth\n",
    "if resnet_version == 1:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth, num_classes=5)\n",
    "else:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth, num_classes=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "#f2_score = my_f2\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=[\"accuracy\"]) #loss=focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gombru.github.io/2018/05/23/cross_entropy_loss/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "988/987 [==============================] - 962s 973ms/step - loss: 4.4130 - acc: 0.7336\n"
     ]
    }
   ],
   "source": [
    "if warmup_enabled:\n",
    "    #train_steps = 2*len(train_indices)/batch_size\n",
    "    #valid_steps = 2*len(valid_indices)/batch_size\n",
    "    train_steps = len(train_indices_2015)/batch_size\n",
    "    valid_steps = len(valid_indices_2015)/batch_size\n",
    "    fit_history = model.fit_generator(\n",
    "            warmup_gen_2015,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs = warmup_2015,\n",
    "            use_multiprocessing=True,\n",
    "            workers=2,\n",
    "            callbacks=[], #no callbacks for warmup\n",
    "        verbose = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "#f2_score = my_f2\n",
    "#https://datascience.stackexchange.com/questions/26112/decay-parameter-in-keras-optimizers\n",
    "if lr_decay > 0:\n",
    "    model.compile(optimizer=Adam(lr=0.0001, decay=lr_decay), loss='categorical_crossentropy', metrics=[\"accuracy\"]) #loss=focal_loss\n",
    "else:\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=[\"accuracy\"]) #loss=focal_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "987/987 [============================>.] - ETA: 1s - loss: 4.3243 - acc: 0.7346Epoch 1/3\n",
      "988/987 [==============================] - 1766s 2s/step - loss: 4.3220 - acc: 0.7348 - val_loss: 4.2949 - val_acc: 0.7347\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.29488, saving model to ../working/Resnet20_best_001_4.29.h5\n",
      "Epoch 2/3\n",
      "988/987 [==============================] - 1711s 2s/step - loss: 4.2856 - acc: 0.7348 - val_loss: 4.2825 - val_acc: 0.7347\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.29488 to 4.28247, saving model to ../working/Resnet20_best_002_4.28.h5\n",
      "Epoch 3/3\n",
      "988/987 [==============================] - 1693s 2s/step - loss: 1.5509 - acc: 0.7259 - val_loss: 0.9381 - val_acc: 0.7347\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.28247 to 0.93809, saving model to ../working/Resnet20_best_003_0.94.h5\n"
     ]
    }
   ],
   "source": [
    "#train_steps = 2*len(train_indices)/batch_size\n",
    "#valid_steps = 2*len(valid_indices)/batch_size\n",
    "train_steps = len(train_indices_2015)/batch_size\n",
    "valid_steps = len(valid_indices_2015)/batch_size\n",
    "fit_history = model.fit_generator(\n",
    "        train_gen_2015,\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs = epochs_2015,\n",
    "        validation_data=valid_gen_2015,\n",
    "        validation_steps=valid_steps,\n",
    "        use_multiprocessing=True,\n",
    "        workers=2,\n",
    "        callbacks=callbacks_list,\n",
    "    verbose = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=[\"accuracy\"]) #loss=focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "103/102 [==============================] - 304s 3s/step - loss: 1.2658 - acc: 0.6237\n"
     ]
    }
   ],
   "source": [
    "if warmup_enabled:\n",
    "    #train_steps = 2*len(train_indices)/batch_size\n",
    "    #valid_steps = 2*len(valid_indices)/batch_size\n",
    "    train_steps = len(train_indices_2019)/batch_size\n",
    "    valid_steps = len(valid_indices_2019)/batch_size\n",
    "    fit_history = model.fit_generator(\n",
    "            warmup_gen_2019,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs = warmup_2019,\n",
    "            use_multiprocessing=True,\n",
    "            workers=2,\n",
    "            callbacks=[], #no callbacks for warmup\n",
    "        verbose = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "#https://datascience.stackexchange.com/questions/26112/decay-parameter-in-keras-optimizers\n",
    "if lr_decay > 0:\n",
    "    model.compile(optimizer=Adam(lr=0.0001, decay=lr_decay), loss='categorical_crossentropy', metrics=[\"accuracy\"]) #loss=focal_loss\n",
    "else:\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=[\"accuracy\"]) #loss=focal_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "103/102 [==============================] - 434s 4s/step - loss: 1.3892 - acc: 0.5442 - val_loss: 3.6874 - val_acc: 0.3351\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.93809\n",
      "Epoch 2/5\n",
      "103/102 [==============================] - 430s 4s/step - loss: 0.9651 - acc: 0.6552 - val_loss: 1.7905 - val_acc: 0.4632\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.93809\n",
      "Epoch 3/5\n",
      "103/102 [==============================] - 438s 4s/step - loss: 0.8929 - acc: 0.6865 - val_loss: 2.2210 - val_acc: 0.4278\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.93809\n",
      "Epoch 4/5\n",
      "103/102 [==============================] - 450s 4s/step - loss: 0.8422 - acc: 0.7011 - val_loss: 2.1590 - val_acc: 0.4796\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.93809\n",
      "Epoch 5/5\n",
      "103/102 [==============================] - 444s 4s/step - loss: 0.8164 - acc: 0.7078 - val_loss: 2.6891 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.93809\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n"
     ]
    }
   ],
   "source": [
    "#train_steps = 2*len(train_indices)/batch_size\n",
    "#valid_steps = 2*len(valid_indices)/batch_size\n",
    "train_steps = len(train_indices_2019)/batch_size\n",
    "valid_steps = len(valid_indices_2019)/batch_size\n",
    "fit_history = model.fit_generator(\n",
    "        train_gen_2019,\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs = epochs_2019,\n",
    "        validation_data=valid_gen_2019,\n",
    "        validation_steps=valid_steps,\n",
    "        use_multiprocessing=True,\n",
    "        workers=2,\n",
    "        callbacks=callbacks_list,\n",
    "    verbose = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(fit_history):\n",
    "    plt.clf()\n",
    "    plt.plot(fit_history.history['acc'])\n",
    "    plt.plot(fit_history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    # summarize history for loss\n",
    "    plt.plot(fit_history.history['loss'])\n",
    "    plt.plot(fit_history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_accuracy(fit_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.687380</td>\n",
       "      <td>0.335150</td>\n",
       "      <td>1.389332</td>\n",
       "      <td>0.544158</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.790485</td>\n",
       "      <td>0.463215</td>\n",
       "      <td>0.965065</td>\n",
       "      <td>0.655235</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.221013</td>\n",
       "      <td>0.427793</td>\n",
       "      <td>0.892760</td>\n",
       "      <td>0.686495</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.159013</td>\n",
       "      <td>0.479564</td>\n",
       "      <td>0.842177</td>\n",
       "      <td>0.701062</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.689073</td>\n",
       "      <td>0.487738</td>\n",
       "      <td>0.816402</td>\n",
       "      <td>0.707739</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   val_loss   val_acc      loss       acc      lr\n",
       "0  3.687380  0.335150  1.389332  0.544158  0.0001\n",
       "1  1.790485  0.463215  0.965065  0.655235  0.0001\n",
       "2  2.221013  0.427793  0.892760  0.686495  0.0001\n",
       "3  2.159013  0.479564  0.842177  0.701062  0.0001\n",
       "4  2.689073  0.487738  0.816402  0.707739  0.0001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(fit_history.history).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddca7324ea8d44d7ac2f25544e2dd367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_path' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_tta = 10\n",
    "\n",
    "submit = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "predicted = []\n",
    "\n",
    "for name in tqdm(submit['id_code']):\n",
    "    np_img = img_pad_resize(df_test, name+\".png\", test_path, False)\n",
    "#    path = os.path.join('../input/imet-2019-fgvc6/test/', name)\n",
    "#    img = PIL.Image.open(path+\".png\")\n",
    "#    resized = img.resize((model_img_size, model_img_size))\n",
    "#    np_img = np.array(resized)\n",
    "    if tta:\n",
    "        img_pred = np.zeros(n_classes)\n",
    "        for x in range(n_tta):\n",
    "            aug_img = img_augment(np_img)\n",
    "            #img_augment already runs resnet50.preprocess_input\n",
    "            score_predict = model.predict(aug_img[np.newaxis])\n",
    "            img_pred += np.squeeze(score_predict)\n",
    "        img_pred /= n_tta\n",
    "        score_predict = img_pred\n",
    "    else:\n",
    "        score_predict = model.predict(resnet50.preprocess_input(np_img[np.newaxis]))\n",
    "    label_predict = np.argmax(score_predict)\n",
    "    predicted.append(str(label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-4ca1cf038db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diagnosis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4001\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "submit['diagnosis'] = predicted\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08956eff20a94d12b3e9800b9fee0576": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b3a126ce6c944558e76f66a8d116c6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_08956eff20a94d12b3e9800b9fee0576",
       "max": 1928,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c3f72b53b75b49d08fc84a36665622c7",
       "value": 0
      }
     },
     "3327b2f768f1474293620bcbc7068ff0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_456d3f822e68489c95626c2b6c9717a8",
        "IPY_MODEL_599a6b45440d48fbaf53f3bb7b09f872"
       ],
       "layout": "IPY_MODEL_47bc11d00d664582bbf77f8192b00b27"
      }
     },
     "456d3f822e68489c95626c2b6c9717a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_693e4db9e0b54a78bc7a7c86273a1dde",
       "max": 40716,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fd0232cea7794d11ba8f8baa211bdd1e",
       "value": 40716
      }
     },
     "47bc11d00d664582bbf77f8192b00b27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "599a6b45440d48fbaf53f3bb7b09f872": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7b5b2f01c13f43faabd6c68c2f17ffc7",
       "placeholder": "​",
       "style": "IPY_MODEL_a59d23f46819432d98cdbff0cc6b83f1",
       "value": "100% 40716/40716 [01:40&lt;00:00, 403.51it/s]"
      }
     },
     "5f80ab444cb54250a1cbf52fdb3c3b28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "609da610d5094eeaa3a3b42970e4f425": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f31378395b8f40929eceafca65c81c16",
       "placeholder": "​",
       "style": "IPY_MODEL_d0e962bece0d403da7d8b347e4080783",
       "value": "  0% 0/1928 [00:00&lt;?, ?it/s]"
      }
     },
     "693e4db9e0b54a78bc7a7c86273a1dde": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b5b2f01c13f43faabd6c68c2f17ffc7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a59d23f46819432d98cdbff0cc6b83f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c3f72b53b75b49d08fc84a36665622c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d0e962bece0d403da7d8b347e4080783": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ddca7324ea8d44d7ac2f25544e2dd367": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1b3a126ce6c944558e76f66a8d116c6c",
        "IPY_MODEL_609da610d5094eeaa3a3b42970e4f425"
       ],
       "layout": "IPY_MODEL_5f80ab444cb54250a1cbf52fdb3c3b28"
      }
     },
     "f31378395b8f40929eceafca65c81c16": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd0232cea7794d11ba8f8baa211bdd1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
