{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d05929a6055563ff49aa0be47c96fb60c326698"
   },
   "source": [
    "For me, this VSB power line competition was a good chance to learn how to use LSTM or RNN in general (I expect GRU should not be much different to apply with Keras..). I need a place to write things down so I remember another day and not just today. I wrote myself a [blog post](https://swenotes.wordpress.com/2019/02/22/learning-to-lstm/) to remind myself. This kernel is an attempt to put some working code somewhere.\n",
    "\n",
    "If I got any part wrong about here, or missing something, do let me know :).\n",
    "\n",
    "I started with the [kernel](https://www.kaggle.com/braquino/5-fold-lstm-attention-fully-commented-0-694) by Bruno Marek. Then played with the data and classifiers myself, built a separate [preprocessing kernel](https://www.kaggle.com/donkeys/preprocessing-with-python-multiprocessing) as well. This kernel uses data produced by that preprocessing kernel.\n",
    "\n",
    "There are other public kernels in the competition with better scores but I wanted to keep this simple to help myself more clearly understand the core concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K \n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from keras.callbacks import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select how many folds will be created\n",
    "N_SPLITS = 5\n",
    "# it is just a constant with the measurements data size\n",
    "sample_size = 800000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad473b0c30029b79cf45700fdc44ecf994186c0c"
   },
   "source": [
    "Matthews correlation coefficient is simply the measure given in this Kaggle competition as a way to measure the score. It [seems](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) that a value of 1 would mean perfect prediction, and 0 equal to random values. So maybe 0.6-0.7 that many kernels get is not all that bad? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d892404a7d36bb161574aac74d42afcefcc7d44d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matthews_correlation_coeff(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred = tf.convert_to_tensor(y_pred, np.float32)\n",
    "    y_true = tf.convert_to_tensor(y_true, np.float32)\n",
    "\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6ab983e9f5733a42ec4d5e4a8b1532b1453cc244",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training set metadata, defines which signals are in which order in the data\n",
    "train_meta = pd.read_csv('../input/vsb-power-line-fault-detection/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "train_meta = train_meta.set_index(['id_measurement', 'phase'])\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "04c76b951dad765be6d88a96f15aefec0a03b057",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test set metadata, defines which signals are in which order in the data\n",
    "test_meta = pd.read_csv('../input/vsb-power-line-fault-detection/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "test_meta = test_meta.set_index(['id_measurement', 'phase'])\n",
    "test_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e938b19c039d1b0a56d6a0f1df74031e0c5c64fc",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  my_test_combined_scaled.csv.gz\r\n",
      "__output__.json     my_test_scaled.csv.gz\r\n",
      "__results__.html    my_train.csv.gz\r\n",
      "__results___files   my_train_combined_scaled.csv.gz\r\n",
      "custom.css\t    my_train_scaled.csv.gz\r\n",
      "my_test.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/preprocessing-with-python-multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "350be95fee64e5501ba8dd349ee10438b7b8e773"
   },
   "source": [
    "The data files produced by the preprocessing kernel is shown above. I had to compress them using gzip to fit them into the kernel 5GB output size limit. Hence the decompression and the filename suffix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fb2d20e2d696000313594830cee891007c943ce8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_pre = pd.read_csv(\"../input/preprocessing-with-python-multiprocessing/my_test_combined_scaled.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8280547b2c5c3adaf28d53fa874934862687b51f",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1084640, 67)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c68738513aebfcd9f363695b7228e960b54999b0"
   },
   "source": [
    "The test dataframe loaded above has 22 features calculated for each of the 3 signals per measurement id. So 66 columns. It becomes 67 when loaded, because dumping the values to disk with pandas.to_csv seems to have generated one extra column (maybe the index?).  Number of actual rows should match the number of measurement id's in the corresponding dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "34a062e44abec351a1b7351d31173df71587ab72",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6779.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1084640/160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90632d6de91dfd52be1fccca109ad2630ecc819e"
   },
   "source": [
    "The training data-set should look about the same but with only the 2904 measurements, so fewer rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "651386a3a97046f6a440eb32f2fd0629eb88d9b4",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464640, 67)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pre = pd.read_csv(\"../input/preprocessing-with-python-multiprocessing/my_train_combined_scaled.csv.gz\", compression=\"gzip\")\n",
    "df_train_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5be9275fa98790adbcc918bd0da8a4703f58acce"
   },
   "source": [
    "To drop the excess column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6b3132ab94f4363bd5c67516f3115116f5e59f44",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
       "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
       "       '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
       "       '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
       "       '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58',\n",
       "       '59', '60', '61', '62', '63', '64', '65'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pre.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "aeda5c10faf750695b6a2cc1bb8fc7bdd8cafbcf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_pre.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_test_pre.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "268d81f222ed136ec2ee9821446fd17af3ba850e"
   },
   "source": [
    "The preprocessed data has 160 timesteps. Number of rows should match the number of measurements times the number of timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "51da21a0c7c4c2e5b38ed59e6d7a8622ea1f77fb",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6779.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of \"observations\" in test dataset\n",
    "df_test_pre.shape[0]/160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "70a5c35c03a05a031f650efbe792bcfbd5da2634",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464640, 66)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "22e8d03b21ff871308d1f9a851695fa2fd043900",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2904.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of \"observations\" in training dataset\n",
    "464640/160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "23447fc71d67c2a94249974b023b0ad1e1e2fa42",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
       "            ...\n",
       "            2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903],\n",
       "           dtype='int64', name='id_measurement', length=2904)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.index.get_level_values('id_measurement').unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd76fad6f620c23d2057324bff512420fceddc93"
   },
   "source": [
    "LSTM is about timesteps, and in this case the 800k measurements per signal were summarized to 160 timesteps per signal in the pre-processing. So things like average values of 5000 measurementes (800k/5000=160). These are now in rows 0-159 for the first measurements id, where the columns 0-21 are for the first signal, columns 22-43 for second signal, and 44-65 for the third signal.\n",
    "\n",
    "This continues for the following measurements with the 3 signals per measurement id in the columns. So the signals for the second measurement id are in rows 160-319.\n",
    "\n",
    "A look at first signal for the first measurement id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "2b9f2cc68d598fa14e35fd037ba7dae565b492d3",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.494648</td>\n",
       "      <td>-0.952345</td>\n",
       "      <td>0.357633</td>\n",
       "      <td>0.578792</td>\n",
       "      <td>-0.968254</td>\n",
       "      <td>-0.890359</td>\n",
       "      <td>-0.953549</td>\n",
       "      <td>-0.890387</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.382716</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>-0.376623</td>\n",
       "      <td>0.978171</td>\n",
       "      <td>0.957951</td>\n",
       "      <td>0.830951</td>\n",
       "      <td>-0.036067</td>\n",
       "      <td>-0.924594</td>\n",
       "      <td>-0.943128</td>\n",
       "      <td>-0.972498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.489032</td>\n",
       "      <td>-0.931789</td>\n",
       "      <td>0.357123</td>\n",
       "      <td>0.568724</td>\n",
       "      <td>-0.960317</td>\n",
       "      <td>-0.880009</td>\n",
       "      <td>-0.945913</td>\n",
       "      <td>-0.880009</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.719626</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.382716</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>-0.376623</td>\n",
       "      <td>0.968572</td>\n",
       "      <td>0.931391</td>\n",
       "      <td>0.868738</td>\n",
       "      <td>0.076707</td>\n",
       "      <td>-0.888232</td>\n",
       "      <td>-0.935788</td>\n",
       "      <td>-0.969848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.513685</td>\n",
       "      <td>-0.933146</td>\n",
       "      <td>0.380146</td>\n",
       "      <td>0.592253</td>\n",
       "      <td>-0.896825</td>\n",
       "      <td>-0.852738</td>\n",
       "      <td>-0.931397</td>\n",
       "      <td>-0.852738</td>\n",
       "      <td>0.776316</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>-0.350649</td>\n",
       "      <td>0.883337</td>\n",
       "      <td>0.932998</td>\n",
       "      <td>0.876992</td>\n",
       "      <td>-0.418356</td>\n",
       "      <td>-0.880289</td>\n",
       "      <td>-0.934185</td>\n",
       "      <td>-0.957056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.503390</td>\n",
       "      <td>-0.943940</td>\n",
       "      <td>0.367875</td>\n",
       "      <td>0.585069</td>\n",
       "      <td>-0.944444</td>\n",
       "      <td>-0.880590</td>\n",
       "      <td>-0.945568</td>\n",
       "      <td>-0.880590</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>-0.363636</td>\n",
       "      <td>0.949511</td>\n",
       "      <td>0.946492</td>\n",
       "      <td>0.772128</td>\n",
       "      <td>-0.211620</td>\n",
       "      <td>-0.813630</td>\n",
       "      <td>-0.954552</td>\n",
       "      <td>-0.964410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2    ...           19        20        21\n",
       "0    0.494648 -0.952345  0.357633    ...    -0.924594 -0.943128 -0.972498\n",
       "1    0.489032 -0.931789  0.357123    ...    -0.888232 -0.935788 -0.969848\n",
       "..        ...       ...       ...    ...          ...       ...       ...\n",
       "158  0.513685 -0.933146  0.380146    ...    -0.880289 -0.934185 -0.957056\n",
       "159  0.503390 -0.943940  0.367875    ...    -0.813630 -0.954552 -0.964410\n",
       "\n",
       "[160 rows x 22 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 5)\n",
    "df_train_pre.iloc[0:160,:22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6347010934564e651745e9cb1ddb8a69b04d440"
   },
   "source": [
    "Second signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "2139528960831684c24e9dadbb33a5ca701b0077",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050128</td>\n",
       "      <td>-0.955448</td>\n",
       "      <td>-0.063952</td>\n",
       "      <td>0.160955</td>\n",
       "      <td>-0.960317</td>\n",
       "      <td>-0.899325</td>\n",
       "      <td>-0.959588</td>\n",
       "      <td>-0.899351</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.352443</td>\n",
       "      <td>-0.597403</td>\n",
       "      <td>0.968076</td>\n",
       "      <td>0.963942</td>\n",
       "      <td>0.861703</td>\n",
       "      <td>0.055711</td>\n",
       "      <td>-0.895001</td>\n",
       "      <td>-0.970975</td>\n",
       "      <td>-0.970341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.091872</td>\n",
       "      <td>-0.936412</td>\n",
       "      <td>-0.019979</td>\n",
       "      <td>0.195834</td>\n",
       "      <td>-0.960317</td>\n",
       "      <td>-0.888039</td>\n",
       "      <td>-0.951721</td>\n",
       "      <td>-0.888039</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>-0.295305</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>0.972821</td>\n",
       "      <td>0.943148</td>\n",
       "      <td>0.929093</td>\n",
       "      <td>-0.262862</td>\n",
       "      <td>-0.830152</td>\n",
       "      <td>-0.924066</td>\n",
       "      <td>-0.965615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.019824</td>\n",
       "      <td>-0.932546</td>\n",
       "      <td>-0.124825</td>\n",
       "      <td>0.089763</td>\n",
       "      <td>-0.849206</td>\n",
       "      <td>-0.866652</td>\n",
       "      <td>-0.938501</td>\n",
       "      <td>-0.866652</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.364486</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>-0.086420</td>\n",
       "      <td>-0.390534</td>\n",
       "      <td>-0.545455</td>\n",
       "      <td>0.878620</td>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.809985</td>\n",
       "      <td>-0.098638</td>\n",
       "      <td>-0.777201</td>\n",
       "      <td>-0.947199</td>\n",
       "      <td>-0.888475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.020600</td>\n",
       "      <td>-0.931131</td>\n",
       "      <td>-0.086222</td>\n",
       "      <td>0.127496</td>\n",
       "      <td>-0.952381</td>\n",
       "      <td>-0.891018</td>\n",
       "      <td>-0.951423</td>\n",
       "      <td>-0.891018</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>-0.061728</td>\n",
       "      <td>-0.352443</td>\n",
       "      <td>-0.610390</td>\n",
       "      <td>0.957546</td>\n",
       "      <td>0.934803</td>\n",
       "      <td>0.886256</td>\n",
       "      <td>-0.390708</td>\n",
       "      <td>-0.871374</td>\n",
       "      <td>-0.932386</td>\n",
       "      <td>-0.968619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           22        23        24    ...           41        42        43\n",
       "0    0.050128 -0.955448 -0.063952    ...    -0.895001 -0.970975 -0.970341\n",
       "1    0.091872 -0.936412 -0.019979    ...    -0.830152 -0.924066 -0.965615\n",
       "..        ...       ...       ...    ...          ...       ...       ...\n",
       "158 -0.019824 -0.932546 -0.124825    ...    -0.777201 -0.947199 -0.888475\n",
       "159  0.020600 -0.931131 -0.086222    ...    -0.871374 -0.932386 -0.968619\n",
       "\n",
       "[160 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pre.iloc[0:160,22:44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a76cdf4c2f1ddf74c358dec0d016d26916efdcf"
   },
   "source": [
    "Third signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "72bb1fbb510737a78fc5ec71347e36e5512ebe85",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.473546</td>\n",
       "      <td>-0.938704</td>\n",
       "      <td>-0.555837</td>\n",
       "      <td>-0.336029</td>\n",
       "      <td>-0.944444</td>\n",
       "      <td>-0.901068</td>\n",
       "      <td>-0.959178</td>\n",
       "      <td>-0.901093</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.480519</td>\n",
       "      <td>-0.530864</td>\n",
       "      <td>-0.714313</td>\n",
       "      <td>-0.844156</td>\n",
       "      <td>0.958701</td>\n",
       "      <td>0.937998</td>\n",
       "      <td>0.902659</td>\n",
       "      <td>-0.341752</td>\n",
       "      <td>-0.855589</td>\n",
       "      <td>-0.929200</td>\n",
       "      <td>-0.955255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.498778</td>\n",
       "      <td>-0.934701</td>\n",
       "      <td>-0.578790</td>\n",
       "      <td>-0.360719</td>\n",
       "      <td>-0.960317</td>\n",
       "      <td>-0.882963</td>\n",
       "      <td>-0.948802</td>\n",
       "      <td>-0.882963</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.506494</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-0.733359</td>\n",
       "      <td>-0.870130</td>\n",
       "      <td>0.970653</td>\n",
       "      <td>0.937150</td>\n",
       "      <td>0.898306</td>\n",
       "      <td>-0.354745</td>\n",
       "      <td>-0.859778</td>\n",
       "      <td>-0.930046</td>\n",
       "      <td>-0.967774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.418144</td>\n",
       "      <td>-0.941282</td>\n",
       "      <td>-0.503986</td>\n",
       "      <td>-0.283263</td>\n",
       "      <td>-0.912698</td>\n",
       "      <td>-0.877181</td>\n",
       "      <td>-0.943457</td>\n",
       "      <td>-0.877181</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.481481</td>\n",
       "      <td>-0.676221</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.933222</td>\n",
       "      <td>0.878141</td>\n",
       "      <td>-0.414926</td>\n",
       "      <td>-0.879183</td>\n",
       "      <td>-0.933962</td>\n",
       "      <td>-0.956975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.447092</td>\n",
       "      <td>-0.939463</td>\n",
       "      <td>-0.530969</td>\n",
       "      <td>-0.310944</td>\n",
       "      <td>-0.960317</td>\n",
       "      <td>-0.892205</td>\n",
       "      <td>-0.950902</td>\n",
       "      <td>-0.892205</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.506173</td>\n",
       "      <td>-0.695267</td>\n",
       "      <td>-0.844156</td>\n",
       "      <td>0.970688</td>\n",
       "      <td>0.937245</td>\n",
       "      <td>0.898793</td>\n",
       "      <td>-0.353290</td>\n",
       "      <td>-0.859309</td>\n",
       "      <td>-0.929951</td>\n",
       "      <td>-0.967740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           44        45        46    ...           63        64        65\n",
       "0   -0.473546 -0.938704 -0.555837    ...    -0.855589 -0.929200 -0.955255\n",
       "1   -0.498778 -0.934701 -0.578790    ...    -0.859778 -0.930046 -0.967774\n",
       "..        ...       ...       ...    ...          ...       ...       ...\n",
       "158 -0.418144 -0.941282 -0.503986    ...    -0.879183 -0.933962 -0.956975\n",
       "159 -0.447092 -0.939463 -0.530969    ...    -0.859309 -0.929951 -0.967740\n",
       "\n",
       "[160 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pre.iloc[0:160,44:66]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7266b17a55655d08d2e765bf431399aba9280a1"
   },
   "source": [
    "And the 3 signals for the second measurement id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "bbbe99fa67cda6178995a315a98baff6b343955b",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.369433</td>\n",
       "      <td>-0.953609</td>\n",
       "      <td>-0.460751</td>\n",
       "      <td>-0.234531</td>\n",
       "      <td>-0.865079</td>\n",
       "      <td>-0.959812</td>\n",
       "      <td>-0.973771</td>\n",
       "      <td>-0.959826</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.376623</td>\n",
       "      <td>-0.432099</td>\n",
       "      <td>-0.657176</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>0.909381</td>\n",
       "      <td>0.937218</td>\n",
       "      <td>0.898654</td>\n",
       "      <td>-0.353705</td>\n",
       "      <td>-0.859443</td>\n",
       "      <td>-0.963799</td>\n",
       "      <td>-0.882256</td>\n",
       "      <td>-0.103595</td>\n",
       "      <td>-0.959423</td>\n",
       "      <td>-0.210422</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>-0.944444</td>\n",
       "      <td>-0.964459</td>\n",
       "      <td>-0.984430</td>\n",
       "      <td>-0.964472</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.308411</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.185185</td>\n",
       "      <td>-0.466717</td>\n",
       "      <td>-0.649351</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>0.961906</td>\n",
       "      <td>0.851255</td>\n",
       "      <td>0.024530</td>\n",
       "      <td>-0.905056</td>\n",
       "      <td>-0.973005</td>\n",
       "      <td>-0.946647</td>\n",
       "      <td>0.547861</td>\n",
       "      <td>-0.955799</td>\n",
       "      <td>0.407206</td>\n",
       "      <td>0.629701</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>-0.961075</td>\n",
       "      <td>-0.971403</td>\n",
       "      <td>-0.961089</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.558442</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>0.028473</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.916185</td>\n",
       "      <td>0.956045</td>\n",
       "      <td>0.821164</td>\n",
       "      <td>-0.065274</td>\n",
       "      <td>-0.934011</td>\n",
       "      <td>-0.945028</td>\n",
       "      <td>-0.912118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-0.346308</td>\n",
       "      <td>-0.950447</td>\n",
       "      <td>-0.438117</td>\n",
       "      <td>-0.213491</td>\n",
       "      <td>-0.801587</td>\n",
       "      <td>-0.956140</td>\n",
       "      <td>-0.958566</td>\n",
       "      <td>-0.956140</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.121495</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.350649</td>\n",
       "      <td>-0.407407</td>\n",
       "      <td>-0.638130</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.873907</td>\n",
       "      <td>0.940827</td>\n",
       "      <td>0.917182</td>\n",
       "      <td>-0.298410</td>\n",
       "      <td>-0.841614</td>\n",
       "      <td>-0.960200</td>\n",
       "      <td>-0.819890</td>\n",
       "      <td>-0.125541</td>\n",
       "      <td>-0.958939</td>\n",
       "      <td>-0.231087</td>\n",
       "      <td>-0.003643</td>\n",
       "      <td>-0.912698</td>\n",
       "      <td>-0.963185</td>\n",
       "      <td>-0.979853</td>\n",
       "      <td>-0.963185</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.116883</td>\n",
       "      <td>-0.209877</td>\n",
       "      <td>-0.466717</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.940959</td>\n",
       "      <td>0.956750</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>-0.054464</td>\n",
       "      <td>-0.930526</td>\n",
       "      <td>-0.944325</td>\n",
       "      <td>-0.924077</td>\n",
       "      <td>0.546355</td>\n",
       "      <td>-0.951482</td>\n",
       "      <td>0.406790</td>\n",
       "      <td>0.627279</td>\n",
       "      <td>-0.809524</td>\n",
       "      <td>-0.955357</td>\n",
       "      <td>-0.949939</td>\n",
       "      <td>-0.955357</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.558442</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>0.028473</td>\n",
       "      <td>-0.220779</td>\n",
       "      <td>0.855601</td>\n",
       "      <td>0.958019</td>\n",
       "      <td>0.831299</td>\n",
       "      <td>-0.035028</td>\n",
       "      <td>-0.924259</td>\n",
       "      <td>-0.943060</td>\n",
       "      <td>-0.850341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-0.408335</td>\n",
       "      <td>-0.958612</td>\n",
       "      <td>-0.498752</td>\n",
       "      <td>-0.269997</td>\n",
       "      <td>-0.928571</td>\n",
       "      <td>-0.964296</td>\n",
       "      <td>-0.979347</td>\n",
       "      <td>-0.964296</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.402597</td>\n",
       "      <td>-0.481481</td>\n",
       "      <td>-0.676221</td>\n",
       "      <td>-0.779221</td>\n",
       "      <td>0.964587</td>\n",
       "      <td>0.954287</td>\n",
       "      <td>0.812144</td>\n",
       "      <td>-0.092194</td>\n",
       "      <td>-0.942691</td>\n",
       "      <td>-0.946780</td>\n",
       "      <td>-0.924964</td>\n",
       "      <td>-0.056101</td>\n",
       "      <td>-0.958038</td>\n",
       "      <td>-0.165133</td>\n",
       "      <td>0.061532</td>\n",
       "      <td>-0.936508</td>\n",
       "      <td>-0.967427</td>\n",
       "      <td>-0.984778</td>\n",
       "      <td>-0.967427</td>\n",
       "      <td>0.539474</td>\n",
       "      <td>0.327103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.064935</td>\n",
       "      <td>-0.135802</td>\n",
       "      <td>-0.428626</td>\n",
       "      <td>-0.649351</td>\n",
       "      <td>0.932583</td>\n",
       "      <td>0.933575</td>\n",
       "      <td>0.879952</td>\n",
       "      <td>-0.409521</td>\n",
       "      <td>-0.877440</td>\n",
       "      <td>-0.967431</td>\n",
       "      <td>-0.969061</td>\n",
       "      <td>0.538198</td>\n",
       "      <td>-0.956311</td>\n",
       "      <td>0.397938</td>\n",
       "      <td>0.620721</td>\n",
       "      <td>-0.912698</td>\n",
       "      <td>-0.962326</td>\n",
       "      <td>-0.974544</td>\n",
       "      <td>-0.962326</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>-0.311688</td>\n",
       "      <td>0.933022</td>\n",
       "      <td>0.934789</td>\n",
       "      <td>0.886186</td>\n",
       "      <td>-0.390916</td>\n",
       "      <td>-0.871441</td>\n",
       "      <td>-0.966220</td>\n",
       "      <td>-0.931984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-0.387611</td>\n",
       "      <td>-0.958211</td>\n",
       "      <td>-0.479037</td>\n",
       "      <td>-0.250577</td>\n",
       "      <td>-0.912698</td>\n",
       "      <td>-0.965684</td>\n",
       "      <td>-0.979653</td>\n",
       "      <td>-0.965684</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.376623</td>\n",
       "      <td>-0.456790</td>\n",
       "      <td>-0.676221</td>\n",
       "      <td>-0.779221</td>\n",
       "      <td>0.930251</td>\n",
       "      <td>0.961045</td>\n",
       "      <td>0.846832</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>-0.909312</td>\n",
       "      <td>-0.973864</td>\n",
       "      <td>-0.934744</td>\n",
       "      <td>-0.084046</td>\n",
       "      <td>-0.960776</td>\n",
       "      <td>-0.192230</td>\n",
       "      <td>0.035856</td>\n",
       "      <td>-0.952381</td>\n",
       "      <td>-0.972022</td>\n",
       "      <td>-0.986385</td>\n",
       "      <td>-0.972022</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.327103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.160494</td>\n",
       "      <td>-0.447672</td>\n",
       "      <td>-0.662338</td>\n",
       "      <td>0.958080</td>\n",
       "      <td>0.970203</td>\n",
       "      <td>0.893848</td>\n",
       "      <td>-0.368049</td>\n",
       "      <td>-0.864068</td>\n",
       "      <td>-0.964732</td>\n",
       "      <td>-0.968087</td>\n",
       "      <td>0.545744</td>\n",
       "      <td>-0.959408</td>\n",
       "      <td>0.404358</td>\n",
       "      <td>0.628547</td>\n",
       "      <td>-0.944444</td>\n",
       "      <td>-0.966240</td>\n",
       "      <td>-0.980883</td>\n",
       "      <td>-0.966240</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.558442</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>0.028473</td>\n",
       "      <td>-0.337662</td>\n",
       "      <td>0.953966</td>\n",
       "      <td>0.958819</td>\n",
       "      <td>0.835408</td>\n",
       "      <td>-0.022763</td>\n",
       "      <td>-0.920304</td>\n",
       "      <td>-0.942262</td>\n",
       "      <td>-0.959972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2    ...           63        64        65\n",
       "160 -0.369433 -0.953609 -0.460751    ...    -0.934011 -0.945028 -0.912118\n",
       "161 -0.346308 -0.950447 -0.438117    ...    -0.924259 -0.943060 -0.850341\n",
       "..        ...       ...       ...    ...          ...       ...       ...\n",
       "318 -0.408335 -0.958612 -0.498752    ...    -0.871441 -0.966220 -0.931984\n",
       "319 -0.387611 -0.958211 -0.479037    ...    -0.920304 -0.942262 -0.959972\n",
       "\n",
       "[160 rows x 66 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pre.iloc[160:320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "7e40047ccd43b9587ba282d97c17cad5ee4b01e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "f61162d75cc9848929bc50108613f546e28afb45",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n",
    "# So, find the best threshold to convert float to binary is crucial to the result\n",
    "# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    scores = []\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        yp_np = np.array(y_proba)\n",
    "        yp_bool = yp_np >= threshold\n",
    "        score = matthews_corrcoef(y_true, yp_bool)\n",
    "        #score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n",
    "        scores.append(score)\n",
    "        if score > best_score:\n",
    "            print(\"found better score:\"+str(score)+\", th=\"+str(threshold))\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n",
    "    scores_df = pd.DataFrame({\"score\": scores})\n",
    "    print(\"scores plot:\")\n",
    "    scores_df.plot()\n",
    "    plt.show()\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7423e955bb77b7c693e693e829a752039edbcd2a"
   },
   "source": [
    "Create the actual LSTM model. For more explanations, see my [blog post](https://swenotes.wordpress.com/2019/02/22/learning-to-lstm/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "baf198807424261f69cd4e1dacfcb08f2b076a2e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_data):\n",
    "    input_shape = input_data.shape\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],), name=\"input_signal\")\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True, name=\"lstm1\"), name=\"bi1\")(inp)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=False, name=\"lstm2\"), name=\"bi2\")(x)\n",
    "    #other kernels have used also a custom Attention layer but I leave it out for simplicity here\n",
    "#    x = Attention(input_shape[1])(x)\n",
    "    x = Dense(128, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "    x = Dense(1, activation='sigmoid', name=\"output\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation_coeff])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76b14da058fb7aef695e84385b38c5b3120337d5"
   },
   "source": [
    "Since the dataset has been combined to have all 3 phase signals per measurement id on a single row (22\\*3=66 features/columns), as combined features, I need to combine the prediction targets for all 3 signals also into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "239f7df87a4fc532f5bf85ed45108498e648e07a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if any of the 3 signals for a measurement id is labeled as faulty, this labels the whole set of 3 as faulty\n",
    "y = (train_meta.groupby(\"id_measurement\").sum()/3 > 0)[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "a5914d97cd75f8a7dfd08a1a47ad6dca5880c550",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2904,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see the number of targets matches the number of rows in training dataset\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "845a53f037e08b3cad8d61babc8c9f9bf9d15a00"
   },
   "source": [
    "LSTM requires 3-dimensional input, so this reshapes the dataframe values from 2D dataframe to a 3D numpy matrix in the required format. 2904 observations, 160 timesteps, each with 66 features. \n",
    "\n",
    "Features for each timestep are on a single row in the dataframe (66 on a row) as is. The dataframe here being \"df_train_pre\". The dataframe has 160 rows per measurement id as shown above (df_train_pre.iloc[0:160] for measurement id 1 and df_train_pre.iloc[160:320] for measurement id 2, and so on). Each of these measurement id sets should be its own \"observation\" in the numpy matrix used as input for the LSTM. \n",
    "\n",
    "The following reshape creates the required input format, setting the overall input shape as (2904, 160, 66). This is 2904 observations, 160 timesteps for each of those 2904 observations, and 66 features for each of those 160 timesteps. This is the 3D format format LSTM expects as input. A timestep has been formed by splitting the sequence of signal values over time to 160 separate values on after the other, and collecting the 66 features for that timeslot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "47572a667c8206b1c86c634829dcfd545c44b704",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2904, 160, 66)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if using all signal values separately, the number of rows would be 8712, or 2904*3.\n",
    "#X = df_train_pre.values.reshape(8712, 160, 22)\n",
    "#but with the current data format I show above, it is 2904 rows, or \"observations\"\n",
    "X = df_train_pre.values.reshape(2904, 160, 66)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d718714f990b294e56b2627118bcd648ce2ba0f1"
   },
   "source": [
    "Now to do the same for the test-dataset, but remembering it has more rows, so the first dimension is higher. Maybe because the people at Kaggle want to make life difficult for the competitors and so the test set is much bigger :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "aaa44bd8ec393e5570f5967c1ecbfca776fb07d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = df_test_pre.values.reshape(6779, 160, 66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8ba0fa7e0e7e8bbb734984ceeb1dcf975fdaef7"
   },
   "source": [
    "Finally, train and run a simple LSTM classifier for all this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "77513d34d31037a14211c0828cbe04e4f97d74c2",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 4s 2ms/step - loss: 0.3178 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2456 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation_coeff improved from -inf to 0.00000, saving model to weights.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 1s 539us/step - loss: 0.2458 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2440 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 1s 540us/step - loss: 0.2432 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2411 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 1s 541us/step - loss: 0.2412 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2330 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 1s 542us/step - loss: 0.2358 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2376 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 1s 547us/step - loss: 0.2298 - matthews_correlation_coeff: 0.0820 - val_loss: 0.2250 - val_matthews_correlation_coeff: 0.1325\n",
      "\n",
      "Epoch 00006: val_matthews_correlation_coeff improved from 0.00000 to 0.13255, saving model to weights.h5\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 1s 545us/step - loss: 0.2238 - matthews_correlation_coeff: 0.1487 - val_loss: 0.2375 - val_matthews_correlation_coeff: 0.3012\n",
      "\n",
      "Epoch 00007: val_matthews_correlation_coeff improved from 0.13255 to 0.30124, saving model to weights.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 1s 550us/step - loss: 0.2403 - matthews_correlation_coeff: 0.1431 - val_loss: 0.2244 - val_matthews_correlation_coeff: 0.1035\n",
      "\n",
      "Epoch 00008: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 1s 545us/step - loss: 0.2272 - matthews_correlation_coeff: 0.2088 - val_loss: 0.2294 - val_matthews_correlation_coeff: 0.1446\n",
      "\n",
      "Epoch 00009: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 1s 548us/step - loss: 0.2346 - matthews_correlation_coeff: 0.1922 - val_loss: 0.2430 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 1s 545us/step - loss: 0.2417 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2386 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 1s 546us/step - loss: 0.2389 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2281 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 1s 545us/step - loss: 0.2333 - matthews_correlation_coeff: 0.0159 - val_loss: 0.2230 - val_matthews_correlation_coeff: 0.0711\n",
      "\n",
      "Epoch 00013: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 1s 541us/step - loss: 0.2335 - matthews_correlation_coeff: 0.0899 - val_loss: 0.2217 - val_matthews_correlation_coeff: 0.1592\n",
      "\n",
      "Epoch 00014: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 1s 543us/step - loss: 0.2218 - matthews_correlation_coeff: 0.1821 - val_loss: 0.2237 - val_matthews_correlation_coeff: 0.1535\n",
      "\n",
      "Epoch 00015: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 1s 547us/step - loss: 0.2198 - matthews_correlation_coeff: 0.1715 - val_loss: 0.2090 - val_matthews_correlation_coeff: 0.2456\n",
      "\n",
      "Epoch 00016: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 1s 544us/step - loss: 0.2277 - matthews_correlation_coeff: 0.2079 - val_loss: 0.2325 - val_matthews_correlation_coeff: 0.3012\n",
      "\n",
      "Epoch 00017: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 1s 543us/step - loss: 0.2271 - matthews_correlation_coeff: 0.2892 - val_loss: 0.2243 - val_matthews_correlation_coeff: 0.1535\n",
      "\n",
      "Epoch 00018: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 1s 543us/step - loss: 0.2189 - matthews_correlation_coeff: 0.2198 - val_loss: 0.2143 - val_matthews_correlation_coeff: 0.2700\n",
      "\n",
      "Epoch 00019: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 1s 542us/step - loss: 0.2210 - matthews_correlation_coeff: 0.2560 - val_loss: 0.2175 - val_matthews_correlation_coeff: 0.2700\n",
      "\n",
      "Epoch 00020: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 1s 541us/step - loss: 0.2126 - matthews_correlation_coeff: 0.2085 - val_loss: 0.1976 - val_matthews_correlation_coeff: 0.2700\n",
      "\n",
      "Epoch 00021: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 1s 543us/step - loss: 0.2084 - matthews_correlation_coeff: 0.2755 - val_loss: 0.2284 - val_matthews_correlation_coeff: -0.0057\n",
      "\n",
      "Epoch 00022: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 1s 544us/step - loss: 0.2316 - matthews_correlation_coeff: 0.1250 - val_loss: 0.2279 - val_matthews_correlation_coeff: 0.1325\n",
      "\n",
      "Epoch 00023: val_matthews_correlation_coeff did not improve from 0.30124\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 1s 539us/step - loss: 0.2089 - matthews_correlation_coeff: 0.2671 - val_loss: 0.2031 - val_matthews_correlation_coeff: 0.3303\n",
      "\n",
      "Epoch 00024: val_matthews_correlation_coeff improved from 0.30124 to 0.33026, saving model to weights.h5\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 1s 544us/step - loss: 0.2200 - matthews_correlation_coeff: 0.2777 - val_loss: 0.2250 - val_matthews_correlation_coeff: 0.1147\n",
      "\n",
      "Epoch 00025: val_matthews_correlation_coeff did not improve from 0.33026\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 1s 546us/step - loss: 0.2296 - matthews_correlation_coeff: 0.1555 - val_loss: 0.2188 - val_matthews_correlation_coeff: 0.3294\n",
      "\n",
      "Epoch 00026: val_matthews_correlation_coeff did not improve from 0.33026\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 1s 540us/step - loss: 0.2127 - matthews_correlation_coeff: 0.3146 - val_loss: 0.2159 - val_matthews_correlation_coeff: 0.1138\n",
      "\n",
      "Epoch 00027: val_matthews_correlation_coeff did not improve from 0.33026\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 1s 546us/step - loss: 0.2059 - matthews_correlation_coeff: 0.2810 - val_loss: 0.2132 - val_matthews_correlation_coeff: 0.1420\n",
      "\n",
      "Epoch 00028: val_matthews_correlation_coeff did not improve from 0.33026\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 1s 544us/step - loss: 0.2139 - matthews_correlation_coeff: 0.2682 - val_loss: 0.2047 - val_matthews_correlation_coeff: 0.2768\n",
      "\n",
      "Epoch 00029: val_matthews_correlation_coeff did not improve from 0.33026\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 1s 542us/step - loss: 0.1977 - matthews_correlation_coeff: 0.3198 - val_loss: 0.1845 - val_matthews_correlation_coeff: 0.3311\n",
      "\n",
      "Epoch 00030: val_matthews_correlation_coeff improved from 0.33026 to 0.33108, saving model to weights.h5\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 1s 549us/step - loss: 0.1823 - matthews_correlation_coeff: 0.2921 - val_loss: 0.1947 - val_matthews_correlation_coeff: 0.1420\n",
      "\n",
      "Epoch 00031: val_matthews_correlation_coeff did not improve from 0.33108\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 1s 546us/step - loss: 0.1891 - matthews_correlation_coeff: 0.3074 - val_loss: 0.1750 - val_matthews_correlation_coeff: 0.3770\n",
      "\n",
      "Epoch 00032: val_matthews_correlation_coeff improved from 0.33108 to 0.37702, saving model to weights.h5\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 1s 545us/step - loss: 0.1710 - matthews_correlation_coeff: 0.3593 - val_loss: 0.2099 - val_matthews_correlation_coeff: 0.1999\n",
      "\n",
      "Epoch 00033: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 1s 546us/step - loss: 0.1726 - matthews_correlation_coeff: 0.3243 - val_loss: 0.1582 - val_matthews_correlation_coeff: 0.3473\n",
      "\n",
      "Epoch 00034: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 1s 545us/step - loss: 0.1883 - matthews_correlation_coeff: 0.3532 - val_loss: 0.2099 - val_matthews_correlation_coeff: 0.2680\n",
      "\n",
      "Epoch 00035: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 1s 542us/step - loss: 0.1797 - matthews_correlation_coeff: 0.3513 - val_loss: 0.1644 - val_matthews_correlation_coeff: 0.3472\n",
      "\n",
      "Epoch 00036: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 1s 523us/step - loss: 0.1825 - matthews_correlation_coeff: 0.4293 - val_loss: 0.2220 - val_matthews_correlation_coeff: 0.1624\n",
      "\n",
      "Epoch 00037: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.2024 - matthews_correlation_coeff: 0.2069 - val_loss: 0.1895 - val_matthews_correlation_coeff: 0.3311\n",
      "\n",
      "Epoch 00038: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 1s 512us/step - loss: 0.1727 - matthews_correlation_coeff: 0.2780 - val_loss: 0.2155 - val_matthews_correlation_coeff: 0.3311\n",
      "\n",
      "Epoch 00039: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.1853 - matthews_correlation_coeff: 0.3251 - val_loss: 0.1674 - val_matthews_correlation_coeff: 0.3553\n",
      "\n",
      "Epoch 00040: val_matthews_correlation_coeff did not improve from 0.37702\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 1s 512us/step - loss: 0.1524 - matthews_correlation_coeff: 0.4110 - val_loss: 0.1896 - val_matthews_correlation_coeff: 0.4236\n",
      "\n",
      "Epoch 00041: val_matthews_correlation_coeff improved from 0.37702 to 0.42360, saving model to weights.h5\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 1s 519us/step - loss: 0.1665 - matthews_correlation_coeff: 0.3744 - val_loss: 0.2507 - val_matthews_correlation_coeff: 0.2694\n",
      "\n",
      "Epoch 00042: val_matthews_correlation_coeff did not improve from 0.42360\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.1943 - matthews_correlation_coeff: 0.3300 - val_loss: 0.1853 - val_matthews_correlation_coeff: 0.3496\n",
      "\n",
      "Epoch 00043: val_matthews_correlation_coeff did not improve from 0.42360\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.1520 - matthews_correlation_coeff: 0.4362 - val_loss: 0.1723 - val_matthews_correlation_coeff: 0.3185\n",
      "\n",
      "Epoch 00044: val_matthews_correlation_coeff did not improve from 0.42360\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 1s 521us/step - loss: 0.1548 - matthews_correlation_coeff: 0.4055 - val_loss: 0.2151 - val_matthews_correlation_coeff: 0.1147\n",
      "\n",
      "Epoch 00045: val_matthews_correlation_coeff did not improve from 0.42360\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 1s 546us/step - loss: 0.1608 - matthews_correlation_coeff: 0.4243 - val_loss: 0.2164 - val_matthews_correlation_coeff: 0.3690\n",
      "\n",
      "Epoch 00046: val_matthews_correlation_coeff did not improve from 0.42360\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 1s 545us/step - loss: 0.1500 - matthews_correlation_coeff: 0.5262 - val_loss: 0.1589 - val_matthews_correlation_coeff: 0.4549\n",
      "\n",
      "Epoch 00047: val_matthews_correlation_coeff improved from 0.42360 to 0.45493, saving model to weights.h5\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 1s 546us/step - loss: 0.1468 - matthews_correlation_coeff: 0.5407 - val_loss: 0.1494 - val_matthews_correlation_coeff: 0.3979\n",
      "\n",
      "Epoch 00048: val_matthews_correlation_coeff did not improve from 0.45493\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 1s 541us/step - loss: 0.1261 - matthews_correlation_coeff: 0.5769 - val_loss: 0.1813 - val_matthews_correlation_coeff: 0.3252\n",
      "\n",
      "Epoch 00049: val_matthews_correlation_coeff did not improve from 0.45493\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 1s 542us/step - loss: 0.1285 - matthews_correlation_coeff: 0.5249 - val_loss: 0.1629 - val_matthews_correlation_coeff: 0.5026\n",
      "\n",
      "Epoch 00050: val_matthews_correlation_coeff improved from 0.45493 to 0.50255, saving model to weights.h5\n",
      "finding threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:543: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found better score:0.1415128685786186, th=0.01\n",
      "found better score:0.2465212123683504, th=0.02\n",
      "found better score:0.30229914450062456, th=0.03\n",
      "found better score:0.31776458765870236, th=0.04\n",
      "found better score:0.33340217684654555, th=0.05\n",
      "found better score:0.34861122032586195, th=0.06\n",
      "found better score:0.36661456923387986, th=0.07\n",
      "found better score:0.375299816920896, th=0.08\n",
      "found better score:0.3856817757059246, th=0.1\n",
      "found better score:0.38985079269855577, th=0.12\n",
      "found better score:0.39745803194189216, th=0.14\n",
      "found better score:0.40209582745322736, th=0.15\n",
      "found better score:0.41413586917801515, th=0.16\n",
      "found better score:0.42168761021819906, th=0.17\n",
      "found better score:0.4268687517719549, th=0.18\n",
      "found better score:0.45179774191906286, th=0.19\n",
      "found better score:0.45477253510860993, th=0.2\n",
      "found better score:0.4849901407190725, th=0.21\n",
      "found better score:0.49602133400129295, th=0.22\n",
      "found better score:0.5239987409133151, th=0.23\n",
      "found better score:0.5707072736576622, th=0.24\n",
      "found better score:0.5756083499062404, th=0.26\n",
      "found better score:0.594606059149878, th=0.28\n",
      "found better score:0.6011069249820091, th=0.29\n",
      "scores plot:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting test set\n",
      "6779/6779 [==============================] - 1s 184us/step\n",
      "Beginning fold 2\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 2s 937us/step - loss: 0.3098 - matthews_correlation_coeff: 0.0086 - val_loss: 0.2451 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation_coeff improved from -inf to 0.00000, saving model to weights.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.2438 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2440 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.2398 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2356 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.2394 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2344 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.2314 - matthews_correlation_coeff: 0.0880 - val_loss: 0.2356 - val_matthews_correlation_coeff: 0.1728\n",
      "\n",
      "Epoch 00005: val_matthews_correlation_coeff improved from 0.00000 to 0.17282, saving model to weights.h5\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.2262 - matthews_correlation_coeff: 0.1906 - val_loss: 0.2378 - val_matthews_correlation_coeff: 0.1297\n",
      "\n",
      "Epoch 00006: val_matthews_correlation_coeff did not improve from 0.17282\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 1s 497us/step - loss: 0.2310 - matthews_correlation_coeff: 0.1479 - val_loss: 0.2359 - val_matthews_correlation_coeff: 0.1616\n",
      "\n",
      "Epoch 00007: val_matthews_correlation_coeff did not improve from 0.17282\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 1s 493us/step - loss: 0.2308 - matthews_correlation_coeff: 0.2799 - val_loss: 0.2309 - val_matthews_correlation_coeff: 0.1579\n",
      "\n",
      "Epoch 00008: val_matthews_correlation_coeff did not improve from 0.17282\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2234 - matthews_correlation_coeff: 0.2497 - val_loss: 0.2269 - val_matthews_correlation_coeff: 0.1849\n",
      "\n",
      "Epoch 00009: val_matthews_correlation_coeff improved from 0.17282 to 0.18489, saving model to weights.h5\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 1s 497us/step - loss: 0.2225 - matthews_correlation_coeff: 0.2583 - val_loss: 0.2353 - val_matthews_correlation_coeff: 0.1116\n",
      "\n",
      "Epoch 00010: val_matthews_correlation_coeff did not improve from 0.18489\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.2300 - matthews_correlation_coeff: 0.2110 - val_loss: 0.2354 - val_matthews_correlation_coeff: 0.1003\n",
      "\n",
      "Epoch 00011: val_matthews_correlation_coeff did not improve from 0.18489\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 1s 495us/step - loss: 0.2260 - matthews_correlation_coeff: 0.1360 - val_loss: 0.2403 - val_matthews_correlation_coeff: 0.1707\n",
      "\n",
      "Epoch 00012: val_matthews_correlation_coeff did not improve from 0.18489\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 1s 494us/step - loss: 0.2222 - matthews_correlation_coeff: 0.2597 - val_loss: 0.2314 - val_matthews_correlation_coeff: 0.1796\n",
      "\n",
      "Epoch 00013: val_matthews_correlation_coeff did not improve from 0.18489\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.2197 - matthews_correlation_coeff: 0.2337 - val_loss: 0.2272 - val_matthews_correlation_coeff: 0.1297\n",
      "\n",
      "Epoch 00014: val_matthews_correlation_coeff did not improve from 0.18489\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.2130 - matthews_correlation_coeff: 0.2792 - val_loss: 0.2406 - val_matthews_correlation_coeff: 0.1598\n",
      "\n",
      "Epoch 00015: val_matthews_correlation_coeff did not improve from 0.18489\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 1s 461us/step - loss: 0.2212 - matthews_correlation_coeff: 0.2333 - val_loss: 0.2230 - val_matthews_correlation_coeff: 0.2049\n",
      "\n",
      "Epoch 00016: val_matthews_correlation_coeff improved from 0.18489 to 0.20489, saving model to weights.h5\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 1s 469us/step - loss: 0.2073 - matthews_correlation_coeff: 0.2908 - val_loss: 0.2236 - val_matthews_correlation_coeff: 0.1264\n",
      "\n",
      "Epoch 00017: val_matthews_correlation_coeff did not improve from 0.20489\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 1s 466us/step - loss: 0.2008 - matthews_correlation_coeff: 0.3141 - val_loss: 0.2136 - val_matthews_correlation_coeff: 0.1863\n",
      "\n",
      "Epoch 00018: val_matthews_correlation_coeff did not improve from 0.20489\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 1s 463us/step - loss: 0.2046 - matthews_correlation_coeff: 0.3371 - val_loss: 0.2315 - val_matthews_correlation_coeff: 0.2183\n",
      "\n",
      "Epoch 00019: val_matthews_correlation_coeff improved from 0.20489 to 0.21828, saving model to weights.h5\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 1s 465us/step - loss: 0.2157 - matthews_correlation_coeff: 0.2785 - val_loss: 0.2261 - val_matthews_correlation_coeff: 0.2049\n",
      "\n",
      "Epoch 00020: val_matthews_correlation_coeff did not improve from 0.21828\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 1s 460us/step - loss: 0.2022 - matthews_correlation_coeff: 0.2868 - val_loss: 0.2092 - val_matthews_correlation_coeff: 0.2056\n",
      "\n",
      "Epoch 00021: val_matthews_correlation_coeff did not improve from 0.21828\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 1s 463us/step - loss: 0.1890 - matthews_correlation_coeff: 0.3457 - val_loss: 0.2111 - val_matthews_correlation_coeff: 0.1816\n",
      "\n",
      "Epoch 00022: val_matthews_correlation_coeff did not improve from 0.21828\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 1s 471us/step - loss: 0.2052 - matthews_correlation_coeff: 0.2654 - val_loss: 0.2137 - val_matthews_correlation_coeff: 0.2096\n",
      "\n",
      "Epoch 00023: val_matthews_correlation_coeff did not improve from 0.21828\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 1s 466us/step - loss: 0.1745 - matthews_correlation_coeff: 0.3155 - val_loss: 0.1618 - val_matthews_correlation_coeff: 0.4333\n",
      "\n",
      "Epoch 00024: val_matthews_correlation_coeff improved from 0.21828 to 0.43326, saving model to weights.h5\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 1s 487us/step - loss: 0.2194 - matthews_correlation_coeff: 0.3732 - val_loss: 0.2265 - val_matthews_correlation_coeff: 0.2096\n",
      "\n",
      "Epoch 00025: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2053 - matthews_correlation_coeff: 0.2598 - val_loss: 0.2205 - val_matthews_correlation_coeff: 0.1968\n",
      "\n",
      "Epoch 00026: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 1s 494us/step - loss: 0.1893 - matthews_correlation_coeff: 0.3317 - val_loss: 0.2618 - val_matthews_correlation_coeff: 0.1863\n",
      "\n",
      "Epoch 00027: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 1s 495us/step - loss: 0.2199 - matthews_correlation_coeff: 0.2869 - val_loss: 0.2293 - val_matthews_correlation_coeff: 0.1499\n",
      "\n",
      "Epoch 00028: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 1s 497us/step - loss: 0.2035 - matthews_correlation_coeff: 0.3507 - val_loss: 0.2007 - val_matthews_correlation_coeff: 0.2261\n",
      "\n",
      "Epoch 00029: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.1821 - matthews_correlation_coeff: 0.2867 - val_loss: 0.2175 - val_matthews_correlation_coeff: 0.2989\n",
      "\n",
      "Epoch 00030: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.1792 - matthews_correlation_coeff: 0.3850 - val_loss: 0.2759 - val_matthews_correlation_coeff: 0.1361\n",
      "\n",
      "Epoch 00031: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2278 - matthews_correlation_coeff: 0.1824 - val_loss: 0.2140 - val_matthews_correlation_coeff: 0.1288\n",
      "\n",
      "Epoch 00032: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.1879 - matthews_correlation_coeff: 0.1826 - val_loss: 0.1796 - val_matthews_correlation_coeff: 0.3568\n",
      "\n",
      "Epoch 00033: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.1710 - matthews_correlation_coeff: 0.4865 - val_loss: 0.1934 - val_matthews_correlation_coeff: 0.3946\n",
      "\n",
      "Epoch 00034: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.1853 - matthews_correlation_coeff: 0.4063 - val_loss: 0.1694 - val_matthews_correlation_coeff: 0.3209\n",
      "\n",
      "Epoch 00035: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2181 - matthews_correlation_coeff: 0.1231 - val_loss: 0.2078 - val_matthews_correlation_coeff: 0.0502\n",
      "\n",
      "Epoch 00036: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.1814 - matthews_correlation_coeff: 0.1768 - val_loss: 0.1807 - val_matthews_correlation_coeff: 0.2857\n",
      "\n",
      "Epoch 00037: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2203 - matthews_correlation_coeff: 0.2869 - val_loss: 0.2213 - val_matthews_correlation_coeff: 0.0382\n",
      "\n",
      "Epoch 00038: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.2096 - matthews_correlation_coeff: 0.2233 - val_loss: 0.2233 - val_matthews_correlation_coeff: 0.1847\n",
      "\n",
      "Epoch 00039: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 1s 494us/step - loss: 0.1982 - matthews_correlation_coeff: 0.2710 - val_loss: 0.2241 - val_matthews_correlation_coeff: 0.2096\n",
      "\n",
      "Epoch 00040: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.1774 - matthews_correlation_coeff: 0.3489 - val_loss: 0.1671 - val_matthews_correlation_coeff: 0.3048\n",
      "\n",
      "Epoch 00041: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.1559 - matthews_correlation_coeff: 0.4464 - val_loss: 0.1642 - val_matthews_correlation_coeff: 0.3985\n",
      "\n",
      "Epoch 00042: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.1541 - matthews_correlation_coeff: 0.4223 - val_loss: 0.2129 - val_matthews_correlation_coeff: 0.3011\n",
      "\n",
      "Epoch 00043: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 1s 497us/step - loss: 0.2594 - matthews_correlation_coeff: 0.4166 - val_loss: 0.3263 - val_matthews_correlation_coeff: 0.1268\n",
      "\n",
      "Epoch 00044: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 1s 497us/step - loss: 0.2362 - matthews_correlation_coeff: 0.3043 - val_loss: 0.2470 - val_matthews_correlation_coeff: 0.2073\n",
      "\n",
      "Epoch 00045: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 1s 497us/step - loss: 0.2134 - matthews_correlation_coeff: 0.3482 - val_loss: 0.2422 - val_matthews_correlation_coeff: 0.2096\n",
      "\n",
      "Epoch 00046: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 1s 496us/step - loss: 0.2093 - matthews_correlation_coeff: 0.3306 - val_loss: 0.2367 - val_matthews_correlation_coeff: 0.2096\n",
      "\n",
      "Epoch 00047: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.1990 - matthews_correlation_coeff: 0.3233 - val_loss: 0.2240 - val_matthews_correlation_coeff: 0.2096\n",
      "\n",
      "Epoch 00048: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 1s 487us/step - loss: 0.1873 - matthews_correlation_coeff: 0.3962 - val_loss: 0.2057 - val_matthews_correlation_coeff: 0.1991\n",
      "\n",
      "Epoch 00049: val_matthews_correlation_coeff did not improve from 0.43326\n",
      "Epoch 00049: early stopping\n",
      "finding threshold\n",
      "found better score:0.02962277460144054, th=0.01\n",
      "found better score:0.23876369845783288, th=0.02\n",
      "found better score:0.38022303133719676, th=0.03\n",
      "found better score:0.41544995326824524, th=0.04\n",
      "found better score:0.43103219835047596, th=0.06\n",
      "found better score:0.4433270868653302, th=0.07\n",
      "found better score:0.44586821725321424, th=0.08\n",
      "found better score:0.44843809171596183, th=0.09\n",
      "found better score:0.45366656148551304, th=0.1\n",
      "found better score:0.45901766357650714, th=0.11\n",
      "found better score:0.4638303539660723, th=0.22\n",
      "found better score:0.4677085546794111, th=0.23\n",
      "found better score:0.47282304397569375, th=0.24\n",
      "found better score:0.5001091958965107, th=0.25\n",
      "scores plot:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting test set\n",
      "6779/6779 [==============================] - 1s 143us/step\n",
      "Beginning fold 3\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 2s 979us/step - loss: 0.2863 - matthews_correlation_coeff: -0.0058 - val_loss: 0.2474 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation_coeff improved from -inf to 0.00000, saving model to weights.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 1s 467us/step - loss: 0.2432 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2462 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 1s 472us/step - loss: 0.2416 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2447 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 1s 467us/step - loss: 0.2343 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2445 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 1s 472us/step - loss: 0.2261 - matthews_correlation_coeff: 0.2565 - val_loss: 0.2383 - val_matthews_correlation_coeff: 0.0995\n",
      "\n",
      "Epoch 00005: val_matthews_correlation_coeff improved from 0.00000 to 0.09954, saving model to weights.h5\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 1s 467us/step - loss: 0.2247 - matthews_correlation_coeff: 0.2479 - val_loss: 0.2377 - val_matthews_correlation_coeff: 0.0513\n",
      "\n",
      "Epoch 00006: val_matthews_correlation_coeff did not improve from 0.09954\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 1s 471us/step - loss: 0.2228 - matthews_correlation_coeff: 0.2832 - val_loss: 0.2340 - val_matthews_correlation_coeff: -0.0047\n",
      "\n",
      "Epoch 00007: val_matthews_correlation_coeff did not improve from 0.09954\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.2146 - matthews_correlation_coeff: 0.2846 - val_loss: 0.2513 - val_matthews_correlation_coeff: 0.1026\n",
      "\n",
      "Epoch 00008: val_matthews_correlation_coeff improved from 0.09954 to 0.10261, saving model to weights.h5\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2296 - matthews_correlation_coeff: 0.1008 - val_loss: 0.2293 - val_matthews_correlation_coeff: -0.0047\n",
      "\n",
      "Epoch 00009: val_matthews_correlation_coeff did not improve from 0.10261\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.2085 - matthews_correlation_coeff: 0.2170 - val_loss: 0.2402 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00010: val_matthews_correlation_coeff did not improve from 0.10261\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 1s 510us/step - loss: 0.2082 - matthews_correlation_coeff: 0.2718 - val_loss: 0.2442 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00011: val_matthews_correlation_coeff did not improve from 0.10261\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2353 - matthews_correlation_coeff: 0.0398 - val_loss: 0.2410 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_matthews_correlation_coeff did not improve from 0.10261\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2312 - matthews_correlation_coeff: 0.1608 - val_loss: 0.2490 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_matthews_correlation_coeff did not improve from 0.10261\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.2290 - matthews_correlation_coeff: -0.0013 - val_loss: 0.2782 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_matthews_correlation_coeff did not improve from 0.10261\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2339 - matthews_correlation_coeff: 0.0956 - val_loss: 0.2315 - val_matthews_correlation_coeff: 0.0813\n",
      "\n",
      "Epoch 00015: val_matthews_correlation_coeff did not improve from 0.10261\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.2083 - matthews_correlation_coeff: 0.2486 - val_loss: 0.2394 - val_matthews_correlation_coeff: 0.1676\n",
      "\n",
      "Epoch 00016: val_matthews_correlation_coeff improved from 0.10261 to 0.16758, saving model to weights.h5\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.2211 - matthews_correlation_coeff: 0.2359 - val_loss: 0.2278 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00017: val_matthews_correlation_coeff did not improve from 0.16758\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.2152 - matthews_correlation_coeff: 0.2928 - val_loss: 0.2457 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00018: val_matthews_correlation_coeff did not improve from 0.16758\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.2074 - matthews_correlation_coeff: 0.2723 - val_loss: 0.2293 - val_matthews_correlation_coeff: 0.1151\n",
      "\n",
      "Epoch 00019: val_matthews_correlation_coeff did not improve from 0.16758\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.2270 - matthews_correlation_coeff: 0.1743 - val_loss: 0.2273 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00020: val_matthews_correlation_coeff did not improve from 0.16758\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2080 - matthews_correlation_coeff: 0.2872 - val_loss: 0.2214 - val_matthews_correlation_coeff: 0.1909\n",
      "\n",
      "Epoch 00021: val_matthews_correlation_coeff improved from 0.16758 to 0.19086, saving model to weights.h5\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.2026 - matthews_correlation_coeff: 0.2216 - val_loss: 0.2218 - val_matthews_correlation_coeff: 0.1909\n",
      "\n",
      "Epoch 00022: val_matthews_correlation_coeff did not improve from 0.19086\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.2000 - matthews_correlation_coeff: 0.3116 - val_loss: 0.2295 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00023: val_matthews_correlation_coeff did not improve from 0.19086\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2108 - matthews_correlation_coeff: 0.2663 - val_loss: 0.2220 - val_matthews_correlation_coeff: 0.1783\n",
      "\n",
      "Epoch 00024: val_matthews_correlation_coeff did not improve from 0.19086\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2035 - matthews_correlation_coeff: 0.2040 - val_loss: 0.2113 - val_matthews_correlation_coeff: 0.0919\n",
      "\n",
      "Epoch 00025: val_matthews_correlation_coeff did not improve from 0.19086\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1784 - matthews_correlation_coeff: 0.3685 - val_loss: 0.1780 - val_matthews_correlation_coeff: 0.2821\n",
      "\n",
      "Epoch 00026: val_matthews_correlation_coeff improved from 0.19086 to 0.28210, saving model to weights.h5\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.1969 - matthews_correlation_coeff: 0.3523 - val_loss: 0.2243 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00027: val_matthews_correlation_coeff did not improve from 0.28210\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.1920 - matthews_correlation_coeff: 0.3279 - val_loss: 0.2244 - val_matthews_correlation_coeff: 0.1967\n",
      "\n",
      "Epoch 00028: val_matthews_correlation_coeff did not improve from 0.28210\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.2010 - matthews_correlation_coeff: 0.3399 - val_loss: 0.2018 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00029: val_matthews_correlation_coeff did not improve from 0.28210\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.1720 - matthews_correlation_coeff: 0.3693 - val_loss: 0.1783 - val_matthews_correlation_coeff: 0.3034\n",
      "\n",
      "Epoch 00030: val_matthews_correlation_coeff improved from 0.28210 to 0.30343, saving model to weights.h5\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.1558 - matthews_correlation_coeff: 0.4450 - val_loss: 0.2133 - val_matthews_correlation_coeff: 0.2430\n",
      "\n",
      "Epoch 00031: val_matthews_correlation_coeff did not improve from 0.30343\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.2142 - matthews_correlation_coeff: 0.3099 - val_loss: 0.2255 - val_matthews_correlation_coeff: 0.1151\n",
      "\n",
      "Epoch 00032: val_matthews_correlation_coeff did not improve from 0.30343\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.1967 - matthews_correlation_coeff: 0.2944 - val_loss: 0.1848 - val_matthews_correlation_coeff: 0.3564\n",
      "\n",
      "Epoch 00033: val_matthews_correlation_coeff improved from 0.30343 to 0.35644, saving model to weights.h5\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.1626 - matthews_correlation_coeff: 0.4804 - val_loss: 0.1553 - val_matthews_correlation_coeff: 0.2073\n",
      "\n",
      "Epoch 00034: val_matthews_correlation_coeff did not improve from 0.35644\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.1479 - matthews_correlation_coeff: 0.3838 - val_loss: 0.1485 - val_matthews_correlation_coeff: 0.4392\n",
      "\n",
      "Epoch 00035: val_matthews_correlation_coeff improved from 0.35644 to 0.43918, saving model to weights.h5\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.1547 - matthews_correlation_coeff: 0.4568 - val_loss: 0.2454 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00036: val_matthews_correlation_coeff did not improve from 0.43918\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 1s 509us/step - loss: 0.1804 - matthews_correlation_coeff: 0.3864 - val_loss: 0.1758 - val_matthews_correlation_coeff: 0.3341\n",
      "\n",
      "Epoch 00037: val_matthews_correlation_coeff did not improve from 0.43918\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.1488 - matthews_correlation_coeff: 0.5211 - val_loss: 0.1754 - val_matthews_correlation_coeff: 0.4421\n",
      "\n",
      "Epoch 00038: val_matthews_correlation_coeff improved from 0.43918 to 0.44206, saving model to weights.h5\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.1464 - matthews_correlation_coeff: 0.5346 - val_loss: 0.1797 - val_matthews_correlation_coeff: 0.4302\n",
      "\n",
      "Epoch 00039: val_matthews_correlation_coeff did not improve from 0.44206\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.1431 - matthews_correlation_coeff: 0.5743 - val_loss: 0.1654 - val_matthews_correlation_coeff: 0.1151\n",
      "\n",
      "Epoch 00040: val_matthews_correlation_coeff did not improve from 0.44206\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 1s 505us/step - loss: 0.1627 - matthews_correlation_coeff: 0.4474 - val_loss: 0.2757 - val_matthews_correlation_coeff: 0.1909\n",
      "\n",
      "Epoch 00041: val_matthews_correlation_coeff did not improve from 0.44206\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.1898 - matthews_correlation_coeff: 0.4260 - val_loss: 0.1620 - val_matthews_correlation_coeff: 0.4637\n",
      "\n",
      "Epoch 00042: val_matthews_correlation_coeff improved from 0.44206 to 0.46365, saving model to weights.h5\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 1s 508us/step - loss: 0.1970 - matthews_correlation_coeff: 0.2486 - val_loss: 0.2366 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00043: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2296 - matthews_correlation_coeff: 0.0243 - val_loss: 0.2125 - val_matthews_correlation_coeff: 0.0813\n",
      "\n",
      "Epoch 00044: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.1980 - matthews_correlation_coeff: 0.1598 - val_loss: 0.2114 - val_matthews_correlation_coeff: 0.2285\n",
      "\n",
      "Epoch 00045: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2021 - matthews_correlation_coeff: 0.2659 - val_loss: 0.2018 - val_matthews_correlation_coeff: 0.2670\n",
      "\n",
      "Epoch 00046: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.1927 - matthews_correlation_coeff: 0.3129 - val_loss: 0.2126 - val_matthews_correlation_coeff: 0.2496\n",
      "\n",
      "Epoch 00047: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1775 - matthews_correlation_coeff: 0.3524 - val_loss: 0.1903 - val_matthews_correlation_coeff: 0.2894\n",
      "\n",
      "Epoch 00048: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1611 - matthews_correlation_coeff: 0.4284 - val_loss: 0.4267 - val_matthews_correlation_coeff: 0.4232\n",
      "\n",
      "Epoch 00049: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 1s 506us/step - loss: 0.2144 - matthews_correlation_coeff: 0.3270 - val_loss: 0.2182 - val_matthews_correlation_coeff: 0.0544\n",
      "\n",
      "Epoch 00050: val_matthews_correlation_coeff did not improve from 0.46365\n",
      "finding threshold\n",
      "found better score:0.03549887631905916, th=0.01\n",
      "found better score:0.21545594398244133, th=0.02\n",
      "found better score:0.41191339795487747, th=0.03\n",
      "found better score:0.534596262160952, th=0.04\n",
      "found better score:0.5358448793514854, th=0.05\n",
      "found better score:0.5868638131988603, th=0.06\n",
      "found better score:0.5991494281359356, th=0.07\n",
      "found better score:0.6072763406282706, th=0.08\n",
      "scores plot:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting test set\n",
      "6779/6779 [==============================] - 1s 183us/step\n",
      "Beginning fold 4\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 2s 933us/step - loss: 0.3011 - matthews_correlation_coeff: 0.0014 - val_loss: 0.2569 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation_coeff improved from -inf to 0.00000, saving model to weights.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.2454 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2432 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 1s 495us/step - loss: 0.2403 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2369 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2383 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2300 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 1s 497us/step - loss: 0.2347 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2403 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.2346 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2373 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.2355 - matthews_correlation_coeff: 0.0316 - val_loss: 0.2297 - val_matthews_correlation_coeff: 0.0608\n",
      "\n",
      "Epoch 00007: val_matthews_correlation_coeff improved from 0.00000 to 0.06078, saving model to weights.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2273 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2455 - val_matthews_correlation_coeff: 0.1788\n",
      "\n",
      "Epoch 00008: val_matthews_correlation_coeff improved from 0.06078 to 0.17881, saving model to weights.h5\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 1s 496us/step - loss: 0.2281 - matthews_correlation_coeff: 0.1862 - val_loss: 0.2241 - val_matthews_correlation_coeff: 0.1491\n",
      "\n",
      "Epoch 00009: val_matthews_correlation_coeff did not improve from 0.17881\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2152 - matthews_correlation_coeff: 0.1915 - val_loss: 0.2074 - val_matthews_correlation_coeff: 0.1910\n",
      "\n",
      "Epoch 00010: val_matthews_correlation_coeff improved from 0.17881 to 0.19100, saving model to weights.h5\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2153 - matthews_correlation_coeff: 0.1855 - val_loss: 0.2302 - val_matthews_correlation_coeff: 0.1021\n",
      "\n",
      "Epoch 00011: val_matthews_correlation_coeff did not improve from 0.19100\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.2160 - matthews_correlation_coeff: 0.1979 - val_loss: 0.2267 - val_matthews_correlation_coeff: 0.2791\n",
      "\n",
      "Epoch 00012: val_matthews_correlation_coeff improved from 0.19100 to 0.27910, saving model to weights.h5\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 1s 484us/step - loss: 0.2203 - matthews_correlation_coeff: 0.2381 - val_loss: 0.2109 - val_matthews_correlation_coeff: 0.2371\n",
      "\n",
      "Epoch 00013: val_matthews_correlation_coeff did not improve from 0.27910\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 1s 469us/step - loss: 0.2050 - matthews_correlation_coeff: 0.2283 - val_loss: 0.2200 - val_matthews_correlation_coeff: 0.1835\n",
      "\n",
      "Epoch 00014: val_matthews_correlation_coeff did not improve from 0.27910\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 1s 466us/step - loss: 0.2206 - matthews_correlation_coeff: 0.2237 - val_loss: 0.2087 - val_matthews_correlation_coeff: 0.1788\n",
      "\n",
      "Epoch 00015: val_matthews_correlation_coeff did not improve from 0.27910\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 1s 468us/step - loss: 0.2008 - matthews_correlation_coeff: 0.2726 - val_loss: 0.2172 - val_matthews_correlation_coeff: 0.2412\n",
      "\n",
      "Epoch 00016: val_matthews_correlation_coeff did not improve from 0.27910\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 1s 470us/step - loss: 0.1739 - matthews_correlation_coeff: 0.3369 - val_loss: 0.2698 - val_matthews_correlation_coeff: 0.4146\n",
      "\n",
      "Epoch 00017: val_matthews_correlation_coeff improved from 0.27910 to 0.41459, saving model to weights.h5\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 1s 466us/step - loss: 0.2087 - matthews_correlation_coeff: 0.2714 - val_loss: 0.1714 - val_matthews_correlation_coeff: 0.2093\n",
      "\n",
      "Epoch 00018: val_matthews_correlation_coeff did not improve from 0.41459\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 1s 463us/step - loss: 0.1786 - matthews_correlation_coeff: 0.3510 - val_loss: 0.1938 - val_matthews_correlation_coeff: 0.2849\n",
      "\n",
      "Epoch 00019: val_matthews_correlation_coeff did not improve from 0.41459\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 1s 464us/step - loss: 0.1730 - matthews_correlation_coeff: 0.4009 - val_loss: 0.1676 - val_matthews_correlation_coeff: 0.5133\n",
      "\n",
      "Epoch 00020: val_matthews_correlation_coeff improved from 0.41459 to 0.51330, saving model to weights.h5\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 1s 462us/step - loss: 0.1881 - matthews_correlation_coeff: 0.3807 - val_loss: 0.1985 - val_matthews_correlation_coeff: 0.2390\n",
      "\n",
      "Epoch 00021: val_matthews_correlation_coeff did not improve from 0.51330\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 1s 463us/step - loss: 0.1914 - matthews_correlation_coeff: 0.3487 - val_loss: 0.1692 - val_matthews_correlation_coeff: 0.2471\n",
      "\n",
      "Epoch 00022: val_matthews_correlation_coeff did not improve from 0.51330\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.1578 - matthews_correlation_coeff: 0.4473 - val_loss: 0.2424 - val_matthews_correlation_coeff: 0.2306\n",
      "\n",
      "Epoch 00023: val_matthews_correlation_coeff did not improve from 0.51330\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.1924 - matthews_correlation_coeff: 0.2469 - val_loss: 0.2038 - val_matthews_correlation_coeff: 0.3798\n",
      "\n",
      "Epoch 00024: val_matthews_correlation_coeff did not improve from 0.51330\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.1743 - matthews_correlation_coeff: 0.3675 - val_loss: 0.1553 - val_matthews_correlation_coeff: 0.3808\n",
      "\n",
      "Epoch 00025: val_matthews_correlation_coeff did not improve from 0.51330\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.1590 - matthews_correlation_coeff: 0.4175 - val_loss: 0.1713 - val_matthews_correlation_coeff: 0.4676\n",
      "\n",
      "Epoch 00026: val_matthews_correlation_coeff did not improve from 0.51330\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 1s 507us/step - loss: 0.1358 - matthews_correlation_coeff: 0.5890 - val_loss: 0.1328 - val_matthews_correlation_coeff: 0.6064\n",
      "\n",
      "Epoch 00027: val_matthews_correlation_coeff improved from 0.51330 to 0.60639, saving model to weights.h5\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 1s 502us/step - loss: 0.2238 - matthews_correlation_coeff: 0.2442 - val_loss: 0.2257 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.1743 - matthews_correlation_coeff: 0.0349 - val_loss: 0.1410 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.1728 - matthews_correlation_coeff: 0.2841 - val_loss: 0.1953 - val_matthews_correlation_coeff: 0.2252\n",
      "\n",
      "Epoch 00030: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.1529 - matthews_correlation_coeff: 0.3125 - val_loss: 0.1266 - val_matthews_correlation_coeff: 0.4748\n",
      "\n",
      "Epoch 00031: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.1332 - matthews_correlation_coeff: 0.5869 - val_loss: 0.1666 - val_matthews_correlation_coeff: 0.3987\n",
      "\n",
      "Epoch 00032: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.2203 - matthews_correlation_coeff: 0.4355 - val_loss: 0.2073 - val_matthews_correlation_coeff: 0.2194\n",
      "\n",
      "Epoch 00033: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.2158 - matthews_correlation_coeff: 0.2375 - val_loss: 0.2115 - val_matthews_correlation_coeff: 0.1743\n",
      "\n",
      "Epoch 00034: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.2118 - matthews_correlation_coeff: 0.1574 - val_loss: 0.2071 - val_matthews_correlation_coeff: 0.2510\n",
      "\n",
      "Epoch 00035: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.1905 - matthews_correlation_coeff: 0.2849 - val_loss: 0.1906 - val_matthews_correlation_coeff: 0.2621\n",
      "\n",
      "Epoch 00036: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.1964 - matthews_correlation_coeff: 0.2618 - val_loss: 0.1631 - val_matthews_correlation_coeff: 0.2740\n",
      "\n",
      "Epoch 00037: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.1707 - matthews_correlation_coeff: 0.4041 - val_loss: 0.1417 - val_matthews_correlation_coeff: 0.3024\n",
      "\n",
      "Epoch 00038: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.1783 - matthews_correlation_coeff: 0.4424 - val_loss: 0.2809 - val_matthews_correlation_coeff: 0.1743\n",
      "\n",
      "Epoch 00039: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.2557 - matthews_correlation_coeff: 0.0867 - val_loss: 0.2212 - val_matthews_correlation_coeff: 0.0737\n",
      "\n",
      "Epoch 00040: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.2026 - matthews_correlation_coeff: 0.1702 - val_loss: 0.2049 - val_matthews_correlation_coeff: 0.3373\n",
      "\n",
      "Epoch 00041: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 1s 499us/step - loss: 0.1790 - matthews_correlation_coeff: 0.2824 - val_loss: 0.1995 - val_matthews_correlation_coeff: 0.4983\n",
      "\n",
      "Epoch 00042: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.1613 - matthews_correlation_coeff: 0.4117 - val_loss: 0.1454 - val_matthews_correlation_coeff: 0.4540\n",
      "\n",
      "Epoch 00043: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 1s 498us/step - loss: 0.2339 - matthews_correlation_coeff: 0.3450 - val_loss: 0.1853 - val_matthews_correlation_coeff: 0.1623\n",
      "\n",
      "Epoch 00044: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.2085 - matthews_correlation_coeff: 0.2587 - val_loss: 0.1917 - val_matthews_correlation_coeff: 0.2371\n",
      "\n",
      "Epoch 00045: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.1720 - matthews_correlation_coeff: 0.3405 - val_loss: 0.1484 - val_matthews_correlation_coeff: 0.4589\n",
      "\n",
      "Epoch 00046: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 1s 504us/step - loss: 0.1400 - matthews_correlation_coeff: 0.5336 - val_loss: 0.1348 - val_matthews_correlation_coeff: 0.5030\n",
      "\n",
      "Epoch 00047: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 1s 503us/step - loss: 0.1275 - matthews_correlation_coeff: 0.6161 - val_loss: 0.4401 - val_matthews_correlation_coeff: 0.3366\n",
      "\n",
      "Epoch 00048: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 1s 500us/step - loss: 0.2472 - matthews_correlation_coeff: 0.1600 - val_loss: 0.1970 - val_matthews_correlation_coeff: 0.0638\n",
      "\n",
      "Epoch 00049: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 1s 501us/step - loss: 0.1796 - matthews_correlation_coeff: 0.0640 - val_loss: 0.1552 - val_matthews_correlation_coeff: 0.1513\n",
      "\n",
      "Epoch 00050: val_matthews_correlation_coeff did not improve from 0.60639\n",
      "finding threshold\n",
      "found better score:0.19791975258909697, th=0.01\n",
      "found better score:0.36201148488018514, th=0.02\n",
      "found better score:0.47364586729513397, th=0.03\n",
      "found better score:0.5467138501079843, th=0.04\n",
      "found better score:0.6038670314970938, th=0.05\n",
      "found better score:0.6415954661637625, th=0.07\n",
      "found better score:0.6628218931717514, th=0.08\n",
      "found better score:0.6690176834418132, th=0.09\n",
      "scores plot:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting test set\n",
      "6779/6779 [==============================] - 1s 142us/step\n",
      "Beginning fold 5\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 2s 931us/step - loss: 0.2963 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2801 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation_coeff improved from -inf to 0.00000, saving model to weights.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 1s 498us/step - loss: 0.2522 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2371 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.2377 - matthews_correlation_coeff: 0.0000e+00 - val_loss: 0.2450 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation_coeff did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 1s 498us/step - loss: 0.2387 - matthews_correlation_coeff: 0.0385 - val_loss: 0.2354 - val_matthews_correlation_coeff: 0.1210\n",
      "\n",
      "Epoch 00004: val_matthews_correlation_coeff improved from 0.00000 to 0.12102, saving model to weights.h5\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 1s 512us/step - loss: 0.2359 - matthews_correlation_coeff: -8.7779e-04 - val_loss: 0.2447 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation_coeff did not improve from 0.12102\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 1s 498us/step - loss: 0.2316 - matthews_correlation_coeff: 0.1237 - val_loss: 0.2294 - val_matthews_correlation_coeff: 0.1436\n",
      "\n",
      "Epoch 00006: val_matthews_correlation_coeff improved from 0.12102 to 0.14358, saving model to weights.h5\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 1s 496us/step - loss: 0.2267 - matthews_correlation_coeff: 0.1958 - val_loss: 0.2229 - val_matthews_correlation_coeff: 0.1222\n",
      "\n",
      "Epoch 00007: val_matthews_correlation_coeff did not improve from 0.14358\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.2300 - matthews_correlation_coeff: 0.1736 - val_loss: 0.2252 - val_matthews_correlation_coeff: 0.0537\n",
      "\n",
      "Epoch 00008: val_matthews_correlation_coeff did not improve from 0.14358\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 1s 497us/step - loss: 0.2244 - matthews_correlation_coeff: 0.1942 - val_loss: 0.2302 - val_matthews_correlation_coeff: 0.0763\n",
      "\n",
      "Epoch 00009: val_matthews_correlation_coeff did not improve from 0.14358\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 1s 497us/step - loss: 0.2164 - matthews_correlation_coeff: 0.2019 - val_loss: 0.1975 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00010: val_matthews_correlation_coeff improved from 0.14358 to 0.16111, saving model to weights.h5\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.2139 - matthews_correlation_coeff: 0.2350 - val_loss: 0.2506 - val_matthews_correlation_coeff: 0.1624\n",
      "\n",
      "Epoch 00011: val_matthews_correlation_coeff improved from 0.16111 to 0.16241, saving model to weights.h5\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 1s 499us/step - loss: 0.2097 - matthews_correlation_coeff: 0.3019 - val_loss: 0.1991 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00012: val_matthews_correlation_coeff did not improve from 0.16241\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 1s 502us/step - loss: 0.1645 - matthews_correlation_coeff: 0.3634 - val_loss: 0.1983 - val_matthews_correlation_coeff: 0.2576\n",
      "\n",
      "Epoch 00013: val_matthews_correlation_coeff improved from 0.16241 to 0.25765, saving model to weights.h5\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.2422 - matthews_correlation_coeff: 0.2563 - val_loss: 0.2184 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00014: val_matthews_correlation_coeff did not improve from 0.25765\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 1s 498us/step - loss: 0.2186 - matthews_correlation_coeff: 0.2827 - val_loss: 0.2204 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00015: val_matthews_correlation_coeff did not improve from 0.25765\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.2103 - matthews_correlation_coeff: 0.2944 - val_loss: 0.2140 - val_matthews_correlation_coeff: 0.1761\n",
      "\n",
      "Epoch 00016: val_matthews_correlation_coeff did not improve from 0.25765\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 1s 507us/step - loss: 0.2110 - matthews_correlation_coeff: 0.3507 - val_loss: 0.1903 - val_matthews_correlation_coeff: 0.1807\n",
      "\n",
      "Epoch 00017: val_matthews_correlation_coeff did not improve from 0.25765\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.1602 - matthews_correlation_coeff: 0.3832 - val_loss: 0.1696 - val_matthews_correlation_coeff: 0.2815\n",
      "\n",
      "Epoch 00018: val_matthews_correlation_coeff improved from 0.25765 to 0.28146, saving model to weights.h5\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 1s 499us/step - loss: 0.2628 - matthews_correlation_coeff: 0.2832 - val_loss: 0.2305 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.2303 - matthews_correlation_coeff: 0.0159 - val_loss: 0.2275 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.2248 - matthews_correlation_coeff: 0.1165 - val_loss: 0.2321 - val_matthews_correlation_coeff: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.2213 - matthews_correlation_coeff: 0.1612 - val_loss: 0.2120 - val_matthews_correlation_coeff: 0.0673\n",
      "\n",
      "Epoch 00022: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.1993 - matthews_correlation_coeff: 0.2487 - val_loss: 0.1951 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00023: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 1s 503us/step - loss: 0.1897 - matthews_correlation_coeff: 0.3585 - val_loss: 0.1679 - val_matthews_correlation_coeff: 0.2091\n",
      "\n",
      "Epoch 00024: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.1966 - matthews_correlation_coeff: 0.3080 - val_loss: 0.1872 - val_matthews_correlation_coeff: 0.1542\n",
      "\n",
      "Epoch 00025: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 1s 498us/step - loss: 0.1650 - matthews_correlation_coeff: 0.3185 - val_loss: 0.2086 - val_matthews_correlation_coeff: 0.1542\n",
      "\n",
      "Epoch 00026: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 1s 496us/step - loss: 0.2078 - matthews_correlation_coeff: 0.3348 - val_loss: 0.2176 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00027: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.2118 - matthews_correlation_coeff: 0.3084 - val_loss: 0.1956 - val_matthews_correlation_coeff: 0.1761\n",
      "\n",
      "Epoch 00028: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 1s 486us/step - loss: 0.1856 - matthews_correlation_coeff: 0.3409 - val_loss: 0.1864 - val_matthews_correlation_coeff: 0.2126\n",
      "\n",
      "Epoch 00029: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 1s 464us/step - loss: 0.1814 - matthews_correlation_coeff: 0.3910 - val_loss: 0.2291 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00030: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 1s 462us/step - loss: 0.1645 - matthews_correlation_coeff: 0.4482 - val_loss: 0.1876 - val_matthews_correlation_coeff: 0.2510\n",
      "\n",
      "Epoch 00031: val_matthews_correlation_coeff did not improve from 0.28146\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 1s 463us/step - loss: 0.1316 - matthews_correlation_coeff: 0.5880 - val_loss: 0.2533 - val_matthews_correlation_coeff: 0.4582\n",
      "\n",
      "Epoch 00032: val_matthews_correlation_coeff improved from 0.28146 to 0.45822, saving model to weights.h5\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 1s 468us/step - loss: 0.1454 - matthews_correlation_coeff: 0.5631 - val_loss: 0.1457 - val_matthews_correlation_coeff: 0.4555\n",
      "\n",
      "Epoch 00033: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 1s 462us/step - loss: 0.1642 - matthews_correlation_coeff: 0.5243 - val_loss: 0.2725 - val_matthews_correlation_coeff: 0.1725\n",
      "\n",
      "Epoch 00034: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 1s 461us/step - loss: 0.2414 - matthews_correlation_coeff: 0.2452 - val_loss: 0.2325 - val_matthews_correlation_coeff: 0.1436\n",
      "\n",
      "Epoch 00035: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 1s 462us/step - loss: 0.2209 - matthews_correlation_coeff: 0.2303 - val_loss: 0.2188 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00036: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 1s 464us/step - loss: 0.2116 - matthews_correlation_coeff: 0.2361 - val_loss: 0.2142 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00037: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 1s 472us/step - loss: 0.2076 - matthews_correlation_coeff: 0.3215 - val_loss: 0.2096 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00038: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 1s 501us/step - loss: 0.2055 - matthews_correlation_coeff: 0.2958 - val_loss: 0.1963 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00039: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 1s 498us/step - loss: 0.2099 - matthews_correlation_coeff: 0.3142 - val_loss: 0.2008 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00040: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 1s 496us/step - loss: 0.1889 - matthews_correlation_coeff: 0.3495 - val_loss: 0.1749 - val_matthews_correlation_coeff: 0.2922\n",
      "\n",
      "Epoch 00041: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 1s 499us/step - loss: 0.1553 - matthews_correlation_coeff: 0.4217 - val_loss: 0.1636 - val_matthews_correlation_coeff: 0.4155\n",
      "\n",
      "Epoch 00042: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 1s 497us/step - loss: 0.2088 - matthews_correlation_coeff: 0.4084 - val_loss: 0.1854 - val_matthews_correlation_coeff: 0.1725\n",
      "\n",
      "Epoch 00043: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 1s 496us/step - loss: 0.1860 - matthews_correlation_coeff: 0.3112 - val_loss: 0.1788 - val_matthews_correlation_coeff: 0.2274\n",
      "\n",
      "Epoch 00044: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 1s 498us/step - loss: 0.1675 - matthews_correlation_coeff: 0.4333 - val_loss: 0.1790 - val_matthews_correlation_coeff: 0.3861\n",
      "\n",
      "Epoch 00045: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 1s 497us/step - loss: 0.1730 - matthews_correlation_coeff: 0.4115 - val_loss: 0.1631 - val_matthews_correlation_coeff: 0.2741\n",
      "\n",
      "Epoch 00046: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 1s 500us/step - loss: 0.1310 - matthews_correlation_coeff: 0.5128 - val_loss: 0.2301 - val_matthews_correlation_coeff: 0.4112\n",
      "\n",
      "Epoch 00047: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 1s 495us/step - loss: 0.1812 - matthews_correlation_coeff: 0.4712 - val_loss: 0.1835 - val_matthews_correlation_coeff: 0.3435\n",
      "\n",
      "Epoch 00048: val_matthews_correlation_coeff did not improve from 0.45822\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 1s 497us/step - loss: 0.1323 - matthews_correlation_coeff: 0.4920 - val_loss: 0.1383 - val_matthews_correlation_coeff: 0.5688\n",
      "\n",
      "Epoch 00049: val_matthews_correlation_coeff improved from 0.45822 to 0.56876, saving model to weights.h5\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 1s 499us/step - loss: 0.1406 - matthews_correlation_coeff: 0.5994 - val_loss: 0.2945 - val_matthews_correlation_coeff: 0.1611\n",
      "\n",
      "Epoch 00050: val_matthews_correlation_coeff did not improve from 0.56876\n",
      "finding threshold\n",
      "found better score:0.20868528796688968, th=0.01\n",
      "found better score:0.3510484815402529, th=0.02\n",
      "found better score:0.4777486704413052, th=0.03\n",
      "found better score:0.4963674592262522, th=0.04\n",
      "found better score:0.5507020329393801, th=0.05\n",
      "found better score:0.5626173321546888, th=0.06\n",
      "found better score:0.5754439002268076, th=0.07\n",
      "found better score:0.582120708622977, th=0.08\n",
      "found better score:0.6185126452851037, th=0.09\n",
      "found better score:0.6264690472662081, th=0.12\n",
      "found better score:0.6431629556605036, th=0.13\n",
      "found better score:0.6568971974293901, th=0.23\n",
      "found better score:0.6617994730013393, th=0.27\n",
      "scores plot:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting test set\n",
      "6779/6779 [==============================] - 1s 182us/step\n"
     ]
    }
   ],
   "source": [
    "eval_preds = np.zeros(X.shape[0])\n",
    "label_predictions = []\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=123).split(X, y))\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "\n",
    "    model = create_model(X)\n",
    "    #checkpoint to save model with best validation score. keras seems to add val_xxxxx as name for metric to use here\n",
    "    ckpt = ModelCheckpoint('weights.h5', save_best_only=True, save_weights_only=True, monitor='val_matthews_correlation_coeff', verbose=1, mode='max')\n",
    "    earlystopper = EarlyStopping(patience=25, verbose=1) \n",
    "    model.fit(train_X, train_y, batch_size=128, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt, earlystopper])\n",
    "    # loads the best weights saved by the checkpoint\n",
    "    model.load_weights('weights.h5')\n",
    "\n",
    "    print(\"finding threshold\")\n",
    "    predictions = model.predict(val_X, batch_size=512)\n",
    "    best_threshold = threshold_search(val_y, predictions)['threshold']\n",
    "    \n",
    "    print(\"predicting test set\")\n",
    "    pred = model.predict(X_test, batch_size=300, verbose=1)\n",
    "    pred_bool = pred > best_threshold\n",
    "    labels = pred_bool.astype(\"int32\")\n",
    "    label_predictions.append(labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc42a67794763aeb66832eadb82352e800d2a8d1",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7dca4cfe67e8eccdf536e3fbb26f9055a39d6e7",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4df7db6f6454cbc7a6740cc859e26753953ea04f"
   },
   "source": [
    "Convert the above predictions into suitable submission format for the competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "a086b1c9aa293e62ce75930e40e68e8068327958",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predictions = [pred.flatten() for pred in label_predictions]\n",
    "\n",
    "import scipy\n",
    "\n",
    "# Ensemble with voting\n",
    "labels = np.array(label_predictions)\n",
    "#convert list of predictions into set of columns\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "#take most common value (0 or 1) or each row\n",
    "labels = scipy.stats.mode(labels, axis=-1)[0]\n",
    "labels = np.squeeze(labels)\n",
    "\n",
    "submission = pd.read_csv('../input/vsb-power-line-fault-detection/sample_submission.csv')\n",
    "labels3 = np.repeat(labels, 3)\n",
    "submission['target'] = labels3\n",
    "submission.to_csv('_voted_submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a38931fc7f4d22d1266874590bd43cfd2a95d57"
   },
   "source": [
    "Just a quick look at how many positive (faulty power line) predictions did we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "4a4b5b127f85585f6d8db1b0dfd91503bb086ef0",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1242"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76f2dda07ed573fcfe3e59769e99b0376fb76595",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
