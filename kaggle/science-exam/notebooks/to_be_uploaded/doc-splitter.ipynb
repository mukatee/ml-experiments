{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24247a3d-69cd-4aab-a6b7-da3c332907b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T19:51:06.614999615Z",
     "start_time": "2023-09-15T19:51:04.730291134Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers.generation import GenerationConfig\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5a5bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T19:51:08.022117898Z",
     "start_time": "2023-09-15T19:51:07.837606330Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mystuff/notebooks/to_be_uploaded\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be4f7fe-da8b-481d-b967-6b7084a4f366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T19:51:10.031115225Z",
     "start_time": "2023-09-15T19:51:09.762021372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipfile\n",
      "Pipfile.lock\n",
      "chroma_vectors.db\n",
      "deotte-data\n",
      "embedding_vectors_256_head_bge_small_en.db\n",
      "embedding_vectors_512_head_bge_small_en.db\n",
      "enwiki-20230801-pages-articles-multistream.xml.bz2\n",
      "enwiki-20230801.json\n",
      "extraction\n",
      "faiss-gpu-173-python310.zip\n",
      "faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "faiss_index_256_v1_bge_small_en.index\n",
      "first10k.json\n",
      "wikipedia-2023-07-faiss-index.zip\n",
      "wikipedia_202307.index\n",
      "wikipedia_chunks_128.db\n",
      "wikipedia_chunks_192.db\n",
      "wikipedia_chunks_256.db\n",
      "wikipedia_chunks_384.db\n",
      "wikipedia_chunks_512.db\n",
      "wikipedia_chunks_64.db\n"
     ]
    }
   ],
   "source": [
    "!ls /mystuff/wikipedia2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82dfd7c2e8677f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!head /mystuff/wikipedia2/enwiki-20230801.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f3340-03b4-40c4-a4ef-43f0be6c2097",
   "metadata": {},
   "source": [
    "# Read Articles + Collect Main Structure of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53494f4-696f-45d4-b1fd-e7fcc487ad3e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-15T19:51:59.509295273Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee438fb4ed8e4dc18b051a44151ce987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interlinks', 'section_texts', 'title', 'section_titles']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# recursive key extraction from dict\n",
    "def extract_keys(data, keys_set=None):\n",
    "    if keys_set is None:\n",
    "        keys_set = set()\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            keys_set.add(key)\n",
    "            extract_keys(value, keys_set)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            extract_keys(item, keys_set)\n",
    "\n",
    "    return keys_set\n",
    "\n",
    "# Load JSON dump of Wikipedia\n",
    "file_path = \"/mystuff/wikipedia2/enwiki-20230801.json\"\n",
    "all_keys = set()\n",
    "rows = []\n",
    "\n",
    "# Each line in the JSON as an article\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in tqdm(file):\n",
    "        try:\n",
    "            row = json.loads(line)\n",
    "            rows.append(row)\n",
    "            #keys seem to likes of \"section_texts\" and \"section_titles\"\n",
    "            keys = extract_keys(row)\n",
    "            all_keys.update(keys)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Convert set to list and print\n",
    "all_keys_list = list(all_keys)\n",
    "print(all_keys_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f258d-06e1-477c-b0e9-f57e34f22903",
   "metadata": {},
   "source": [
    "## Collect All Article Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f3dba42-8ddf-47f5-bd37-9207e25c56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_titles = [d[\"title\"] for d in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177f4cf-e3de-407a-aca9-cbe727cee3a7",
   "metadata": {},
   "source": [
    "## Collect All Section Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4667b55-cbc7-4080-bac2-62f0aefa1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_list = [title for d in rows for title in d[\"section_titles\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90c83374-c3c5-4d76-a074-bd15e98aa7e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T19:49:59.092467530Z",
     "start_time": "2023-09-15T19:49:58.840330731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d47ab5e330409",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Count Section Titles\n",
    "\n",
    "to remove the most common but less relevant section titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b76870c-6757-4f46-a418-2a7592e9d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(titles_list)\n",
    "\n",
    "#print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "860f8951-c4dc-4442-896c-9ef9d0b068fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Introduction', 6085645),\n",
       " ('References', 5285231),\n",
       " ('External links', 2815284),\n",
       " ('See also', 1456666),\n",
       " ('History', 764821),\n",
       " ('Notes', 340065),\n",
       " ('Career', 339617),\n",
       " ('Further reading', 232363),\n",
       " ('Biography', 230440),\n",
       " ('Personal life', 216187),\n",
       " ('Early life', 186275),\n",
       " ('Track listing', 161421),\n",
       " ('Geography', 159191),\n",
       " ('Reception', 156430),\n",
       " ('Bibliography', 156010),\n",
       " ('Cast', 152554),\n",
       " ('Background', 149296),\n",
       " ('Plot', 134641),\n",
       " ('Description', 133764),\n",
       " ('Sources', 124457),\n",
       " ('Demographics', 107919),\n",
       " ('Personnel', 100709),\n",
       " ('Awards', 99570),\n",
       " ('Discography', 98291),\n",
       " ('Honours', 93032),\n",
       " ('Education', 92365),\n",
       " ('Filmography', 89360),\n",
       " ('Results', 87131),\n",
       " ('Early life and education', 84268),\n",
       " ('Production', 81372)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f298ac01924e8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## List common section titles to remove from all articles. The point in this use case is to search for answers to queries about the article content. Cited sources etc are not very relevant in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49fa240d-34c9-421a-8043-b36106bf48d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T19:42:51.343451960Z",
     "start_time": "2023-10-11T19:42:51.322859315Z"
    }
   },
   "outputs": [],
   "source": [
    "sections_to_remove = [\"References\", \"External links\", \"See also\", \"Further reading\", \"Bibliography\", \"Sources\", \"Cited sources\", \\\n",
    "                      \"General and cited references\", \"General sources\", \"Copyright note\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34816682-27cf-46cd-ac3d-190af044ca49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T19:42:51.636553504Z",
     "start_time": "2023-10-11T19:42:51.608291786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "Etymology, terminology, and definition\n",
      "History\n",
      "Thought\n",
      "Tactics\n",
      "Key issues\n",
      "Criticism\n",
      "See also\n",
      "References\n",
      "Further reading\n",
      "External links\n"
     ]
    }
   ],
   "source": [
    "for title in rows[0][\"section_titles\"]:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447488d06edc7477",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Quick check to see what gets removed using the above section to remove list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32184365-182e-4742-a070-919f86449380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T19:42:57.881758991Z",
     "start_time": "2023-10-11T19:42:57.812925990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Introduction\n",
      "2073\n",
      "1 Etymology, terminology, and definition\n",
      "5184\n",
      "2 History\n",
      "15869\n",
      "3 Thought\n",
      "24278\n",
      "4 Tactics\n",
      "31818\n",
      "5 Key issues\n",
      "42130\n",
      "6 Criticism\n",
      "47226\n",
      "7 See also\n",
      "found: ['Introduction', 'Etymology, terminology, and definition', 'History', 'Thought', 'Tactics', 'Key issues', 'Criticism']\n",
      "8 References\n",
      "found: ['Introduction', 'Etymology, terminology, and definition', 'History', 'Thought', 'Tactics', 'Key issues', 'Criticism', 'See also']\n",
      "9 Further reading\n",
      "found: ['Introduction', 'Etymology, terminology, and definition', 'History', 'Thought', 'Tactics', 'Key issues', 'Criticism', 'See also', 'References']\n",
      "10 External links\n",
      "found: ['Introduction', 'Etymology, terminology, and definition', 'History', 'Thought', 'Tactics', 'Key issues', 'Criticism', 'See also', 'References', 'Further reading']\n"
     ]
    }
   ],
   "source": [
    "row_text = \"\"\n",
    "for idx, title in enumerate(rows[0][\"section_titles\"]):\n",
    "    print(idx, title)\n",
    "    if title in sections_to_remove:\n",
    "        print(\"found: \"+str(rows[0][\"section_titles\"][:idx]))\n",
    "        continue\n",
    "    row_text += f\"### {title} ### \"\n",
    "    row_text += rows[0][\"section_texts\"][idx]\n",
    "    print(len(row_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4e39f90-906a-4ed2-90a2-f96823640393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:12.357253665Z",
     "start_time": "2023-10-11T19:43:12.341539138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'section_titles', 'section_texts', 'interlinks'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa77c920-8b66-4748-88fc-18ca190ccda6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:12.957395529Z",
     "start_time": "2023-10-11T19:43:12.942817387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows[0][\"section_texts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380da87c063cf5e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Try Some Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49838c05-46dc-4b4f-9183-6bd484c21f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm = \"/mystuff/llm/gte-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ab803fe-64fe-43fa-b3e2-3000a9b9e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9751]])\n",
      "CPU times: user 4.96 s, sys: 648 ms, total: 5.61 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "sentences = ['That is a happy person', 'That is a very happy person']\n",
    "\n",
    "embedding_model = SentenceTransformer(embedding_llm)\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "print(cos_sim(embeddings[0], embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd07abfda5a2df7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the Model and Associated Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61753ed3-5427-447d-bc7e-2f5f1d08d98d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:37:33.158484374Z",
     "start_time": "2023-10-11T20:37:33.135657748Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = \"/mystuff/llm/flan-t5-large/flan-t5-large\"\n",
    "#llm = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96938671-383b-40d2-90d5-e8529ef2a81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:37:33.710825899Z",
     "start_time": "2023-10-11T20:37:33.588061507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d99b3dd0-a39e-4345-bf96-d1e44e5d4e61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:37:33.907902583Z",
     "start_time": "2023-10-11T20:37:33.874765087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n"
     ]
    }
   ],
   "source": [
    "print(rows[0][\"section_titles\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad653f0535857e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Try a Few Data Cleaning Approaches\n",
    "\n",
    "This was never used in the end by me, but sometimes useful to document the trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbdc1466-f74e-4181-838e-a6ebd606008f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:37:34.434518424Z",
     "start_time": "2023-10-11T20:37:34.420827645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test   '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"    test   \".lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "851d2875-dd2d-48d2-8fe4-34d2a2ddd1ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:37:34.848594955Z",
     "start_time": "2023-10-11T20:37:34.781946974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles:\n",
      "Post-WWII\n",
      "Another Section\n",
      "\n",
      "Content:\n",
      "dasdə\n",
      "Content1\n",
      "Content2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string = \"dasdə\\n==== Post-WWII ====\\nContent1\\n==== Another Section ====\\nContent2\"\n",
    "\n",
    "def split_into_subsections(row):\n",
    "    row = \"\\n\"+row\n",
    "    # Using re.split to split the string\n",
    "    sections = re.split(r'\\n==+\\s*(.*?)\\s*==+\\n', row)\n",
    "    \n",
    "    titles = []\n",
    "    content = []\n",
    "    for i, section in enumerate(sections):\n",
    "        if i % 2 == 1: # Titles are at odd indices\n",
    "            titles.append(section)\n",
    "        elif i % 2 == 0 and section.strip(): # Content at even indices, ignoring empty strings\n",
    "            content.append(section.lstrip())\n",
    "    return titles, content\n",
    "\n",
    "titles,content = split_into_subsections(string)\n",
    "\n",
    "print(\"Titles:\")\n",
    "for title in titles:\n",
    "    print(title)\n",
    "\n",
    "print(\"\\nContent:\")\n",
    "for section in content:\n",
    "    print(section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157500c701635d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Remove Sections to Remove from all Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d465990d-2441-4975-82b1-61a532096577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:37:35.318300757Z",
     "start_time": "2023-10-11T20:37:35.282143786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\n",
      "Introduction\n",
      "Etymology, terminology, and definition\n",
      "History\n",
      "Thought\n",
      "Tactics\n",
      "Key issues\n",
      "Criticism\n"
     ]
    }
   ],
   "source": [
    "def process_row(row):\n",
    "    title = row[\"title\"]\n",
    "    print(title)\n",
    "    for section_title, section_text in zip(row[\"section_titles\"], row[\"section_texts\"]):\n",
    "        if section_title in sections_to_remove:\n",
    "            continue\n",
    "        #print(\"------------\")\n",
    "        print(section_title)\n",
    "        #print(\"------------\")\n",
    "        #print(section_text.lstrip())\n",
    "\n",
    "process_row(rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cccfdc300910c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set up Langchain for Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b2a22c3-10c7-4ade-9e2f-fb8552ec7237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:37:36.408356944Z",
     "start_time": "2023-10-11T20:37:36.403189359Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "#chunk_size = 512 \n",
    "#chunk_overlap = 100\n",
    "chunk_size = 64 \n",
    "chunk_overlap = 0\n",
    "\n",
    "# separators: https://github.com/langchain-ai/langchain/discussions/3786\n",
    "#text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "#    tokenizer, chunk_size=512, chunk_overlap=100\n",
    "#)\n",
    "# recursive splitter tries the given separators recursively until best match for given text is found to fit in chunk_size\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "\n",
    "#___text_splitter = SentenceTransformersTokenTextSplitter.from_huggingface_tokenizer(\n",
    "#    model_name=llm, chunk_size=512, chunk_overlap=100\n",
    "#)\n",
    "#__text_splitter = RecursiveCharacterTextSplitter(\n",
    "#    # Set a really small chunk size, just to show.\n",
    "#    chunk_size = 100,\n",
    "#    chunk_overlap  = 20,\n",
    "#    length_function = len,\n",
    "#    add_start_index = True,\n",
    "#)\n",
    "\n",
    "def chunk_section(section_text, title, section_title):\n",
    "    #https://stackoverflow.com/questions/14463277/how-to-disable-python-warnings\n",
    "#    with warnings.catch_warnings():\n",
    "    #langchain actually uses logging.warning and not warning.warn, so have to do this:\n",
    "    #https://stackoverflow.com/questions/27647077/fully-disable-python-logging\n",
    "    #https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/text_splitter.py\n",
    "    logging.disable(logging.WARNING)\n",
    "    texts = text_splitter.split_text(section_text)\n",
    "    texts_filtered = []\n",
    "    for text in texts:\n",
    "        token_count = len(tokenizer(text)[\"input_ids\"])\n",
    "        if token_count > 800:\n",
    "#        if token_count < 5 or token_count > 800:\n",
    "            continue\n",
    "#        if token_count > 800:\n",
    "#            print(\"**\"+title)    \n",
    "#            print(\"**\"+section_title)\n",
    "#            print(text)\n",
    "        texts_filtered.append(text)\n",
    "    logging.disable(logging.NOTSET)\n",
    "    return texts_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd2ff3fba64aa3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define SQLite Helper Functions to save chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fe04e8b9975d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0c19d9ccb1a666",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:40:39.587669417Z",
     "start_time": "2023-10-11T20:40:39.558253837Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table documents already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 34\u001b[0m\n\u001b[1;32m      4\u001b[0m cursor \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m      6\u001b[0m create_table_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mCREATE TABLE documents (\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    document_id INTEGER PRIMARY KEY AUTOINCREMENT,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutescript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_table_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m connection\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m     36\u001b[0m connection\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mOperationalError\u001b[0m: table documents already exists"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "connection = sqlite3.connect(\"wikipedia_chunks_256.db\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE documents (\n",
    "    document_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    document_title TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE sections (\n",
    "    section_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    document_id INTEGER,\n",
    "    section_title TEXT,\n",
    "    FOREIGN KEY (document_id) REFERENCES documents (document_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE text_chunks (\n",
    "    chunk_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    section_id INTEGER,\n",
    "    document_id INTEGER,\n",
    "    content TEXT,\n",
    "    FOREIGN KEY (section_id) REFERENCES sections (section_id)\n",
    "    FOREIGN KEY (document_id) REFERENCES documents (section_id)\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_section_id ON text_chunks (section_id);\n",
    "CREATE INDEX idx_document_id ON text_chunks (document_id);\n",
    "CREATE INDEX idx_section_document_id ON sections (document_id);\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cursor.executescript(create_table_sql)\n",
    "connection.commit()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7b83fd3-8fdc-4713-8cf6-d859ed195166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:41:00.473295162Z",
     "start_time": "2023-10-11T20:41:00.421544241Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "connection = sqlite3.connect(\"wikipedia_chunks_256.db\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "def insert_sqlite_document(doc_title, sections, chunks):\n",
    "    try:\n",
    "        # Insert a new document and retrieve its ID\n",
    "        cursor.execute(\"INSERT INTO documents (document_title) VALUES (?)\", (doc_title,))\n",
    "        document_id = cursor.lastrowid\n",
    "\n",
    "        for idx, section_title in enumerate(sections):\n",
    "            # Insert a new section using the document ID and retrieve its ID\n",
    "            cursor.execute(\"INSERT INTO sections (document_id, section_title) VALUES (?, ?)\", (document_id, section_title))\n",
    "            section_id = cursor.lastrowid\n",
    "\n",
    "            for chunk_text in chunks[idx]:\n",
    "                # Insert a new text chunk using the section ID and document ID\n",
    "                cursor.execute(\"INSERT INTO text_chunks (section_id, document_id, content) VALUES (?, ?, ?)\", (section_id, document_id, chunk_text))\n",
    "        connection.commit()\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error with document: {doc_title}\")\n",
    "        print(\"Error:\", e)\n",
    "        connection.rollback()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "975b3579-cca0-4ab3-9d13-23712bb7f150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:41:16.967106738Z",
     "start_time": "2023-10-11T20:41:05.022624762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4941f3e2828a4f8bad4e69e3328644c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6082528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m chunk_sizes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(rows):\n\u001b[0;32m---> 37\u001b[0m     all_chunks,title \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#if title == \"April 6\":\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#    break\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m section_chunks \u001b[38;5;129;01min\u001b[39;00m all_chunks:\n",
      "Cell \u001b[0;32mIn[42], line 20\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     19\u001b[0m             section_text \u001b[38;5;241m=\u001b[39m section_title \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m section_text\n\u001b[0;32m---> 20\u001b[0m         section_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_section\u001b[49m\u001b[43m(\u001b[49m\u001b[43msection_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#        if title == \"April 6\" and False:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#            print(\"------------\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#            print(section_title)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#            print(\"------------\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#            print(\"\\n****\".join(section_chunks))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         all_chunks\u001b[38;5;241m.\u001b[39mappend(section_chunks)\n",
      "Cell \u001b[0;32mIn[28], line 36\u001b[0m, in \u001b[0;36mchunk_section\u001b[0;34m(section_text, title, section_title)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_section\u001b[39m(section_text, title, section_title):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#https://stackoverflow.com/questions/14463277/how-to-disable-python-warnings\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#    with warnings.catch_warnings():\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#langchain actually uses logging.warning and not warning.warn, so have to do this:\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#https://stackoverflow.com/questions/27647077/fully-disable-python-logging\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/text_splitter.py\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdisable(logging\u001b[38;5;241m.\u001b[39mWARNING)\n\u001b[0;32m---> 36\u001b[0m     texts \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msection_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     texts_filtered \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/text_splitter.py:856\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/text_splitter.py:848\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter._split_text\u001b[0;34m(self, text, separators)\u001b[0m\n\u001b[1;32m    846\u001b[0m             final_chunks\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 848\u001b[0m             other_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_separators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m             final_chunks\u001b[38;5;241m.\u001b[39mextend(other_info)\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _good_splits:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/text_splitter.py:848\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter._split_text\u001b[0;34m(self, text, separators)\u001b[0m\n\u001b[1;32m    846\u001b[0m             final_chunks\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 848\u001b[0m             other_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_separators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m             final_chunks\u001b[38;5;241m.\u001b[39mextend(other_info)\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _good_splits:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/text_splitter.py:851\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter._split_text\u001b[0;34m(self, text, separators)\u001b[0m\n\u001b[1;32m    849\u001b[0m             final_chunks\u001b[38;5;241m.\u001b[39mextend(other_info)\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _good_splits:\n\u001b[0;32m--> 851\u001b[0m     merged_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_good_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_separator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m     final_chunks\u001b[38;5;241m.\u001b[39mextend(merged_text)\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_chunks\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/text_splitter.py:181\u001b[0m, in \u001b[0;36mTextSplitter._merge_splits\u001b[0;34m(self, splits, separator)\u001b[0m\n\u001b[1;32m    179\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[0;32m--> 181\u001b[0m     _len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_length_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    183\u001b[0m         total \u001b[38;5;241m+\u001b[39m _len \u001b[38;5;241m+\u001b[39m (separator_len \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(current_doc) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_size\n\u001b[1;32m    185\u001b[0m     ):\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m total \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_size:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/text_splitter.py:226\u001b[0m, in \u001b[0;36mTextSplitter.from_huggingface_tokenizer.<locals>._huggingface_tokenizer_length\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_huggingface_tokenizer_length\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2585\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2549\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2568\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2569\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2572\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2585\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2993\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2984\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2985\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2986\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2990\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2991\u001b[0m )\n\u001b[0;32m-> 2993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:539\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    538\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 539\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:467\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    460\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    461\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    465\u001b[0m )\n\u001b[0;32m--> 467\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    479\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    481\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    491\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from transformers.utils import logging\n",
    "\n",
    "#https://github.com/langchain-ai/langchain/discussions/3786\n",
    "\n",
    "def process_row(row):\n",
    "    #logging.set_verbosity(40)\n",
    "    title = row[\"title\"]\n",
    "    #print(title)\n",
    "    all_chunks = []\n",
    "    all_sections = []\n",
    "    section_idx = 0\n",
    "    for section_title, section_text in zip(row[\"section_titles\"], row[\"section_texts\"]):\n",
    "        if section_title in sections_to_remove:\n",
    "            continue\n",
    "        section_idx += 1\n",
    "        #subsection_titles, subsection_texts = split_into_subsections(section_text)\n",
    "        section_text = section_text.lstrip()\n",
    "        if section_idx == 1:\n",
    "            section_text = section_title + \" - \" + section_text\n",
    "        section_chunks = chunk_section(section_text, title, section_title)\n",
    "#        if title == \"April 6\" and False:\n",
    "#            print(\"------------\")\n",
    "#            print(section_title)\n",
    "#            print(\"------------\")\n",
    "#            print(\"\\n****\".join(section_chunks))\n",
    "        all_chunks.append(section_chunks)\n",
    "        all_sections.append(section_title)\n",
    "        #print(\"\\n****\".join(section_chunks))\n",
    "        #print(len(section_chunks))\n",
    "    #logging.set_verbosity(30)\n",
    "    insert_sqlite_document(title, all_sections, all_chunks)\n",
    "    return all_chunks, title\n",
    "\n",
    "chunk_sizes = []\n",
    "\n",
    "for row in tqdm(rows):\n",
    "    all_chunks,title = process_row(row)\n",
    "    #if title == \"April 6\":\n",
    "    #    break\n",
    "    for section_chunks in all_chunks:\n",
    "        chunk_sizes.extend([len(tokenizer(chunk)[\"input_ids\"]) for chunk in section_chunks])\n",
    "\n",
    "connection.close()\n",
    "#all_chunks = process_row(rows[2])\n",
    "#all_chunks = chunk_section(state_of_the_union)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8aa22c4b292dfb8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:41:20.562383769Z",
     "start_time": "2023-10-11T20:41:20.469795913Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12272.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.13918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.50598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>43.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>53.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>266.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              size\n",
       "count  12272.00000\n",
       "mean      43.13918\n",
       "std       17.50598\n",
       "min        2.00000\n",
       "25%       32.00000\n",
       "50%       43.00000\n",
       "75%       53.00000\n",
       "max      266.00000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_chunk_sizes = pd.DataFrame()\n",
    "df_chunk_sizes[\"size\"] = chunk_sizes\n",
    "df_chunk_sizes.describe().apply(lambda s: s.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd924a8343e395a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:41:25.964438569Z",
     "start_time": "2023-10-11T20:41:25.928460780Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9815</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5433</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12240</th>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9465</th>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       size\n",
       "9815    156\n",
       "5433    158\n",
       "12240   163\n",
       "1146    173\n",
       "1222    185\n",
       "1038    201\n",
       "9465    211\n",
       "2251    212\n",
       "734     223\n",
       "620     266"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunk_sizes.sort_values(by=\"size\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65d073fe-e72f-41dd-8670-fc0d9970b9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:42:49.094988192Z",
     "start_time": "2023-10-11T20:42:49.068509582Z"
    }
   },
   "outputs": [],
   "source": [
    "missed_docs = [\"2019–20 Coupe de France preliminary rounds, Grand Est\", \"Garbh-bheinn (Skye)\", \\\n",
    "               \"Electoral district of Pastoral District of Murrumbidgee\", \"National Reform Association (chartered 1864)\", \\\n",
    "               \"Florence Moog\", \"Rafi Bohic\", \"Adventures in Silverado\", \"Albert Ganado\", \\\n",
    "               \"The Watchmaker of Everton\", \"Henria railway station\", \"Marcus O'Lone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd0700d5-18aa-4b9f-8269-f8359a5af009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T20:43:01.889846489Z",
     "start_time": "2023-10-11T20:42:58.426879742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f9b2682f4447c79a041e02657352ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6082528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found missing title: 2019–20 Coupe de France preliminary rounds, Grand Est\n",
      "found missing title: Garbh-bheinn (Skye)\n",
      "found missing title: Electoral district of Pastoral District of Murrumbidgee\n",
      "found missing title: National Reform Association (chartered 1864)\n",
      "found missing title: Florence Moog\n",
      "found missing title: Rafi Bohic\n",
      "found missing title: Adventures in Silverado\n",
      "found missing title: Albert Ganado\n",
      "found missing title: The Watchmaker of Everton\n",
      "found missing title: Henria railway station\n",
      "found missing title: Marcus O'Lone\n"
     ]
    }
   ],
   "source": [
    "#from transformers.utils import logging\n",
    "\n",
    "#https://github.com/langchain-ai/langchain/discussions/3786\n",
    "\n",
    "for row in tqdm(rows):\n",
    "    if row[\"title\"] not in missed_docs:\n",
    "        continue\n",
    "    print(f\"found missing title: {row['title']}\")\n",
    "    all_chunks = process_row(row)\n",
    "    for section_chunks in all_chunks:\n",
    "        chunk_sizes.extend([len(tokenizer(chunk)[\"input_ids\"]) for chunk in section_chunks])\n",
    "\n",
    "connection.close()\n",
    "#all_chunks = process_row(rows[2])\n",
    "#all_chunks = chunk_section(state_of_the_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c84c66fe072cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Example Splitter Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f530eb68899c093",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsss\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=512, chunk_overlap=20\n",
    ")\n",
    "texts = text_splitter.split_text(\"dsss\")\n",
    "print(texts[0])\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f5248087747e871",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8e703c96413eff8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the full text of President Joe Biden’s State of the Union address, as prepared for delivery and released by the White House on Tuesday.\n",
      "\n",
      "    Mr. Speaker. Madam Vice President. Our First Lady and Second Gentleman.\n",
      "\n",
      "    Members of Congress and the Cabinet. Leaders of our military.\n",
      "\n",
      "    Mr. Chief Justice, Associate Justices, and retired Justices of the Supreme Court.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=100, chunk_overlap=20\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
