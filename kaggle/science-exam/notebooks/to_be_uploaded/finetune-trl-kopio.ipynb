{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4454e1-b0cc-4630-9736-e61fc14fa853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  9 20:58:06 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:2D:00.0 Off |                  N/A |\n",
      "|  0%   41C    P8              22W / 370W |    102MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999407b-a1d1-424c-8179-a4ff612036bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c122ff-ae5a-4575-ac29-7506e23015d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ccca6-19e6-4fd8-bf01-b89395e618f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa1d0d-7233-40a0-9a02-516ab75156be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cc4ddc-20ef-4f13-be51-65be022297e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e8cd9-de37-4b19-b41c-a3487c49353b",
   "metadata": {},
   "source": [
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The name of the Casual LM model we wish to fine with SFTTrainer\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: Optional[str] = field(default=\"facebook/opt-350m\", metadata={\"help\": \"the model name\"})\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=\"timdettmers/openassistant-guanaco\", metadata={\"help\": \"the dataset name\"}\n",
    "    )\n",
    "    dataset_text_field: Optional[str] = field(default=\"text\", metadata={\"help\": \"the text field of the dataset\"})\n",
    "    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n",
    "    batch_size: Optional[int] = field(default=64, metadata={\"help\": \"the batch size\"})\n",
    "    seq_length: Optional[int] = field(default=512, metadata={\"help\": \"Input sequence length\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=16, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    load_in_8bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 8 bits precision\"})\n",
    "    load_in_4bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 4 bits precision\"})\n",
    "    use_peft: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})\n",
    "    trust_remote_code: Optional[bool] = field(default=True, metadata={\"help\": \"Enable `trust_remote_code`\"})\n",
    "    output_dir: Optional[str] = field(default=\"output\", metadata={\"help\": \"the output directory\"})\n",
    "    peft_lora_r: Optional[int] = field(default=64, metadata={\"help\": \"the r parameter of the LoRA adapters\"})\n",
    "    peft_lora_alpha: Optional[int] = field(default=16, metadata={\"help\": \"the alpha parameter of the LoRA adapters\"})\n",
    "    logging_steps: Optional[int] = field(default=1, metadata={\"help\": \"the number of logging steps\"})\n",
    "    use_auth_token: Optional[bool] = field(default=True, metadata={\"help\": \"Use HF auth token to access the model\"})\n",
    "    num_train_epochs: Optional[int] = field(default=3, metadata={\"help\": \"the number of training epochs\"})\n",
    "    max_steps: Optional[int] = field(default=-1, metadata={\"help\": \"the number of training steps\"})\n",
    "    save_steps: Optional[int] = field(\n",
    "        default=100, metadata={\"help\": \"Number of updates steps before two checkpoint saves\"}\n",
    "    )\n",
    "    save_total_limit: Optional[int] = field(default=10, metadata={\"help\": \"Limits total number of checkpoints.\"})\n",
    "    push_to_hub: Optional[bool] = field(default=False, metadata={\"help\": \"Push the model to HF Hub\"})\n",
    "    hub_model_id: Optional[str] = field(default=None, metadata={\"help\": \"The name of the model on HF Hub\"})\n",
    "\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args = parser.parse_args_into_dataclasses()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a420e7-c7bb-41a1-a02c-28052a5b30eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-13B-GPTQ\t\t\t btlm-3b-8k-base\n",
      "Llama-2-7b-chat-hf\t\t\t datasets\n",
      "Llama-2-7b-hf\t\t\t\t flan-t5-large\n",
      "Mistral-7B-v0.1\t\t\t\t flan-t5-xl\n",
      "Mistral-7B-v0.1-GPTQ\t\t\t flan-t5-xxl\n",
      "Qwen-7B\t\t\t\t\t flan-ul2\n",
      "T0\t\t\t\t\t gte-base\n",
      "T0pp\t\t\t\t\t hmm\n",
      "WizardLM-1.0-Uncensored-Llama2-13B-GPTQ  minotaur-15b\n",
      "WizardLM-33B-V1.0-Uncensored-GPTQ\t mistral-package\n",
      "Xwin-LM-70B-V0.1\t\t\t open_llama_3b_v2\n",
      "airoboros-l2-13B-3.0-GPTQ\t\t phi-1_5\n",
      "airoboros-l2-7B-3.0-GPTQ\t\t processed\n",
      "all-MiniLM-L12-v2\t\t\t starchat-beta\n",
      "bge-base-en\t\t\t\t starcoder\n",
      "bge-small-en\t\t\t\t vicuna-13b-v1.5-16k\n",
      "bge.zip\n"
     ]
    }
   ],
   "source": [
    "!ls /mystuff/llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf5eba1-8a2d-479a-82c9-f60b2cc35a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"parquet\", data_files={'train': 'llama_train_8192.parquet'}, split=\"train\")\n",
    "eval_dataset = load_dataset(\"parquet\", data_files={'test': 'llama_valid.parquet'}, split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cd55d2-380b-4553-a8b2-43f1d352884b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'size'],\n",
       "    num_rows: 8192\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ff73ee-becf-4865-9756-deaee49fb013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST]answer the following multiple choice question with a single letter matching the option for the correct answer. use the provided context with your own knowledge to find the correct answer.the set of possible letters for options is A, B, C, D, E. answer with the letter of the correct answer only, and nothing more. \\n\\ncontext: The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster\\'s total mass; the rest being dark matter. This is highly nontrivial, since although luminous matter such as stars and galaxies are easily summed, baryonic matter can also exist in highly non-luminous form, such as black holes, planets, and highly diffuse interstellar gas. Cosmological hydrodynamical simulations from theory predict that a fraction of the missing baryons are located in galactic haloes at temperatures of 106 K and the (WHIM) at temperatures of 105–107 K, with recent observations providing strong support. 50x50px Available under CC BY 4.0. In models for the gravitational formation of structure with cold dark matter, the smallest structures collapse first and eventually build the largest structures, clusters of galaxies. Large scale galaxy surveys in the 2000s revealed a baryon deficit. At the same time, a census of baryons in the recent observable universe has found that observed baryonic matter accounts for less than half of that amount. A mass deficit is the amount of mass (in stars) that has been removed from the center of a galaxy, presumably by the action of a binary supermassive black hole. thumb|left|The figure illustrates how mass deficits are measured, using the observed brightness profile of a galaxy The density of stars increases toward the center in most galaxies. One claim of a solution was published in 2017 when two groups of scientists said they found evidence for the location of missing baryons in intergalactic matter. When observed visually, clusters appear to be collections of galaxies held together by mutual gravitational attraction. \\n\\nquestion: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\\n\\nanswer options:\\nA: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\\nB: MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\\nC: MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\\nD: MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\\nE: MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\\n[/INST]\\n\\nD\\n</s>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b9695d-4be1-4973-9901-f9a0cad42d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:480: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad22a0165d1f4686be622a326cf14a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62fc59708ce4995817c227a0050a7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4096' max='4096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4096/4096 12:35:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.816811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.855000</td>\n",
       "      <td>1.742076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.855000</td>\n",
       "      <td>1.637434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.635500</td>\n",
       "      <td>1.602499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.635500</td>\n",
       "      <td>1.592242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.590200</td>\n",
       "      <td>1.587006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.590200</td>\n",
       "      <td>1.583118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.602800</td>\n",
       "      <td>1.580192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.602800</td>\n",
       "      <td>1.577583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.565600</td>\n",
       "      <td>1.575887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.565600</td>\n",
       "      <td>1.574306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.581400</td>\n",
       "      <td>1.572379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.581400</td>\n",
       "      <td>1.571382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.563800</td>\n",
       "      <td>1.570190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.563800</td>\n",
       "      <td>1.568798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.562000</td>\n",
       "      <td>1.567833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.562000</td>\n",
       "      <td>1.566855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.541600</td>\n",
       "      <td>1.566396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.541600</td>\n",
       "      <td>1.565559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.564400</td>\n",
       "      <td>1.564927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.564400</td>\n",
       "      <td>1.564578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.534200</td>\n",
       "      <td>1.563954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.534200</td>\n",
       "      <td>1.563266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.567800</td>\n",
       "      <td>1.562921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.567800</td>\n",
       "      <td>1.562653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.534800</td>\n",
       "      <td>1.562169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.534800</td>\n",
       "      <td>1.561941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.548800</td>\n",
       "      <td>1.561150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.548800</td>\n",
       "      <td>1.560888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.563500</td>\n",
       "      <td>1.560561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.563500</td>\n",
       "      <td>1.560185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.539200</td>\n",
       "      <td>1.560052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.539200</td>\n",
       "      <td>1.559794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.535400</td>\n",
       "      <td>1.559834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.535400</td>\n",
       "      <td>1.559420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.532100</td>\n",
       "      <td>1.559105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.532100</td>\n",
       "      <td>1.558783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.537800</td>\n",
       "      <td>1.558830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.537800</td>\n",
       "      <td>1.558266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.526100</td>\n",
       "      <td>1.558366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.526100</td>\n",
       "      <td>1.558391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.539600</td>\n",
       "      <td>1.557676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.539600</td>\n",
       "      <td>1.557633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.525300</td>\n",
       "      <td>1.557630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.525300</td>\n",
       "      <td>1.557407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.544200</td>\n",
       "      <td>1.557248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.544200</td>\n",
       "      <td>1.557052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.539500</td>\n",
       "      <td>1.557015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.539500</td>\n",
       "      <td>1.556732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.551800</td>\n",
       "      <td>1.556767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.551800</td>\n",
       "      <td>1.556563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.533500</td>\n",
       "      <td>1.556439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.533500</td>\n",
       "      <td>1.556407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.528900</td>\n",
       "      <td>1.556603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.528900</td>\n",
       "      <td>1.556388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.536000</td>\n",
       "      <td>1.556114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.536000</td>\n",
       "      <td>1.555735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.536500</td>\n",
       "      <td>1.555906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.536500</td>\n",
       "      <td>1.556189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.555200</td>\n",
       "      <td>1.555854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.555200</td>\n",
       "      <td>1.555639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.576700</td>\n",
       "      <td>1.555789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.576700</td>\n",
       "      <td>1.555362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.516300</td>\n",
       "      <td>1.555164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.516300</td>\n",
       "      <td>1.555552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.536600</td>\n",
       "      <td>1.555190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.536600</td>\n",
       "      <td>1.555375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.536400</td>\n",
       "      <td>1.555089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.536400</td>\n",
       "      <td>1.555340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.559100</td>\n",
       "      <td>1.555043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.559100</td>\n",
       "      <td>1.555037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.541000</td>\n",
       "      <td>1.555021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.541000</td>\n",
       "      <td>1.554961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.516900</td>\n",
       "      <td>1.555046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.516900</td>\n",
       "      <td>1.554835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.548200</td>\n",
       "      <td>1.554833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.548200</td>\n",
       "      <td>1.554864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.522600</td>\n",
       "      <td>1.554712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.522600</td>\n",
       "      <td>1.554739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.536700</td>\n",
       "      <td>1.554623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.536700</td>\n",
       "      <td>1.554893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_load_in_8_bit = True\n",
    "p_load_in_4_bit = False\n",
    "model_name = \"/mystuff/llm/Llama-2-7b-hf\"\n",
    "trust_remote = True\n",
    "use_auth_token = False\n",
    "#dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "output_dir = \"llama_finetuned_base_8k\"\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = batch_size\n",
    "learning_rate = 1.41e-5\n",
    "logging_steps = 100\n",
    "num_train_epochs = 2\n",
    "max_steps = 10\n",
    "log_with = \"none\"\n",
    "save_steps = 1000\n",
    "save_total_limit = 5\n",
    "push_to_hub = False\n",
    "hub_model_id = None\n",
    "use_peft = True\n",
    "peft_lora_r = 64\n",
    "peft_lora_alpha = 16\n",
    "seq_length = 4096\n",
    "load_best = True\n",
    "dataset_text_field = \"text\"\n",
    "\n",
    "\n",
    "# Step 1: Load the model\n",
    "if p_load_in_8_bit and p_load_in_4_bit:\n",
    "    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
    "elif p_load_in_8_bit or p_load_in_4_bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=p_load_in_8_bit, load_in_4bit=p_load_in_4_bit\n",
    "    )\n",
    "    # Copy the model to each device\n",
    "    device_map = {\"\": Accelerator().local_process_index}\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    device_map = None\n",
    "    quantization_config = None\n",
    "    torch_dtype = None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=trust_remote,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_auth_token=use_auth_token,\n",
    ")\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "#dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Step 3: Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    #max_steps=max_steps,\n",
    "    report_to=log_with,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    push_to_hub=push_to_hub,\n",
    "    hub_model_id=hub_model_id,\n",
    "    load_best_model_at_end=load_best,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 50,\n",
    "    #evaluate_during_training=True,\n",
    ")\n",
    "\n",
    "# Step 4: Define the LoraConfig\n",
    "if use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        r=peft_lora_r,\n",
    "        lora_alpha=peft_lora_alpha,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    peft_config = None\n",
    "\n",
    "# Step 5: Define the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    max_seq_length=seq_length,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=dataset_text_field,\n",
    "    peft_config=peft_config,\n",
    "    eval_dataset = eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 6: Save the model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d361176e-a687-410e-866a-650987e4c703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa74c63-077e-4f11-92e5-3b21cecfe8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "089b4fc1-2243-4899-bd4c-c0ef51f2b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b313c9c83d64760b070132d08d0b542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"/mystuff/llm/Llama-2-7b-hf\"\n",
    "lora_dir = \"llama_finetuned_base_8k\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:0\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained()\n",
    "model = PeftModel.from_pretrained(model, lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "812d1ad6-2622-4fc7-9722-66937b05ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/peft/issues/638\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"merged_model_llama2_finetuned_peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f2df6-dbde-4fc5-8e11-8a3c18ba3dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d8c7f-cdbe-48d8-828d-b831ab934dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0a87d-7d17-408b-a0c4-53168244cf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e4891-82a6-4806-abc5-9baa39f88409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e9d59-266f-4d28-a26f-83d436b38f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992faa19-7879-4f74-bd1f-633aebebdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in range(30):\n",
    "#    print(len(dataset[x][\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4739371-6b2f-482e-a91a-ae04c3da5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_load_in_8_bit = True\n",
    "p_load_in_4_bit = False\n",
    "model_name = \"/mystuff/llm/Llama-2-7b-chat-hf\"\n",
    "trust_remote = True\n",
    "use_auth_token = False\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "output_dir = \"llama_finetuned\"\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1.41e-5\n",
    "logging_steps = 1\n",
    "num_train_epochs = 3\n",
    "max_steps = 1\n",
    "log_with = \"none\"\n",
    "save_steps = 100\n",
    "save_total_limit = 10\n",
    "push_to_hub = False\n",
    "hub_model_id = None\n",
    "use_peft = True\n",
    "peft_lora_r = 64\n",
    "peft_lora_alpha = 16\n",
    "seq_length = 1024\n",
    "dataset_text_field = \"text\"\n",
    "\n",
    "\n",
    "# Step 1: Load the model\n",
    "if p_load_in_8_bit and p_load_in_4_bit:\n",
    "    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
    "elif p_load_in_8_bit or p_load_in_4_bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=p_load_in_8_bit, load_in_4bit=p_load_in_4_bit\n",
    "    )\n",
    "    # Copy the model to each device\n",
    "    device_map = {\"\": Accelerator().local_process_index}\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    device_map = None\n",
    "    quantization_config = None\n",
    "    torch_dtype = None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=trust_remote,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_auth_token=use_auth_token,\n",
    ")\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Step 3: Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    max_steps=max_steps,\n",
    "    report_to=log_with,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    push_to_hub=push_to_hub,\n",
    "    hub_model_id=hub_model_id,\n",
    ")\n",
    "\n",
    "# Step 4: Define the LoraConfig\n",
    "if use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        r=peft_lora_r,\n",
    "        lora_alpha=peft_lora_alpha,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    peft_config = None\n",
    "\n",
    "# Step 5: Define the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    max_seq_length=seq_length,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=dataset_text_field,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 6: Save the model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed43fe6-7250-47d9-a6be-15eb4051dd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d0750-5d7a-4a03-b5d1-1b825866854f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039a039-fc51-4233-9628-e3a429ebc2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fa426-0a61-4138-a27d-d49ab6a2967b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d3c69-bc83-43a7-bc09-7a137b02aac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12535170-f6e5-478c-ba91-7c40ed1964f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee6bad-82d5-417c-8a3c-429b2eff1df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1289b-d9f5-43fe-bc00-ac53c900756d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4090fc6-e528-47e9-976a-1e677caa3722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec3d70-717e-43b7-94e6-7a43b7073666",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/mystuff/llm/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c85be-22fb-4a05-bee3-5e62ea7bb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6fdc2-b417-403a-aa56-765d52481739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(dataset_name, split=\"train\")\n",
    "train_dataset = load_dataset(\"parquet\", data_files={'train': 'llama_train.parquet'}, split=\"train\")\n",
    "eval_dataset = load_dataset(\"parquet\", data_files={'test': 'llama_valid.parquet'}, split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af125843-10c5-48c5-b87d-32739af43cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "guantaco_dataset = load_dataset(dataset_name, split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5511f5-31d4-4079-a747-d6b9f77efc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91462a40-f4d3-4ded-b76f-4d903097efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac762ef-2b26-40b1-8e70-148e884cc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sizes = []\n",
    "for n in tqdm(range(train_dataset.shape[0])):\n",
    "    count = len(tokenizer(train_dataset[n][\"text\"])[\"input_ids\"])\n",
    "    my_sizes.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba8586-7c2c-42a5-92c8-1d7b3f541c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "833fdf71-7c59-4b07-80cb-2b2f54627c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_my_counts = pd.DataFrame()\n",
    "df_my_counts[\"size\"] = my_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a3e7ac0-8fea-4c51-9123-fdbec62b3750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60347.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1655.850846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2650.810107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>297.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1084.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1321.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1664.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>405056.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                size\n",
       "count   60347.000000\n",
       "mean     1655.850846\n",
       "std      2650.810107\n",
       "min       297.000000\n",
       "25%      1084.000000\n",
       "50%      1321.000000\n",
       "75%      1664.000000\n",
       "max    405056.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_my_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "327dcc92-bd31-475e-8526-b17c607a31ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21133"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(df_my_counts[\"size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bbeccf9-1738-43eb-a6a2-ec337d6ec22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset[21133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88db9fc0-dbe4-4033-af2e-9228e7f651e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_my_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_dataset[df_my_counts[\"size\"] < 4000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120e71b-2480-4dd6-92d2-6f0fc164a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_my_counts_smaller ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb60ecf5-8f11-49a6-a7a0-583dfa737533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "-rw-r--r-- 1 root root 180072962 Sep 12 21:18 llama_train.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l llama_train.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b430b-2d0b-492e-858f-9959c9e14a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "209fd6fc-6791-4189-a631-58d7753a6fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29624fa5d8b2490693f2e0b3ba5ef288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_sizes = []\n",
    "for n in tqdm(range(guantaco_dataset.shape[0])):\n",
    "    count = len(tokenizer(guantaco_dataset[n][\"text\"])[\"input_ids\"])\n",
    "    q_sizes.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba6bf3d-3670-42a6-914b-a2b3f59bc972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81db8e99-77dd-4f2a-acec-85ff8a978c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_q_counts = pd.DataFrame()\n",
    "df_q_counts[\"size\"] = q_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e16f16c2-a74c-46ec-ab99-14312504cc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9846.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>442.543571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>368.110089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7831.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              size\n",
       "count  9846.000000\n",
       "mean    442.543571\n",
       "std     368.110089\n",
       "min      17.000000\n",
       "25%     200.000000\n",
       "50%     365.000000\n",
       "75%     571.000000\n",
       "max    7831.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_q_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67250515-1942-4912-b354-09ac6b9334d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4824"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(df_q_counts[\"size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e08cdd2e-000f-446b-84da-d27cd1c75359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '### Human: 什麼是期貨？### Assistant: 期货的英文名是Futures，期货是相对于现货来说的，所谓的现货交易就是一手交钱一手交货，而期货是我需要的东西还没有生产出来，但我把全部货款或者部分货款作为定金先给供货商，过段时间供货商给我交货就行。比如，我们现在买的期房都是先付钱，过段时间开发商才会交房。\\n期货在历史上是从远期合约开始的，远期合约在人类历史上很早就出现了，最早可以追溯到公元前的古希腊时代。远期合约市场最早出现在日本的江户幕府时代，那个时代，稻米是一种非常重要战略资源，因为一旦准备打仗，将军就需要向农户大量买米备战，但是将军不知道明年的米价是上涨还是下跌，于是将军就对农户说“我现在先把钱付给你，你就按这个价格明年给我稻米，不管明年米价是涨是跌，必须按这个价格来”。将军和农户达成的这个约定就叫做远期合约。\\n\\n远期合约最大的作用就是套期保值。我们知道商品的价格是在波动的，比如说将军和农户签订合约的时候，他们会按某一个价格去计算，但是明年稻米的价格不一定是他们约定的这个价格，如果稻米价格上涨，按市场价卖，那么对农户自然是有利的，因为他可以卖更多钱；但对于将军来说，他买米需要花更多的钱，所以将军害怕稻米价格上涨，需要合约来提前锁定价格；反之，如果明年稻米价格下跌，自然对将军有利，但对农户来说，他们会少卖钱甚至可能亏本，所以农户害怕稻米价格下跌，也需要通过合约来提前锁定价格。既然双方都有这个需求，于是一个远期现货交易的合约就产生了。合同约定，明年的某个时候，以约定的价格来交易，谁也别吃亏，谁也别占便宜，这就是所谓的套期保值。\\n所以说套期保值对于一些生产型企业是有很大帮助的，可以减少企业的经营风险。\\n\\n不过远期合约也存在很多的问题。首先，远期合约很容易出现毁约的情况。继续用将军跟农户买稻米的事举例，通常情况下，将军肯定不会先把所有的货款都给农户，只会给一部分定金，比如先给100两银子作为定金，约定明年农户给将军100担稻米时，将军再把尾款付给农户。但到了第二年100担稻米的价格涨价到300两银子了，这时候农户一算发现如果自己要是毁约，不把稻米卖给将军，自己赚的更多呀，于是农户打算把原来的定金退还给将军，然后带着粮食到价高的地方去卖。如果这样的事发生，将军就吃亏了。反过来说，将军也可能毁约，比如第二年的时候稻米价格下降很多，将军一算发现自己即便定金不要了，到市场上重新去买也还是划算的，于是他跟农户说我给你的定金不要了，自己从别的地方重新买，这就是期货合约的毁约问题。其次，远期合约的商品质量无法保证，而且交货的时候双方都需要检查，每一次检查都是非常麻烦的。第三，交易可能很不方便。比如农户签约后突然生病没法种地了，所以他跟将军说，“我不能种地履行合约了，您看我能不能把这个合约卖给其他的农户，让他跟您继续去履行呢？”这时候存在的问题是，将军有可能不同意这样做，要求农名生病了也必须下地干活，这就出现了交易不便的情况。\\n\\n因为有以上这些问题的存在，所以人们希望把这种远期合约变得更加标准化，于是就出现了我们现在所说的标准期货合约。\\n标准期货合约最早出现在1865年的芝加哥，那一年芝加哥出现了两家期货交易所，一家叫芝加哥期货交易所，一家叫芝加哥商品交易所，后来这两家合并了。现在的期货合约是以当时芝加哥的第一份标准期货合约为蓝本的。一般一个标准期货合约首先会规定商品的质量，交易的商品需要达到一个规定的级别，现在来说，就是只有某些厂家生产的商品才可以进入这个市场，其他厂家都不行；其次是规定数量，是1千桶原油还是1吨黄金等等；还有就是规定交割时间、地点以及交割的方式；最重要一点是合约规定这个期货合约持有人是可以在场内交易的，也就是说你持有一份期货合约，如果你自己没法履行了，你可以去交易所把合约卖给能履行的人。\\n\\n和股票一样，期货也是有价格的。期货合约的价格等于一个单位商品的单价乘以一个合约的商品数量。比如“中行原油宝”这款产品投资的就是叫做轻质低硫原油作为标的的期货合约，简称WTI原油期货。这个品种的一份期货合约值多少钱呢？首先买卖双方得商量一个单价，假如双方觉得20美元一桶的价格比较合适，于是就立约了，合约规定一桶原油的单价是20美元，然后约定一份合约的规模是1000桶WTI原油，20X1000=20000（美元），也就是说这份合约的价值是2万美元，但这2万美元并不是约定买方要给卖方这么多钱，这跟股票交易的差别还是很大的。期货合约的意思是，在特点时间，约定双方可以以2万美元的价格去交割1000桶原油，而这个买卖的过程我们称为开仓。\\n开仓和平仓，多方和空方\\n\\n现在有两个人，一个人他想买原油，另一个人想卖原油，这里想买原油的一方我们称之为多方，而想卖原油的一方我们称之为空方。然后有一天他们之间进行了交易，空方把一个合约卖给了多方，这就被叫做开仓，在开仓的过程中有一个价格叫做P1，这里的P1就当是上面所说的2万美元。现在多方相当于花了2万美元拥有了1000桶原油的所有权，只不过还没有交货而已，而空方相当于用2万美元的价格已经卖出了1000桶原油。但问题是空方他不一定手里真有原油卖给多方，真到了交割日他未必能交货，而多方他也不一定真的到期后想要原油。\\n\\n怎么办？没有关系，等过段时间后，多方和空方都可以选择平仓，所谓平仓就是这个合约又从多方还给空方，这时候又会出现一个平仓价格即P2。需要记住的是P1和P2这两个价格其实是随着市场在波动的。\\n\\n情况一：\\n\\n假如原油的价格上涨了，也就是开仓的价格低于平仓的价格(P1＜P2），这种情况下对多方来说，他当初买原油的时候花了2万美元，现在还没等空方交货，他就把这个合约卖给其他人了，而且卖价多于两万美元，自然多方就赚钱了。如果多方赚了，那就意味着空方赔了，因为最开始的时候，空方是2万美元卖了1000桶原油，而过了一段时间，没到交货时间，空方就得把这个合约买回来，但买回来的价格高于2万美元，自然空方就赔钱了。\\n\\n情况二：\\n\\n假如原油的价格下跌了，也就是开仓的价格高于平仓的价格(P1＞P2），这种情况下，自然多方就亏钱了，空方就赚钱了。因为空方当初用2万美元卖出了1000桶原油，一段时间后，还没等交割，他就从市场上用低于2万美元的价格买回1000桶原油平仓了。而少出的这部分钱，就是他赚的，同时也是多方亏的。\\n保证金交易制度\\n\\n期货和股票另一个巨大的区别是，股票是一方掏钱从持有股票的手里买股票，交易的钱会实时进入到卖股票的人账户，而期货交易，交易双方立约之后不相互交钱，这个钱会被交易所锁定，这就是保证金交易制度。这个保证金就相当于远期合约中的定金，所以一般只需要交全部总货款的一部分，而不同的商品，保证金的比例是不同的。假设WTI原油期货合约的保证金是总合约价值的10%，现在有甲乙两方交易者，其中甲是一个多方，乙是一个空方，他们之间准备交易WTI原油，最开始时他们立约的价格是2万美元，如果按10%的保证金来算，那么甲账户里需要有20000X10%=2000（美元），如果甲账户没有这么多钱，交易所就不会让甲开仓。同样，乙作为空方，他的账户也需要有2000美元，而他们账户里的2000美元就是保证金。\\n\\n结果到了第二天的时候，市场上的合约价格上涨了，假设每份合约的价格变成了21000美元。那么这个时候作为多方的甲因为每份合约价格上涨了1000美元，而净赚1000美元。这时候交易所就会把双方账户里的钱进行一个划转，他会把乙账户里的1000美元划转到甲账户里，这时候甲账户里就多了1000美元，所以甲账户变成了3000美元，乙账户里只剩下1000美元。\\n\\n这时候乙账户的保证金就不够了，因为前面说了，一份合约的保证金是总合约价值的10%，现在一份合约价值21000美元，21000X10%=2100（美元）。也就说乙如果想再继续持有合约，账户里至少得有2100美元的保证金，现在账户只有1000美元，所以得往账户里转入1100美元才可继续持有，这就叫追加保证金，一般期货公司会及时通知你，如果你不追加保证金，期货公司就会给你强行平仓。\\n\\n假设乙方追加保证金打算继续持有，结果到了第三天合约价格又变了，价格涨到了25000美元，每份合约又上涨了4000美元，这时候甲账户里应该就有3000+4000=7000（美元）。而乙呢，本来有2100美元，结果现在又亏了4000美元，所以他账户还剩下负1900美元，这时候期货公司又找到乙说，你保证金又不够了，得继续追加，这时候得追加多少？一份合约价值25000元，10%保证金，2500+1900=4400（美元），也就是这时候乙需要追加保证金4400美元，如果乙方不追加保证金，会被强行平仓。\\n\\n如果乙方说“我没钱了，不追加，平仓吧”。现在怎么平仓呢？上面说过，甲和乙之前立了约，乙是按照2万的价格卖给甲原油的，现在双方平仓，相当于甲又把这个原油卖回给乙，而再卖回给乙的价格是25000美元，所以甲最后就赚了5000美元，乙最后亏了5000美元。加上初始的2000美元保证金，甲账户这时候就有7000美元了。而乙原来账户2000，追加1100，是3100，总共亏5000美元，所以他现在欠期货公司1900美元，这1900美元乙必须得还，这就是期货交易里所说的穿仓。这种情况的发生，就是因期货交易的保证金下的高杠杆所致，杠杆不仅能放大收益，同时也能放大风险，所以投资期货就是一念天堂，一念地狱。\\n这里需要注意的是，真实的期货交易市场中，市场中的多方和空方很多，而且做多的单和做空的单数量是一样的，所以多方如果想平仓，可以与任何也想在这时平仓的空方交易，并不是像上面的例子中那样，开仓时的多空两方必须平仓时也是他们之间，因为很可能出现，开仓时的甲想平仓了，但乙想继续追加保证金不打算平仓，那么甲就只能找其他想在这时候平仓的空方交易。\\n\\n期货不能长期持有\\n\\n除了杠杆的特性，期货也不能像股票一样长期持有，股票被套了，我们可以一直持有，也许某一天就涨回来了，但期货是不能这么做的，因为期货合约里有到期日和交割的规定。比如甲乙双方买卖的是7月份交割的原油期货合约，那么快到交割日时这个合约就不能再期货市场里交易了。如果你没有能力去交割，持有合约的人必须在到期日前平仓。\\n\\n期货中还有一种方式叫做移仓换月或者叫展期。比如我们现在买卖的是6月份交割的合约，现在马上要到期了，所以我们只能先把6月份交割的合约先平仓，然后再买入7月份才交割的合约，这样我们就可以继续买卖，继续赚钱了，这就叫移仓换月。一般移仓换月发生在最后交易日之前的几天里，而“中行原油宝”事件，之所以发生巨大亏损，就是因为他忘了提前移仓换月，等快到最后交易日了才想起做移仓换月，结果这时候市场的流动性已经快没了，人家该平仓的早就平了，你作为仅剩的多方，当然还有和你持单量一样多的空方，但人家就是撑着不和你交易，互相平仓，如果真到交割日你就得交割了。不想交割，你就得不断降价。\\n\\n交割有两种方式，一种是实物交割，一种是现金交割，因为有些东西是没法实物交割的，比如股指期货、利率期货等，只能按照现金来交割。而“原油宝”这个产品的交易标的—WTI原油是实物交割的，如果你持有多单，交割日时你就必须以某个价格去获得1000桶原油，这是你的责任，必须履行。只不过这个价格是可以商量的，比如开仓的时候一桶WTI原油价格是20美元，你认为20美元这个价格是合理的，结果就拿了一个多单，也就是以后你可以以20美元的价格获得一桶油，两万美元的价格获得1000桶原油，现在你不想要原油了，因为你本身就是一个炒期货的投机者，并不想去美国库欣那儿提原油，也没那个物力和财力去提。所以现在你必须在交割日前把这个合约平仓，\\n但现在你卖20美元没人要呀，那就继续降价，卖10美元，因为新冠疫情消费不振，石油供过于求，一看10美元还是没人要，那就卖5美元，还是没人要，那就卖0.01美元吧。这就意味着接你单的人只需要拿0.01美元就可以买一桶原油，那么1000桶原油只需要10美元。这么便宜的价格居然还是没人要。这时候多方的你指望着空方接单，但空方的处境和多方是一样的，空方也必须在交割日前把自己的仓平掉，否则他就要付出某个价格，提供一些原油了。\\n\\n对多方的你来说，最大的问题是你并不知道这些空方到底是谁，也许这些空方背后就有大型石油公司支持，他们不怕最后实物交割。但问题是作为普通投资者的你怕实物交割呀，空方如果最后真给你石油，你没法去取，更没地方放。如果你说我可不可以把石油放在仓库里，不拿走，先储存着。不好意思，不可以，这样做了，交易所是要罚你钱的。如果你说找运油车把这些石油运走呢，那你就得付不菲的运输费，而且运到哪里去呢？你说如果我像倒牛奶一样把他们倒地里呢，那就更不行了，严重污染环境，罚款更多。\\n\\n在这种情况下，你没法交割原油，空方又不愿意接你的单，于是价格就一直往下压，最终变成负数，最后直到负37.63美元时空方才勉为其难地才接了单，而负37.63美元每桶的价格意味着谁接了你的这个单，不仅可以拿走属于你的原油，而且每桶原油，你还额外的补助空方37.63美元，空方拿了这些钱，就可以去运输和储存这些油，这样空方帮你平了仓，而他们拿到的油也许正是他们需要的。\\n\\n是的，期货就是这么刺激和残酷，他的高杠杆特点在放大收益和风险的同时，其实也进一步强化了人性中的贪婪和恐惧。但是如果我们在投资的过程中合理使用期货工具来做风险对冲，或者控制好仓位的前提下做熟悉的相关品种的套利，那么期货真的就是一种很完美的投资工具，因为它是世界上唯一可以在资产价格下跌时可以让投资者赚钱的投资工具。'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guantaco_dataset[4824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853a5268-f5f5-4c11-a861-637f75c523bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST]answer the following multiple choice question with a single letter matching the option for the correct answer. use the provided context with your own knowledge to find the correct answer.the set of possible letters for options is A, B, C, D, E. answer with the letter of the correct answer only, and nothing more. \\n\\ncontext: Some of these standards are examples of dynamical time scales and/or of coordinate time scales. A standard for civil time can specify both time intervals and time-of- day. Modern civil time is generally national standard time in a time zone at a fixed offset from Coordinated Universal Time (UTC), possibly adjusted by daylight saving time during part of the year. As defined, TCB (as observed from the Earth\\'s surface) is of divergent rate relative to all of ET, Teph and TDT/TT;P K Seidelmann & T Fukushima (1992), \"Why new time scales?\", Astronomy & Astrophysics vol.265 (1992), pages 833-838, including Fig. 1 at p.835, a graph giving an overview of the rate differences and offsets between various standard time scales, present and past, defined by the IAU. and the same is true, to a lesser extent, of TCG. Civil Times around the world are all defined by reference to UTC. TDT has since been redefined as Terrestrial Time (TT). International Atomic Time (TAI)TAI is the primary physically realized time standard. TDB is similar to TDT but includes relativistic corrections that move the origin to the barycenter, hence it is a dynamical time at the barycenter.V Brumberg, S Kopeikin (1990), \\'Relativistic time scales in the solar system\\', Celestial Mechanics and Dynamical Astronomy (1990), Vol. 48, 23-44 TDB differs from TT only in periodic terms. Standard time or civil time in a time zone deviates a fixed, round amount, usually a whole number of hours, from some form of Universal Time, usually UTC. TAI is not related to TCG directly but rather is a realization of Terrestrial Time (TT), a theoretical timescale that is a rescaling of TCG such that the time rate approximately matches proper time at mean sea level. But Coordinated Universal Time (UTC) (an atomic-based time scale which is always kept within 0.9 second of UT1) is in common actual use in the UK, and the name GMT is often used to refer to it. For applications at the Earth\\'s surface, ET\\'s official replacement was Terrestrial Dynamical Time (TDT), which maintained continuity with it. Time scale may refer to: *Time standard, a specification of either the rate at which time passes, points in time, or both *A duration or quantity of time: **Orders of magnitude (time) as a power of 10 in seconds; **A specific unit of time *Geological time scale, a scale that divides up the history of Earth into scientifically meaningful periods In astronomy and physics: *Dynamical time scale, in stellar physics, the time in which changes in one part of a body can be communicated to the rest of that body, or in celestial mechanics, a realization of a time-like argument based on a dynamical theory *Nuclear timescale, an estimate of the lifetime of a star based solely on its rate of fuel consumption *Thermal time scale, an estimate of the lifetime of a star once the fuel reserves at its center are used up In cosmology and particle physics: *Planck time, the time scale beneath which quantum effects are comparable in significance to gravitational effects In mathematics: *Time- scale calculus, the unification of the theory of difference equations with differential equations In music: *Rhythm, a temporal pattern of events *Time scale (music), which divides music into sections of time In project management: *Man-hour, the time scale used in project management to account for human labor planned or utilized The SI second is the basis of all atomic timescales, e.g. coordinated universal time, GPS time, International Atomic Time, etc. == Current time standards == Geocentric Coordinate Time (TCG) is a coordinate time having its spatial origin at the center of Earth\\'s mass. In modern usage, civil time refers to statutory time as designated by civilian authorities. Barycentric Coordinate Time (TCB) is a coordinate time having its spatial origin at the center of mass of the Solar System, which is called the barycenter. === Conversions === Conversions between atomic time systems (TAI, GPST, and UTC) are for the most part exact. An example of a kind of time standard can be a time scale, specifying a method for measuring divisions of time. Coordinated Universal Time (UTC) is an atomic time scale designed to approximate UT1. Various types of second and day are used as the basic time interval for most time scales. Versions of Universal Time such as UT0 and UT2 have been defined but are no longer in use. ==Time standards for planetary motion calculations== Ephemeris time (ET) and its successor time scales described below have all been intended for astronomical use, e.g. in planetary motion calculations, with aims including uniformity, in particular, freedom from irregularities of Earth rotation. \\n\\nquestion: Which time scale is often used for civil purposes?\\n\\nanswer options:\\nA: Terrestrial Time (TT)\\nB: Coordinated Universal Time (UTC)\\nC: International Atomic Time (TAI)\\nD: Terrestrial Dynamical Time (TDT)\\nE: Barycentric Dynamical Time (TDB)\\n[/INST]\\n\\nB\\n</s>',\n",
       " 'size': 1296}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7594c3eb-2545-4074-b19c-c6326cdcf8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]answer the following multiple choice question with a single letter matching the option for the correct answer. use the provided context with your own knowledge to find the correct answer.the set of possible letters for options is A, B, C, D, E. answer with the letter of the correct answer only, and nothing more. \n",
      "\n",
      "context: The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial, since although luminous matter such as stars and galaxies are easily summed, baryonic matter can also exist in highly non-luminous form, such as black holes, planets, and highly diffuse interstellar gas. Cosmological hydrodynamical simulations from theory predict that a fraction of the missing baryons are located in galactic haloes at temperatures of 106 K and the (WHIM) at temperatures of 105–107 K, with recent observations providing strong support. 50x50px Available under CC BY 4.0. In models for the gravitational formation of structure with cold dark matter, the smallest structures collapse first and eventually build the largest structures, clusters of galaxies. Large scale galaxy surveys in the 2000s revealed a baryon deficit. At the same time, a census of baryons in the recent observable universe has found that observed baryonic matter accounts for less than half of that amount. A mass deficit is the amount of mass (in stars) that has been removed from the center of a galaxy, presumably by the action of a binary supermassive black hole. thumb|left|The figure illustrates how mass deficits are measured, using the observed brightness profile of a galaxy The density of stars increases toward the center in most galaxies. One claim of a solution was published in 2017 when two groups of scientists said they found evidence for the location of missing baryons in intergalactic matter. When observed visually, clusters appear to be collections of galaxies held together by mutual gravitational attraction. \n",
      "\n",
      "question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "\n",
      "answer options:\n",
      "A: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
      "B: MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\n",
      "C: MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\n",
      "D: MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
      "E: MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n",
      "[/INST]\n",
      "\n",
      "D\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92692de-c73f-47b8-9ca9-7a0f2c55e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"[INST]answer the following multiple choice question with a single letter matching the option for the correct answer. use the provided context with your own knowledge to find the correct answer.the set of possible letters for options is A, B, C, D, E. answer with the letter of the correct answer only, and nothing more.\n",
    "\n",
    "context: The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial, since although luminous matter such as stars and galaxies are easily summed, baryonic matter can also exist in highly non-luminous form, such as black holes, planets, and highly diffuse interstellar gas. Cosmological hydrodynamical simulations from theory predict that a fraction of the missing baryons are located in galactic haloes at temperatures of 106 K and the (WHIM) at temperatures of 105–107 K, with recent observations providing strong support. 50x50px Available under CC BY 4.0. In models for the gravitational formation of structure with cold dark matter, the smallest structures collapse first and eventually build the largest structures, clusters of galaxies. Large scale galaxy surveys in the 2000s revealed a baryon deficit. At the same time, a census of baryons in the recent observable universe has found that observed baryonic matter accounts for less than half of that amount. A mass deficit is the amount of mass (in stars) that has been removed from the center of a galaxy, presumably by the action of a binary supermassive black hole. thumb|left|The figure illustrates how mass deficits are measured, using the observed brightness profile of a galaxy The density of stars increases toward the center in most galaxies. One claim of a solution was published in 2017 when two groups of scientists said they found evidence for the location of missing baryons in intergalactic matter. When observed visually, clusters appear to be collections of galaxies held together by mutual gravitational attraction. \n",
    "\n",
    "question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
    "\n",
    "answer options:\n",
    "A: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\\\n",
    "B: MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\\\n",
    "C: MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\\\n",
    "D: MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\\\n",
    "E: MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\\\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d75d61b-1a89-4c94-84d4-3d7974ffca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from transformers.generation import GenerationConfig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e352fe3-aa8c-42bf-86f7-2274047fee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]answer the following multiple choice question with a single letter matching the option for the correct answer. use the provided context with your own knowledge to find the correct answer.the set of possible letters for options is A, B, C, D, E. answer with the letter of the correct answer only, and nothing more.\n",
      "\n",
      "context: The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial, since although luminous matter such as stars and galaxies are easily summed, baryonic matter can also exist in highly non-luminous form, such as black holes, planets, and highly diffuse interstellar gas. Cosmological hydrodynamical simulations from theory predict that a fraction of the missing baryons are located in galactic haloes at temperatures of 106 K and the (WHIM) at temperatures of 105–107 K, with recent observations providing strong support. 50x50px Available under CC BY 4.0. In models for the gravitational formation of structure with cold dark matter, the smallest structures collapse first and eventually build the largest structures, clusters of galaxies. Large scale galaxy surveys in the 2000s revealed a baryon deficit. At the same time, a census of baryons in the recent observable universe has found that observed baryonic matter accounts for less than half of that amount. A mass deficit is the amount of mass (in stars) that has been removed from the center of a galaxy, presumably by the action of a binary supermassive black hole. thumb|left|The figure illustrates how mass deficits are measured, using the observed brightness profile of a galaxy The density of stars increases toward the center in most galaxies. One claim of a solution was published in 2017 when two groups of scientists said they found evidence for the location of missing baryons in intergalactic matter. When observed visually, clusters appear to be collections of galaxies held together by mutual gravitational attraction. \n",
      "\n",
      "question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "\n",
      "answer options:\n",
      "A: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"B: MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.C: MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.D: MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.E: MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.[/INST]\n",
      "\n",
      "A\n",
      "CPU times: user 718 ms, sys: 0 ns, total: 718 ms\n",
      "Wall time: 717 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch_device = \"cuda:0\"\n",
    "model_inputs = tokenizer(q, return_tensors='pt').to(torch_device)\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=3, eos_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc2890f-9fba-454c-a654-1eccd9b59bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]answer the following multiple choice question with a single letter matching the option for the correct answer. use the provided context with your own knowledge to find the correct answer.the set of possible letters for options is A, B, C, D, E. answer with the letter of the correct answer only, and nothing more.\n",
      "\n",
      "context: The presence of a clustered thick disk-like component of dark matter in the Galaxy has been suggested by Sanchez-Salcedo (1997, 1999) and Kerins (1997).Kerins, E. J. 1997, Astronomy and Astrophysics, 322, 709-718 (ADS entry )Sánchez-Salcedo, F. J. 1997, Astrophysical Journal, 487, L61-L64 (ADS entry )Sánchez-Salcedo, F. J. 1999, Monthly Notices of the Royal Astronomical Society, 303, 755-772 (ADS entry ) ==See also== * Dark matter * Brown dwarfs * White dwarfs * Microlensing * Hypercompact stellar system * Massive compact halo object (MACHOs) * Weakly interacting massive particles (WIMPs) ==References== Category:Star clusters Category:Open clusters Observations of the Bullet Cluster are the strongest evidence for the existence of dark matter; however, Brownstein and Moffat have shown that their modified gravity theory can also account for the properties of the cluster. == Observational methods == Clusters of galaxies have been found in surveys by a number of observational techniques and have been studied in detail using many methods: * Optical or infrared: The individual galaxies of clusters can be studied through optical or infrared imaging and spectroscopy. The observed distortions can be used to model the distribution of dark matter in the cluster. == Temperature and density == Clusters of galaxies are the most recent and most massive objects to have arisen in the hierarchical structure formation of the Universe and the study of clusters tells one about the way galaxies form and evolve. A 2021 article postulated that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies, and that this would explain the missing baryons not accounted for in the 2017 paper. == Current state == Currently, many groups have observed the intergalactic medium and circum-galactic medium to obtain more measurements and observations of baryons to support the leading observations. In cosmology, the missing baryon problem is an observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs. Brownstein and Moffat use a theory of modified gravity to explain X-ray cluster masses without dark matter. The missing baryon problem has been resolved but research groups are working to detect the WHIM using varying methods to confirm results. ==References== Category:Physical cosmology Category:Baryons Baryons make up only ~5% of the universe, while dark matter makes up 26.8%. ==Early universe measurements== The abundance of baryonic matter in the early universe can be obtained indirectly from two independent methods: * The theory of Big Bang nucleosynthesis, which predicts the observed relative abundance of the chemical elements in observations of the recent universe. The missing baryon problem is different from the dark matter problem, which is non-baryonic in nature.See Lambda-CDM model. In a typical cluster perhaps only 5% of the total mass is in the form of galaxies, maybe 10% in the form of hot X-ray emitting gas and the remainder is dark matter. In astronomy, a RAMBO or robust association of massive baryonic objects is a dark cluster made of brown dwarfs or white dwarfs. It is composed of mostly ionized hydrogen and is about 10% of a galaxy cluster's total mass; the rest being dark matter. This is highly nontrivial, since although luminous matter such as stars and galaxies are easily summed, baryonic matter can also exist in highly non-luminous form, such as black holes, planets, and highly diffuse interstellar gas. Cosmological hydrodynamical simulations from theory predict that a fraction of the missing baryons are located in galactic haloes at temperatures of 106 K and the (WHIM) at temperatures of 105–107 K, with recent observations providing strong support. 50x50px Available under CC BY 4.0. In models for the gravitational formation of structure with cold dark matter, the smallest structures collapse first and eventually build the largest structures, clusters of galaxies. Large scale galaxy surveys in the 2000s revealed a baryon deficit. At the same time, a census of baryons in the recent observable universe has found that observed baryonic matter accounts for less than half of that amount. A mass deficit is the amount of mass (in stars) that has been removed from the center of a galaxy, presumably by the action of a binary supermassive black hole. thumb|left|The figure illustrates how mass deficits are measured, using the observed brightness profile of a galaxy The density of stars increases toward the center in most galaxies. One claim of a solution was published in 2017 when two groups of scientists said they found evidence for the location of missing baryons in intergalactic matter. When observed visually, clusters appear to be collections of galaxies held together by mutual gravitational attraction. \n",
      "\n",
      "question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "\n",
      "answer options:\n",
      "A: MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"B: MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.C: MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.D: MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.E: MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.[/INST]\n",
      "\n",
      "D\n",
      "\n",
      "The observed discrepancy between the amount of baryonic matter detected from shortly after the Big Bang and from more recent epochs is the missing baryon problem.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch_device = \"cuda:0\"\n",
    "model_inputs = tokenizer(q, return_tensors='pt').to(torch_device)\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40, eos_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99c910b-00a1-4e48-a864-edb10e4aa354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LICENSE.txt\t\t\t  pytorch_model-00001-of-00002.bin\n",
      "README.md\t\t\t  pytorch_model-00002-of-00002.bin\n",
      "USE_POLICY.md\t\t\t  pytorch_model.bin.index.json\n",
      "config.json\t\t\t  special_tokens_map.json\n",
      "generation_config.json\t\t  tokenizer.json\n",
      "model-00001-of-00002.safetensors  tokenizer.model\n",
      "model-00002-of-00002.safetensors  tokenizer_config.json\n",
      "model.safetensors.index.json\n"
     ]
    }
   ],
   "source": [
    "!ls /mystuff/llm/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3060482-cb9c-4dd2-9b75-5ad9b842dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = \"/mystuff/llm/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3183788-8d3e-4800-aec6-ca08071d73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = \"/mystuff/notebooks/llama_finetuned_base_8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa66f4-e2c0-4d85-8922-64c2ab92fb00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
