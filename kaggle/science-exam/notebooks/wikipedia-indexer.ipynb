{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a5bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:14.233714657Z",
     "start_time": "2023-07-26T07:15:13.911841550Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a2c13-cc03-49b8-8182-7d409f440705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:14.537750907Z",
     "start_time": "2023-07-26T07:15:14.229797687Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342f9d4-7b4b-4428-8147-23ce88fa30b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:14.718389891Z",
     "start_time": "2023-07-26T07:15:14.533117634Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9aa574-b485-4cdf-af60-1936341ada7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:15.000635062Z",
     "start_time": "2023-07-26T07:15:14.979156766Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f49cc-ab5b-472f-84d1-af347ab1f8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:15.690611655Z",
     "start_time": "2023-07-26T07:15:15.478865461Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99fe04-6c21-43bf-925c-d1912afb31e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:20.313784231Z",
     "start_time": "2023-07-26T07:15:17.676048452Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m torch.utils.collect_env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64db39-0084-459b-b3bd-550237aedbf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:20.458319978Z",
     "start_time": "2023-07-26T07:15:20.297851414Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da995c27-07f6-46bb-ad8d-14fecab9ba18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:22.472921130Z",
     "start_time": "2023-07-26T07:15:20.430375275Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c597b44-2094-475f-9f36-f8c661ff643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4f7fe-da8b-481d-b967-6b7084a4f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mystuff/wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb738aa0-2841-4464-80ac-97a936e7011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_parquet(\"/mystuff/wikipedia/a.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d78bbe-bccd-4985-ad2c-fefdbed0f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path containing the .parquet files\n",
    "folder_path = '/mystuff/wikipedia'\n",
    "\n",
    "# List all files with .parquet extension\n",
    "parquet_files = [f for f in os.listdir(folder_path) if f.endswith('.parquet') and \"wiki_2023\" not in f]\n",
    "\n",
    "# Load each file into a DataFrame and store in a dictionary\n",
    "dataframes = {}\n",
    "for file in parquet_files:\n",
    "    key = file.rsplit('.', 1)[0]  # Remove the .parquet extension to get the key\n",
    "    dataframes[key] = pd.read_parquet(os.path.join(folder_path, file))\n",
    "\n",
    "print(dataframes.keys())  # This will print all the keys (i.e., filenames without .parquet extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d031c-76bc-4f09-9018-50befcfbb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ad150-dc37-4725-93ca-6ecc2b2609cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_a.iloc[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c7da3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:15:44.112381673Z",
     "start_time": "2023-07-26T07:15:44.096416452Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df_a.iloc[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b073108-eaf2-4aa9-9851-d11335d1ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_identifiers(text):\n",
    "    # The regular expression looks for '==', followed by any number of characters \n",
    "    # that aren't '=', followed by another '=='. This captures the IDENTIFIER.\n",
    "    return re.findall(r'==([^=]+)==', text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789c76d-3e23-48be-8a4e-5d42cc511001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'text' column and get the unique values\n",
    "unique_identifiers = df_a['text'].apply(extract_identifiers).explode().unique()\n",
    "\n",
    "print(unique_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613f596-5912-4a10-86cc-07c2f6a7fe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b8c07-458c-4ca6-9876-ab4e16e4a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Applying the function to extract identifiers for each document\n",
    "df_a['identifiers'] = df_a['text'].apply(extract_identifiers)\n",
    "\n",
    "# Finding the common identifiers across all documents\n",
    "common_identifiers = set(df_a['identifiers'].iloc[0])\n",
    "for ids in df_a['identifiers']:\n",
    "    common_identifiers.intersection_update(ids)\n",
    "\n",
    "print(common_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ca90f-01fe-4d2a-8174-760bb6020af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to extract identifiers for each document\n",
    "all_identifiers = df_a['text'].apply(extract_identifiers).explode()\n",
    "\n",
    "# Getting the count of each identifier and displaying the top 10\n",
    "top_10_identifiers = all_identifiers.value_counts().head(20)\n",
    "print(top_10_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3659382-c737-450b-b1d9-88ae718bd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdb373-e43b-402d-a5b0-de4920b1af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def extract_raw_identifiers(text):\n",
    "    return re.findall(r'==([^=]+)==', text)\n",
    "\n",
    "# Extracting all raw identifiers\n",
    "print(\"extracting\")\n",
    "df_a['raw_identifiers'] = df_a['text'].apply(extract_raw_identifiers)\n",
    "print(\"exploding\")\n",
    "df_exploded = df_a.explode('raw_identifiers')\n",
    "\n",
    "print(\"unique\")\n",
    "# Grouping by stripped identifiers to gather variations\n",
    "identifier_variations = (df_exploded.groupby(df_exploded['raw_identifiers'].str.strip())\n",
    "                         ['raw_identifiers'].unique())\n",
    "\n",
    "print(\"keeping only multiple variations\")\n",
    "# Filtering to keep only entries with multiple variations\n",
    "identifier_variations = identifier_variations[identifier_variations.str.len() > 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f42f51-7985-42c8-9d8d-86dd86b3586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379f62f-a00f-450e-876d-ddefaf05fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching example documents for each variation\n",
    "example_docs = {}\n",
    "for identifier, variations in tqdm(identifier_variations.items()):\n",
    "    example_docs[identifier] = {}\n",
    "    for variation in variations:\n",
    "        example_text = df_exploded[df_exploded['raw_identifiers'] == variation]['text'].iloc[0]\n",
    "        example_docs[identifier][variation] = example_text\n",
    "\n",
    "for identifier, docs in example_docs.items():\n",
    "    print(f\"Identifier: {identifier}\")\n",
    "    for variation, doc in docs.items():\n",
    "        print(f\"  Variation: {variation}\\n  Example Document: {doc}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d51ca-5a1c-4c6c-9b72-c396cb224253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting identifiers directly from text without normalization\n",
    "def extract_raw_identifiers(text):\n",
    "    return re.findall(r'==([^=]+)==', text)\n",
    "\n",
    "df_a['raw_identifiers'] = df_a['text'].apply(extract_raw_identifiers)\n",
    "df_exploded = df_a.explode('raw_identifiers')\n",
    "\n",
    "print(\"get value counts\")\n",
    "# Normalizing identifiers and counting their frequencies\n",
    "identifier_counts = df_exploded['raw_identifiers'].str.strip().value_counts()\n",
    "\n",
    "# Filter normalized identifiers with count over 1000\n",
    "print(\"filter over 1000\")\n",
    "popular_identifiers = identifier_counts[identifier_counts > 1000].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e80c3-0cc5-4c94-a588-7c4626bde4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"collect example docs\")\n",
    "# Gather variations for these identifiers and fetch example docs\n",
    "example_docs = {}\n",
    "for identifier in tqdm(popular_identifiers):\n",
    "    variations = df_exploded[df_exploded['raw_identifiers'].str.strip() == identifier]['raw_identifiers'].unique()\n",
    "    \n",
    "    # Check for multiple variations\n",
    "    if len(variations) > 1:\n",
    "        example_docs[identifier] = {}\n",
    "        for variation in variations:\n",
    "            example_text = df_exploded[df_exploded['raw_identifiers'] == variation]['text'].iloc[0]\n",
    "            example_docs[identifier][variation] = example_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2cf1a2-ad89-4561-aa7d-f20fb0594c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"print example docs\")\n",
    "for identifier, docs in tqdm(example_docs.items()):\n",
    "    print(f\"Identifier: {identifier}\")\n",
    "    for variation, doc in docs.items():\n",
    "        print(f\"  Variation: '{variation}'\\n  Example Document: {doc}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9cce4-3c81-43ca-a3c2-1e23feb562ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting identifiers directly from text without normalization\n",
    "def extract_raw_identifiers(text):\n",
    "    return re.findall(r'==([^=]+)==', text)\n",
    "\n",
    "# Normalize and count identifiers\n",
    "df_exploded['normalized_identifiers'] = df_exploded['raw_identifiers'].str.strip()\n",
    "identifier_counts = df_exploded['normalized_identifiers'].value_counts()\n",
    "\n",
    "# Only consider identifiers with more than 1000 occurrences\n",
    "popular_identifiers = identifier_counts[identifier_counts > 1000].index\n",
    "\n",
    "# Filter the mapping dictionary to consider only prevalent identifiers\n",
    "variation_to_normalized = {\n",
    "    var: var.strip()\n",
    "    for var in df_exploded['raw_identifiers'].unique()\n",
    "    if isinstance(var, str) and var.strip() in popular_identifiers\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afb066-a6f2-4f30-90c7-21381b603a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variation_to_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06315535-b511-4b50-852e-ffc8d0c40631",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Replace all variations with the normalized form using regex\n",
    "pattern = re.compile('==' + '|'.join(map(re.escape, variation_to_normalized.keys())) + '==')\n",
    "#def replace_with_normalized(text):\n",
    "#    return pattern.sub(lambda m: '==' + variation_to_normalized[m.group(0)] + '==', text)\n",
    "def replace_with_normalized(text):\n",
    "    return pattern.sub(lambda m: '==' + variation_to_normalized.get(m.group(0), m.group(0).strip()) + '==', text)\n",
    "\n",
    "df_a['normalized_text'] = df_a['text'].apply(replace_with_normalized)\n",
    "#df_a['normalized_text'] = df_a['text'].progress_apply(replace_with_normalized)\n",
    "\n",
    "print(df_a[['text', 'normalized_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c3bec-74ca-42de-9be0-b1c74d94990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Extracting identifiers directly from text without normalization\n",
    "def extract_raw_identifiers(text):\n",
    "    return re.findall(r'==([^=]+)==', text)\n",
    "\n",
    "def normalize_data(df_to_normalize):\n",
    "    # Extracting all raw identifiers\n",
    "    df_to_normalize['raw_identifiers'] = df_to_normalize['text'].progress_apply(extract_raw_identifiers)\n",
    "    df_exploded = df_to_normalize.explode('raw_identifiers')\n",
    "    \n",
    "    # Normalize and count identifiers\n",
    "    df_exploded['normalized_identifiers'] = df_exploded['raw_identifiers'].str.strip()\n",
    "    identifier_counts = df_exploded['normalized_identifiers'].value_counts()\n",
    "    \n",
    "    # Only consider identifiers with more than 1000 occurrences\n",
    "    popular_identifiers = identifier_counts[identifier_counts > 100].index\n",
    "    \n",
    "    # Filter the mapping dictionary to consider only prevalent identifiers\n",
    "    variation_to_normalized = {\n",
    "        var: var.strip()\n",
    "        for var in df_exploded['raw_identifiers'].unique()\n",
    "        if isinstance(var, str) and var.strip() in popular_identifiers\n",
    "    }\n",
    "    \n",
    "    # Replace all variations with the normalized form using regex\n",
    "    pattern = re.compile('|'.join(map(lambda x: '==' + re.escape(x) + '==', variation_to_normalized.keys())))\n",
    "    def replace_with_normalized(text):\n",
    "        return pattern.sub(lambda m: '==' + variation_to_normalized.get(m.group(0).strip('=='), m.group(0).strip('==')) + '==', text)\n",
    "    \n",
    "    df_to_normalize['normalized_text'] = df_to_normalize['text'].progress_apply(replace_with_normalized)\n",
    "    \n",
    "    #print(df_to_normalize[['text', 'normalized_text']])\n",
    "    return df_to_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33edbb1b-c1b9-47eb-bb10-b8e4f30e4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataframes = {}\n",
    "for key in dataframes:\n",
    "    print(f\"normalizing {key}\")\n",
    "    df_normalized = normalize_data(dataframes[key])\n",
    "    normalized_dataframes[key] = df_normalized\n",
    "    print(f\"normalizing {key} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc54f4c-d5c0-4f2e-a7bd-0811740eaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataframes[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6826f-4bc2-4795-84d4-5bc9b2ca1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/mystuff/wikipedia'\n",
    "for key in normalized_dataframes:\n",
    "    normalized_dataframes[key].to_parquet(f\"{folder_path}/normalized_{key}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45033364-3e98-4a8b-ac19-380f68861a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to extract identifiers for each document\n",
    "all_identifiers = normalized_dataframes[\"a\"]['normalized_text'].apply(extract_identifiers).explode()\n",
    "\n",
    "# Getting the count of each identifier and displaying the top 10\n",
    "top_10_identifiers = all_identifiers.value_counts().head(20)\n",
    "print(top_10_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79bbad-187f-4f58-8350-f7aafa515d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
