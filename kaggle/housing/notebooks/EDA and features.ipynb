{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea60a849",
   "metadata": {},
   "source": [
    "# EDA tests\n",
    "\n",
    "Trying a few EDA tools using the basic housing regression dataset. Mostly it is about visualizing the data, also with the help of some automated analysis tools. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab77689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from autoviz.classify_method import data_cleaning_suggestions, data_suggestions\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70537d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88292d49",
   "metadata": {},
   "source": [
    "# Goal for Housing Dataset\n",
    "\n",
    "This data is from the Kaggle [Housing competition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques) for practicing regression problems.\n",
    "The goal here is to use these basic housing features to predict house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list + remove should preserve order\n",
    "data_columns = list(df_train.columns)\n",
    "data_columns.remove(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c985f1",
   "metadata": {},
   "source": [
    "## Data Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b3a1d",
   "metadata": {},
   "source": [
    "The file \"data_descriptions.txt\" should contain descriptions for the columns in the dataset. Building a simple mapping here for column and value identifiers to their names and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98cebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_description.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    #print(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b37dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e930d",
   "metadata": {},
   "source": [
    "The data_descriptions.txt file contains a description of each column, and for the categorical ones a mapping of values (\"20\", \"30\", \"40\" in above example) to their descriptions. Some columns lack a descriptions, I add those separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e290a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = None\n",
    "column_descriptions = {}\n",
    "key_descriptions = None\n",
    "\n",
    "# not all column actually have a description in the file, so need to add manually\n",
    "# the way to identify is to try them all and collect which ones are missing\n",
    "def add_missing_col_desc(col_name, col_desc):\n",
    "    column_descriptions[col_name] = {}\n",
    "    column_descriptions[col_name][\"description\"] = col_desc\n",
    "    column_descriptions[col_name][\"keys\"] = {}\n",
    "\n",
    "add_missing_col_desc(\"SalePrice\", \"Target variable\")\n",
    "    \n",
    "for line in tqdm(lines):\n",
    "    stripped = line.strip()\n",
    "    if len(stripped) > 0 and not line.startswith(\" \"):\n",
    "        # this branch happens when a new column description starts\n",
    "        parts = stripped.split(\":\")\n",
    "        column_name = parts[0]\n",
    "        column_description = parts[1].strip()\n",
    "        key_descriptions = {}\n",
    "        column_descriptions[column_name] = {}\n",
    "        column_descriptions[column_name][\"keys\"] = key_descriptions\n",
    "        column_descriptions[column_name][\"description\"] = column_description\n",
    "        print(f\"column: {column_name} = {column_description}\")\n",
    "    else:\n",
    "        # this branch happens when a column name is identified and its key is processed\n",
    "        parts = stripped.split(\"\\t\")\n",
    "        if len(parts) > 1:\n",
    "            key = parts[0].strip()\n",
    "            description = parts[1].strip()\n",
    "            key_descriptions[key] = description\n",
    "            #print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason these two are missing from the descriptions file, so had to make it up\n",
    "add_missing_col_desc(\"BedroomAbvGr\", \"Number of bedrooms above ground\")\n",
    "add_missing_col_desc(\"KitchenAbvGr\", \"Number of kitchens above ground\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4a54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a05168",
   "metadata": {},
   "source": [
    "# Visualizing Valuespaces\n",
    "\n",
    "First, I try to build a chart describing value space for one variable. Later once this works, it is easier to expand to other variables (columns) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b013680",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = data_columns[0]\n",
    "x = df_train[col_name]\n",
    "\n",
    "value_counts = x.value_counts().sort_index()\n",
    "keys = value_counts.index\n",
    "keys = [str(key) for key in keys]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(keys, value_counts)\n",
    "\n",
    "title = column_descriptions[col_name][\"description\"]\n",
    "title = f\"{col_name}: {title}\"\n",
    "ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "keys = column_descriptions[col_name][\"keys\"]\n",
    "keys_txt = \"\"\n",
    "for key in keys:\n",
    "    keys_txt += f\"{key}: {keys[key]}\\n\"\n",
    "ax.annotate(keys_txt, (0,0), (0, -20), xycoords='axes fraction', textcoords='offset points', va='top')\n",
    "\n",
    "ax.bar_label(bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f44b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# printing all column descriptions makes it a too long list for the lazy reader, \n",
    "# so i stick with 2 column_descriptions\n",
    "print(json.dumps(column_descriptions[data_columns[0]], indent=4))\n",
    "print(json.dumps(column_descriptions[data_columns[1]], indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1546ab6",
   "metadata": {},
   "source": [
    "# Fixing Typos in Data\n",
    "\n",
    "Some of the value descriptions in data_description.txt file do not match the actual column/key names in the data file itself. Because of typos or different form of writing. This replacement changes that, to make them match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\"C (all)\": 'C', \n",
    "                \"NAmes\": \"Names\", \n",
    "                \"Duplx\": \"Duplex\",\n",
    "                \"2fmCon\": \"2FmCon\",\n",
    "                \"Twnhs\": \"TwnhsI\",\n",
    "                \"CmentBd\": \"CemntBd\",\n",
    "                \"Brk Cmn\": \"BrkComm\",\n",
    "                \"Wd Shng\": \"WdShing\",\n",
    "               }\n",
    "\n",
    "def process_values(col_name, values):\n",
    "    if col_name == \"MSSubClass\":\n",
    "        return [str(v) for v in values]\n",
    "    # if it is in the above replacemens table use it, else leave it as is\n",
    "    values = [replacements[v] if v in replacements else v for v in values]\n",
    "    return values\n",
    "\n",
    "def process_keys(col_name, keys):\n",
    "    if col_name == \"OverallQual\" or col_name == \"OverallCond\":\n",
    "        return [int(v) for v in keys]\n",
    "    keys = [replacements[v] if v in replacements else v for v in keys ]\n",
    "    return keys\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1fde7",
   "metadata": {},
   "source": [
    "# Split Variables by Data Type\n",
    "\n",
    "Now to collect categorical vs numerical columns. Here numerical just means continuous variables. After this it is possible to process different types of data in different ways based on type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"SalePrice\"]\n",
    "cat_num_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "for col_name in data_columns:\n",
    "    if col_name not in column_descriptions:\n",
    "        print(f\"WARN: column not in descriptions: {col_name}\")\n",
    "        # here i added BedroomAbvGr and KitchenAbvGr to descriptions as they were missing\n",
    "        continue\n",
    "#    print(col_name)\n",
    "#    print(column_descriptions[col_name][\"keys\"])\n",
    "    keys = column_descriptions[col_name][\"keys\"].keys()\n",
    "    keys = process_keys(col_name, keys)\n",
    "    key_count = len(keys)\n",
    "    if key_count == 0:\n",
    "        print(f\"{col_name}: zero keys, assuming numeric (continuous) value\")\n",
    "        if df_train[col_name].nunique() < 20:\n",
    "            # some variables actually have very few unique values, so will treat them\n",
    "            # in some ways as categorical later (visualization type at least)\n",
    "            cat_num_cols.append(col_name)\n",
    "        else:\n",
    "            # these are the actual variables assumed to have continuous values\n",
    "            numeric_cols.append(col_name)\n",
    "        continue\n",
    "    else:\n",
    "        cat_cols.append(col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ea4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method used to check value space of missing variable\n",
    "# BedroomAvgGr seems to indicate number of bedrooms above ground by named values\n",
    "# while it might not be categorical as such, visualizing it using techniques for \n",
    "# datasets with a few distinct values later seems more useful than line charts for continous\n",
    "# mostly becaues there number of value options is so small, and in a way categorical even\n",
    "df_train[\"BedroomAbvGr\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085cca5",
   "metadata": {},
   "source": [
    "# Remove Columns with Single Value\n",
    "\n",
    "Sometimes there are columns with a single value, this checks for such columns and removes them as features is they exist. Should check for NaN values separately perhaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = []\n",
    "\n",
    "for col in df_train.columns:\n",
    "    #print(df_train[col].nunique())\n",
    "    if df_train[col].nunique() == 1:\n",
    "        print(\"match\")\n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcc810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6781815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this variable had a different value for missing/empty variables, so keeping in line with that\n",
    "df_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].fillna(\"None\")\n",
    "# and filling the rest with \"NA\"\n",
    "df_train[cat_cols] = df_train[cat_cols].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55341c3",
   "metadata": {},
   "source": [
    "# Check for Mismatch in Expected vs Found Values\n",
    "\n",
    "The data description text file has some values described for the categorical variables. Sometimes these differ from the ones found in the actual data. The following code builds two sets, one listing the values found in the csv file but not in the description text file. And one listing the values found in the text file but not in the csv data.\n",
    "\n",
    "## Collect Mismatched Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b3c70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# key = column name, value=list of values found in text file but not in data\n",
    "unknown_keys_dict = {}\n",
    "# key = column name, value=list of values found in data but not in text file\n",
    "unknown_values_dict = {}\n",
    "# key = column name, value = number of unique values for it found in data\n",
    "cat_value_counts = {}\n",
    "\n",
    "for col_name in cat_cols:\n",
    "    keys = column_descriptions[col_name][\"keys\"].keys()\n",
    "    keys = process_keys(col_name, keys)\n",
    "    key_count = len(keys)\n",
    "    unique_values = df_train[col_name].unique()\n",
    "    cat_value_counts[col_name] = df_train[col_name].value_counts()\n",
    "    unique_values = process_values(col_name, unique_values)\n",
    "    unique_count = len(unique_values)\n",
    "    unknown_keys = set(keys) - set(unique_values)\n",
    "    unknown_values = set(unique_values) - set(keys)\n",
    "    unknown_keys_dict[col_name] = unknown_keys\n",
    "    unknown_values_dict[col_name] = unknown_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd42be",
   "metadata": {},
   "source": [
    "## Values in Descriptions but not in Data\n",
    "\n",
    "Print all keys and their descriptions found in text file but not in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in unknown_keys_dict:\n",
    "    unknown_keys = unknown_keys_dict[col_name]\n",
    "    col_keys = column_descriptions[col_name][\"keys\"]\n",
    "    if len(unknown_keys) > 0:\n",
    "        # print names of columns with keys found in description text file but not in data\n",
    "        print(f\"{col_name}:\")\n",
    "        for key in unknown_keys:\n",
    "            # print the unknown values\n",
    "            print(f\"- {key}: {col_keys[str(key)]}\")\n",
    "            print(cat_value_counts[col_name].dtype)\n",
    "            if cat_value_counts[col_name].index.is_integer():\n",
    "                key = int(key)\n",
    "            # later on cat_value_counts may be used for visualization, so need to fill it\n",
    "            cat_value_counts[col_name][key] = 0\n",
    "        # print counts for found values in data, to compare why it might be missing\n",
    "        print(df_train[col_name].value_counts())\n",
    "        print(\"----------\")\n",
    "        print(cat_value_counts[col_name])\n",
    "        print()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6caeb8",
   "metadata": {},
   "source": [
    "## Values in Data but no in Descriptions\n",
    "\n",
    "Print values found in data but not in the descriptions text file.\n",
    "Here Eletrical has NA value but this is not in the text file.\n",
    "Perhaps due to earlier filling empty slots with NA. In any case, \n",
    "the empty (nan) value would not be there either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7030fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in unknown_values_dict:\n",
    "    unknown_values = unknown_values_dict[col_name]\n",
    "    col_keys = column_descriptions[col_name][\"keys\"]\n",
    "    if len(unknown_values) > 0:\n",
    "        print(f\"{col_name}: {unknown_values}\")\n",
    "        print(col_keys)\n",
    "        print(df_train[col_name].value_counts())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd941e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786e5002",
   "metadata": {},
   "source": [
    "# Visualize Categorical vs Continous Variables\n",
    "\n",
    "Now to visualize the different column values using bar charts for the categorical values and line charts for the continous values. Along with the descriptions from the text file. First try with just 2 columns to see how it might work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "for p in range(2):\n",
    "    col_name = data_columns[p]\n",
    "    x = df_train[col_name]\n",
    "\n",
    "    value_counts = cat_value_counts[col_name].sort_index()\n",
    "    keys = value_counts.index\n",
    "    keys = [str(key) for key in keys]\n",
    "\n",
    "    ax = axes[p]\n",
    "    bars = ax.bar(keys, value_counts)\n",
    "\n",
    "    title = column_descriptions[col_name][\"description\"]\n",
    "    title = f\"{col_name}:\"\n",
    "    print(title)\n",
    "    ax.set_title(title, fontsize=8, fontweight='bold')\n",
    "\n",
    "    keys = column_descriptions[col_name][\"keys\"]\n",
    "    keys_txt = \"\"\n",
    "    for key in keys:\n",
    "        keys_txt += f\"{key}: {keys[key]}\\n\"\n",
    "    ax.annotate(keys_txt, (0,0), (0, -20), xycoords='axes fraction', textcoords='offset points', va='top')\n",
    "\n",
    "    # add count labels on top of the bars\n",
    "    ax.bar_label(bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88109500",
   "metadata": {},
   "source": [
    "## Categorical Columns\n",
    "\n",
    "Next to visualize all categorical / low count numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e53457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following loop should be number of cat cols / 2 as 2 charts per row\n",
    "len(cat_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c75d3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2 charts per row, so 23*2=46\n",
    "for row in range(0,23):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "    for p in range(2):\n",
    "#        col_name = data_columns[row*2+p]\n",
    "        col_name = list(cat_value_counts.keys())[row*2+p]\n",
    "        x = df_train[col_name]\n",
    "\n",
    "    #    value_counts = x.value_counts().sort_index()\n",
    "        #print(cat_value_counts[col_name].index)\n",
    "        value_counts = cat_value_counts[col_name].sort_index()\n",
    "        #print(value_counts)\n",
    "        keys = value_counts.index\n",
    "        keys = [str(key) for key in keys]\n",
    "\n",
    "        ax = axes[p]\n",
    "        bars = ax.bar(keys, value_counts)\n",
    "\n",
    "        title_description = column_descriptions[col_name][\"description\"]\n",
    "        # the title + description was too long and charts became messy\n",
    "    #    title = f\"{col_name}: {title}\"\n",
    "        title = f\"{col_name}:\"\n",
    "        print(title+\" \"+title_description)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "        keys = column_descriptions[col_name][\"keys\"]\n",
    "        keys_txt = \"\"\n",
    "        for key in keys:\n",
    "            keys_txt += f\"{key}: {keys[key]}\\n\"\n",
    "        ax.annotate(keys_txt, (0,0), (0, -20), xycoords='axes fraction', textcoords='offset points', va='top')\n",
    "\n",
    "        ax.bar_label(bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1730a",
   "metadata": {},
   "source": [
    "In the above it shows why I wanted to add 0 as a value for those keys that had no value in data. This way the descriptions vs bars are easier to map together as there is always some value in both.\n",
    "\n",
    "The charts above also highlight how some columns might have very few distinct values, such as Utilities only having 2 classes and all but one value in the AllPublic category. Sometimes it can be useful information though, for example, to know the house has no sewage. So why not keep it? Well depends on the use case I guess. \n",
    "\n",
    "There are also many others with a bit more values, such as Street pavement type of PoolQC. One might combine these into features such as HasPool if the separate categories are very small. But again, I guess depends on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273fc08",
   "metadata": {},
   "source": [
    "# Outlier Analysis\n",
    "\n",
    "Before visualizing the continous variables, I will try two basic outlier detection methods. These are the [standard deviation - based method](https://stats.stackexchange.com/questions/575483/can-i-remove-sample-outliers-using-standard-deviation), and the [interquantile range - based method](https://math.stackexchange.com/questions/966331/why-john-tukey-set-1-5-iqr-to-detect-outliers-instead-of-1-or-2).\n",
    "\n",
    "Both of these methods are run, and the results are stored in their own dataframe each for later analysis.\n",
    "\n",
    "Looking at the results and the later visualizations, I believe it would make much more sense to look at the data and figure out what kind of operations to run on it first. In many cases here, the outlier removal does not seem to make much sense. I guess that is a good point to remember, if only I souldn't forget it by tomorrow anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_thresholds_high_std = {}\n",
    "outlier_thresholds_low_std = {}\n",
    "outlier_thresholds_high_tukey = {}\n",
    "outlier_thresholds_low_tukey = {}\n",
    "\n",
    "#assuming the data is normally distributed, this removes outliers using the \"three sigma\" rule.\n",
    "#that is 3*std from mean is expected to contain the 0.03% of smallest/highest values.\n",
    "#those are replaced with the min/mam here (actually just max in this implementation)\n",
    "def remove_outliers_normal(df, col):\n",
    "    df = df.copy()\n",
    "    upper = df[col].mean()+3*df[col].std()\n",
    "    lower = df[col].mean()-3*df[col].std()\n",
    "    #even variable outlier_thresholds does not exist in this code, since this method variant was not used by me in the end\n",
    "    #however, should be trivial to fix both min/max and thresholding\n",
    "    if col in outlier_thresholds_high_std:\n",
    "        upper = outlier_thresholds_high_std[col]\n",
    "        lower = outlier_thresholds_low_std[col]\n",
    "    else:\n",
    "        outlier_thresholds_high_std[col] = upper\n",
    "        outlier_thresholds_low_std[col] = lower\n",
    "        \n",
    "    high_mask = df[col] > upper\n",
    "    low_mask = df[col] < lower\n",
    "    df.loc[high_mask, col] = upper\n",
    "    df.loc[low_mask, col] = lower\n",
    "    print(f\"col: {col}, hight std: {sum(high_mask)}, low std: {sum(low_mask)}\")\n",
    "    return df\n",
    "    \n",
    "#tukey outlier removal should work for any distribution, not just normal.\n",
    "#the normal version above is just perhaps more focused for normal distributions\n",
    "def remove_outliers_tukey(df, col):\n",
    "    df = df.copy()\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    upper = q3 + iqr*2 #tukey default uses 1.5 multiplier, using *2 here to get more extreme outliers\n",
    "    lower = q1 - iqr*2 #*3 gave std up to 5.5x, lets see 2*\n",
    "    # if the values are really small, it messes this method up. so this just addresses that\n",
    "    if iqr < 1:\n",
    "        print(f\"very small IQR for {col}, defaulting to last quantile or 1% and 99% outside capping\")\n",
    "        upper = df[col].quantile(0.99)\n",
    "        lower = df[col].quantile(0.01)\n",
    "\n",
    "    if col in outlier_thresholds_high_tukey:\n",
    "        upper = outlier_thresholds_high_tukey[col]\n",
    "        lower = outlier_thresholds_low_tukey[col]\n",
    "    else:\n",
    "        outlier_thresholds_high_tukey[col] = upper\n",
    "        outlier_thresholds_low_tukey[col] = lower\n",
    "\n",
    "    high_mask = df[col] > upper\n",
    "    low_mask = df[col] < lower\n",
    "    print(f\"col: {col}, q1: {q1}, q3: {q3}, iqr: {iqr}, hight tukey: {sum(high_mask)}, low tukey: {sum(low_mask)}\")\n",
    "    \n",
    "    #print(sum(mask))\n",
    "    df.loc[high_mask, col] = upper\n",
    "\n",
    "    #print(sum(mask))\n",
    "    df.loc[low_mask, col] = lower\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_df_outliers(df):\n",
    "    df_tukey = df\n",
    "    df_std = df\n",
    "    for col in tqdm(numeric_cols):\n",
    "        #note that in its current implementation remove_outliers_tukey does not work across multiple dataframes\n",
    "        #if the outlier threshold is updated by a latter dataframe, the earlier ones used a different one\n",
    "        df_tukey = remove_outliers_tukey(df_tukey, col)\n",
    "        df_std = remove_outliers_normal(df_std, col)\n",
    "    return df_tukey, df_std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tukey, df_std = remove_df_outliers(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42608b",
   "metadata": {},
   "source": [
    "Convert the outlier thresholds for the different methods into a dataframe for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85123c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.DataFrame()\n",
    "df_thresholds[\"column_name\"] = numeric_cols\n",
    "\n",
    "tukey_lows = []\n",
    "tukey_highs = []\n",
    "std_lows = []\n",
    "std_highs = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tukey_lows.append(outlier_thresholds_low_tukey[col])\n",
    "    tukey_highs.append(outlier_thresholds_high_tukey[col])\n",
    "    std_lows.append(outlier_thresholds_low_std[col])\n",
    "    std_highs.append(outlier_thresholds_high_std[col])\n",
    "    \n",
    "df_thresholds[\"tukey_low\"] = tukey_lows\n",
    "df_thresholds[\"tukey_high\"] = tukey_highs\n",
    "df_thresholds[\"std_low\"] = std_lows\n",
    "df_thresholds[\"std_high\"] = std_highs\n",
    "df_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4efa2b",
   "metadata": {},
   "source": [
    "## Look at Saleprice Distribution before Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a520dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"SalePrice\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93125142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tukey[\"SalePrice\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std[\"SalePrice\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e104f",
   "metadata": {},
   "source": [
    "## Binning Continous Data for Plotting\n",
    "\n",
    "Now to plot the saleprice. First, collect the bin ranges to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = df_train[\"SalePrice\"].min()\n",
    "high = df_train[\"SalePrice\"].max()\n",
    "print(low)\n",
    "print(high)\n",
    "bin_ranges = np.arange(0, 800001, 50000)\n",
    "bin_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60edda75",
   "metadata": {},
   "source": [
    "Matplotlib with bar_label makes the labeling of bars easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaed5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.cut(df_train[\"SalePrice\"], bins=bin_ranges).value_counts()\n",
    "bins = bins.sort_index()\n",
    "bins = pd.DataFrame(data=bins, index=bin_ranges, columns=[\"SalePrice\"])\n",
    "ax = bins.plot(kind=\"bar\", figsize=(12, 8), width=0.9)\n",
    "\n",
    "# annotate\n",
    "ax.bar_label(ax.containers[0], label_type='edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f425b",
   "metadata": {},
   "source": [
    "Seaborn plots can be a little more pleasant looking, but labeling the bars seems difficult:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_palette(\"deep\")\n",
    "\n",
    "ax = sb.displot(df_train['SalePrice'].values,\n",
    "             bins=bin_ranges,\n",
    "             kde=True, aspect=2) #defaults to height = 5, aspect = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948dfc2b",
   "metadata": {},
   "source": [
    "# Plotting Continous Variables\n",
    "\n",
    "Earlier (above) I plotted the categorical columns. And above tried the two outlier methods. Now for plotting the continous variables and their values, while also comparing the \"raw\" data vs the two outlier approaches (std and Tukey):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframes(df1, df2, df3, title1, title2, title3):\n",
    "    figs_per_row = 3\n",
    "    fig_keys = numeric_cols\n",
    "    row_count = len(fig_keys)\n",
    "    fig_height = 5*row_count\n",
    "    fig, axes = plt.subplots(row_count, figs_per_row, figsize=(12,fig_height))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    dfs = [df1, df2, df3]\n",
    "    titles = [title1, title2, title3]\n",
    "    for p in range(len(numeric_cols)):\n",
    "        #col_name = data_columns[p]\n",
    "        col_name = numeric_cols[p]\n",
    "        for df_id in range(0,3):\n",
    "            df = dfs[df_id]\n",
    "            x = df[col_name]\n",
    "\n",
    "            #value_counts = x.value_counts()#.sort_index()\n",
    "            value_counts = df[col_name].value_counts().sort_index()\n",
    "            print(col_name)\n",
    "\n",
    "            p_y = p\n",
    "            ax = axes[p_y, df_id]\n",
    "            if len(value_counts) < 20:\n",
    "                keys = value_counts.index\n",
    "                keys = [str(key) for key in keys]\n",
    "                bars = ax.bar(keys, value_counts)\n",
    "                ax.bar_label(bars)\n",
    "            else:\n",
    "                lines = ax.plot(value_counts)\n",
    "\n",
    "            description = column_descriptions[col_name][\"description\"]\n",
    "            desc_title = f\"{col_name}: {description}\"\n",
    "            words = desc_title.split()\n",
    "            new_desc_title = \"\"\n",
    "            new_line = \"\"\n",
    "            for word in words:\n",
    "                if len(new_line) > 50:\n",
    "                    new_desc_title += new_line + \"\\n\"\n",
    "                    new_line = \"\"\n",
    "                else:\n",
    "                    new_line += word + \" \"\n",
    "            if len(new_line.strip()) > 0: \n",
    "                new_desc_title += new_line + \"\\n\"\n",
    "            desc_title = new_desc_title\n",
    "\n",
    "            title = f\"{col_name} ({titles[df_id]}):\"\n",
    "            print(title)\n",
    "            ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "            ax.annotate(desc_title, (0,0), (0, -20), xycoords='axes fraction', textcoords='offset points', va='top')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4ba1c",
   "metadata": {},
   "source": [
    "In the following figures, the leftmost is a plot of the 'raw' data, the middle is the tukey processed outlier removal, and rightmost is the std processed outlier removal. If the Tukey or STD versions have a high spike on the right, this is typically from capping the long right-side tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a59b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataframes(df_train, df_tukey, df_std, \"base\", \"tukey\", \"std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d4968",
   "metadata": {},
   "source": [
    "The zero values also show as high spikes on the left side, due to not having some property in the house. For example, MasVnrArea has a spike of about 800+ houses on the very left. Most likely due to most houses not having any such masonry (area size is 0).\n",
    "\n",
    "Overall, my impression from the above side-by-side charts is that the outlier removal here is not really useful. While much of the data has a long tail, the spike on the right shows that quite many values had to be capped. And those larger values in house properties may well be useful information.\n",
    "\n",
    "However, the outlier removals above do make the overall distribution more visible in many cases. So for such exploration purposes at least it seems useful. But in general, perhaps better to study the data bit before training any model on blindly capped outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42c577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33e2fdee",
   "metadata": {},
   "source": [
    "## Cleanup Suggestions after Outlier Removal\n",
    "\n",
    "With correlations taken care of, let's look at what the data cleaning package suggest to do for cleaning up the data in the differently processed dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ff8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_suggestions(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_suggestions(df_tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30191c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_suggestions(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4baa4",
   "metadata": {},
   "source": [
    "Some of the above look a little strange. For example BsmtFinSF2 and EnclosedPorch both have suggestions to remove outliers in the Tukey version, after the Tukey was all about removing outliers in the first place. And this suggestions is not in there for the original, uncapped version of the same data. The STD version of the data has even more of such suggestions. \n",
    "\n",
    "I would expect the tool to have rather suggested those for the original data, not the Tukey or STD outlier capped versions. Possibly something in how it translates the distribution, since the in the modified ones there are more values clusted at the right edge. Just a guess. \n",
    "\n",
    "But again, I would try to remember to check the data myself in detail rather than use only these automated tools, although they are useful aides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a740c9",
   "metadata": {},
   "source": [
    "## ProfileReports into HTML files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9fc9f",
   "metadata": {},
   "source": [
    "This actually writes some data summary reports into separate files. They seemed quite useful when looking at the generated files, for getting an overview. But harder to visualize in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ProfileReport(df_train,title='All features report')\n",
    "report.to_file(output_file='Numeric_Feature_EDA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81fc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404db60a",
   "metadata": {},
   "source": [
    "# Correlations Check\n",
    "\n",
    "Another common topic to check is to see correlations between different variables. As this is such as common topic, there is good support for calculating the correlations and visualizing them. First a correlation heatmap using Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119eb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train[numeric_cols].corr()\n",
    "#change annot=False to annot=True to get correlation values in the plot\n",
    "#in this case it is just a bit too crowded after adding those\n",
    "sb.heatmap(corr, cmap=\"Blues\", annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6803fcb",
   "metadata": {},
   "source": [
    "And the same correlation values in dataframe, numerical format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63518825",
   "metadata": {},
   "source": [
    "For a bit more detailed exploration of correlations, it is useful to be able to find the most correlated pairs, both negatively and positively. And why not the least correlated pairs.. \n",
    "\n",
    "These functions allow doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "\n",
    "# redundant pairs removes duplicate pairs, since typically correlation matrices show\n",
    "# both the a->b and b->a correlation.\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "# this provides the list of top correlations, either positive, negative, or both ways (absolute)\n",
    "# or even lear correlated (lowest absolute values)\n",
    "def get_top_abs_correlations(df, n=5, asc=False, absolute=True):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    if not absolute:\n",
    "        #in case we want to get highest negative value\n",
    "        au_corr = df.corr().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=asc)\n",
    "    return au_corr[0:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dca5a2",
   "metadata": {},
   "source": [
    "Lets check the results of running the above functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5dfde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are as far from each other as possible\n",
    "print(\"Top Absolute Un-Correlations\")\n",
    "print(get_top_abs_correlations(df_train[numeric_cols], 5, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7d08b",
   "metadata": {},
   "source": [
    "# Plot Highest Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a06eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most correlated pairs, positively or negatively\n",
    "print(\"Top Absolute Correlations\")\n",
    "highest_corrs = get_top_abs_correlations(df_train[numeric_cols], 5, False)\n",
    "highest_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2eddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_corrs.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33892ac5",
   "metadata": {},
   "source": [
    "Collect the actual column names for the highest correlations, for correlation plotting after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17802760",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x[0] for x in highest_corrs.index]\n",
    "cols += [x[1] for x in highest_corrs.index]\n",
    "cols = list(set(cols))\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16c024",
   "metadata": {},
   "source": [
    "Pairplots, plotting all variables against each other to see a form of a correlation matrix. First using seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37350dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(df_train[cols], hue=\"YearBuilt\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201fe28",
   "metadata": {},
   "source": [
    "The charts above look strange since the scatterplots seems compressed on x-axis. It appears this is due to the KDE plot on diagonal using the same x-axis and having a much wider spread.\n",
    "Changing the diagonal to histograms syncs the x-axis, makes the scatters more readable. I find the histogram easier to read than KDE anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50462cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.PairGrid(df_train[cols], diag_sharey=False)\n",
    "\n",
    "g.map_lower(sns.scatterplot, s=50, hue=df_train[\"YearBuilt\"])\n",
    "g.map_diag(sns.histplot, kde=False)\n",
    "#use a smaller plot size on the upper size, just to see the size parameter effect\n",
    "g.map_upper(sns.scatterplot, s=10, hue=df_train[\"YearBuilt\"])\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187994a7",
   "metadata": {},
   "source": [
    "The 3 bottom rows in these charts show a single high outlier for each of TotalBsmtSD, GrLivArea, and 1stFlrSF. Instead of blindly shooting with STD or Tukey capping, this seems a more realistic outlier to consider.\n",
    "\n",
    "But from the above Seaborn charts it is hard to directly see what this value is. I could just filter the highest numbers for the above 3 columns, but interactively seeing it from the graph would be nice. \n",
    "\n",
    "While I did not find a way to enable such interaction in Seaborn, Plotly provides a simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec022b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_matrix(df_train[cols], width=1200, height=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5672978",
   "metadata": {},
   "source": [
    "In the above Plotly charts, hovering over each dot with the mouse will display a tooltip with the values it is plotted from. For example, the outlier on TotalBsmtSF shows a value of 6110.\n",
    "Filtering this value from the dataframe shows it is the same value that has the outlier values for most of the big outliers in the chart above (just mouse over the dots for columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', None): \n",
    "    display(df_train[df_train[\"TotalBsmtSF\"] == 6110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"TotalBsmtSF\"] == 6110][[\"TotalBsmtSF\", \"2ndFlrSF\", \"GrLivArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a7fd8",
   "metadata": {},
   "source": [
    "Try to remove the manually identified outlier and plot again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2 = df_train[df_train[\"TotalBsmtSF\"] != 6110]\n",
    "df_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53be027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_matrix(df_train_2[cols], width=1200, height=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11089cd",
   "metadata": {},
   "source": [
    "Looks much better now, showing the overall shape more clearly.\n",
    "\n",
    "Now to see the \"automated\" Tukey and STD removed plots,\n",
    "\n",
    "## Tukey Processed Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_matrix(df_tukey[cols], width=1200, height=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f601a",
   "metadata": {},
   "source": [
    "## STD Processed Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_matrix(df_std[cols], width=1200, height=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ae94a",
   "metadata": {},
   "source": [
    "The diagonal typically separates the two copies of the pairs in the above plots. That is, each pair is there twice. With Seaborn it is possible to add correlation coefficients into the plot, and to drop the duplicate plots if wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91510c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrfunc(x, y, **kws):\n",
    "    #https://stackoverflow.com/questions/48591713/pearson-correlation-and-nan-values\n",
    "    good = ~np.logical_or(np.isnan(x), np.isnan(y))\n",
    "    r, _ = stats.pearsonr(x[good], y[good])\n",
    "    annotation = f\"{r:.2f}\"\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(f\"r = {annotation}\", fontsize=25,\n",
    "                xy=(.1, .9), xycoords=ax.transAxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431bafe",
   "metadata": {},
   "source": [
    "Plotting actual correlation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf66593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pairgrid vs pairplot allows calculating and plotting correlation?\n",
    "g = sb.PairGrid(df_train_2[cols], diag_sharey=False)\n",
    "\n",
    "g.map_lower(sns.scatterplot, s=50, hue=df_train_2[\"YearBuilt\"])\n",
    "g.map_diag(sns.histplot, kde=True)\n",
    "g.map_lower(corrfunc) # add also correlation coefficient annotations to lower half plots\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3acb2",
   "metadata": {},
   "source": [
    "Above plots show the actual Pearson correlation value now for the two variables of concern. \n",
    "\n",
    "Some of the data structure is quite well visible in these plots. For example:\n",
    "- The 2ndFlrSF is zero in many cases, leading to a vertical line in the second column from the left. Because many houses don't have a second floor.\n",
    "- In column descriptions GrLivArea is described as the living area above ground (see below). Thus, the 1stFlrSF vs GrLivArea has a diagonal line. The total living area above ground cannot be less than 1stFlrSF, and is only higher in cases where the house has a 2nd floor. Hence the diagonal line.\n",
    "- The 1stFlrSF vs TotalBsmtSF has another diagonal line, with a few dots above it only. Because in very few cases the basement is larger than the living area (1st floor) above it. More commonly the size is the same or less.\n",
    "- GarageYrBlt vs YearBuilt has a diagonal line with a few dots just very little below it. Because there might be a few cases where the garage finished the previous year before finishing the house. But appears never did someone build a garage and wait 10 years to build the house. Makes sense.\n",
    "- YearRemodAdd is described as being the same as construction date if no remodelling done. However, this change does not seem to apply before 1950 or there is something else at hand, referring to the diagonal line starting from around 1950 for YearRemodAdd vs YearBuilt and GarageYrBlt. Also the yearRemodAdd row itself shows its value starting from 1950+ so apparently before that it is always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_descriptions[\"GrLivArea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3985aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_descriptions[\"YearRemodAdd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d34680",
   "metadata": {},
   "source": [
    "Now to add the top plot, it shows the relations different in a way of A->B vs B->A, so might be helpful at time to capture visually some relations better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779baa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sb.pairplot(df_tukey[cols], hue=\"YearBuilt\")\n",
    "\n",
    "g = sb.PairGrid(df_tukey[cols], diag_sharey=False)\n",
    "\n",
    "#https://medium.com/@morganjonesartist/color-guide-to-seaborn-palettes-da849406d44f\n",
    "g.map_lower(sns.scatterplot, s=50, hue=df_tukey[\"YearBuilt\"])\n",
    "g.map_diag(sns.histplot, kde=False)\n",
    "g.map_upper(sns.scatterplot, s=10, hue=df_tukey[\"YearBuilt\"])\n",
    "g.map_lower(corrfunc)\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c2f09",
   "metadata": {},
   "source": [
    "## Tighten The Plot Margins\n",
    "\n",
    "This is what I tried initially to get the compressed looking scatters to widen. But it just removes the small white space between the charts. Might be useful still, sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "orig_xm = mpl.rcParams['axes.xmargin']\n",
    "orig_ym = mpl.rcParams['axes.ymargin']\n",
    "print(orig_xm)\n",
    "print(orig_ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfcb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['axes.xmargin'] = 0.01\n",
    "mpl.rcParams['axes.ymargin'] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10f782",
   "metadata": {},
   "source": [
    "## Matrix with both Scatter + KDE Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = sns.PairGrid(df_train_2[cols], diag_sharey=False)\n",
    "\n",
    "g.map_lower(sns.scatterplot, s=50)\n",
    "g.map_lower(corrfunc)\n",
    "g.map_diag(sns.histplot, kde=True)\n",
    "g.map_upper(sns.kdeplot, lw=1, cmap=\"Reds\")\n",
    "g.map_upper(corrfunc)\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "#for ax in g.axes.flat:\n",
    "#    ax.set_xlabel('')\n",
    "#    ax.set_ylabel('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553bda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_style(\"darkgrid\")\n",
    "sns.set_style(\"dark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02597c",
   "metadata": {},
   "source": [
    "# Top Negative Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7305ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bf4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Negative Correlations\")\n",
    "lowest_corrs = get_top_abs_correlations(df_train[numeric_cols], 5, True, False)\n",
    "lowest_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c2df2",
   "metadata": {},
   "source": [
    "Collect columns for plotting / comparison later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f205aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x[0] for x in lowest_corrs.index]\n",
    "cols += [x[1] for x in lowest_corrs.index]\n",
    "cols = list(set(cols))\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa747820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://stackoverflow.com/questions/30942577/seaborn-correlation-coefficient-on-pairgrid\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "g = sns.PairGrid(df_train_2[cols])\n",
    "\n",
    "g.map_lower(sns.scatterplot, s=50, hue=df_train_2[\"YearBuilt\"]) \n",
    "g.map_diag(sns.histplot)\n",
    "g.map_upper(sns.kdeplot, lw=1, cmap=\"Reds\")\n",
    "#off diag = off the diagonal = botn upper and lower at the same time\n",
    "g.map_offdiag(corrfunc)\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "#for ax in g.axes.flat:\n",
    "#    ax.set_xlabel('')\n",
    "#    ax.set_ylabel('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3be0e",
   "metadata": {},
   "source": [
    "## Correlations Near Zero\n",
    "\n",
    "The ones that seem most unrelated, sometimes for a reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Closest to Zero Correlations\")\n",
    "irr_corrs = get_top_abs_correlations(df_train[numeric_cols], 5, True, True)\n",
    "irr_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x[0] for x in irr_corrs.index]\n",
    "cols += [x[1] for x in irr_corrs.index]\n",
    "cols = list(set(cols))\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = sns.PairGrid(df_train_2[cols], diag_sharey=False)\n",
    "#g = sns.PairGrid(df_train_2[cols], diag_sharey=False, hue=\"YearBuilt\")\n",
    "\n",
    "g.map_lower(sns.scatterplot, s=50,hue=df_train_2[\"YearBuilt\"], palette=\"BuGn\")\n",
    "g.map_diag(sns.histplot, kde=True)\n",
    "g.map_lower(corrfunc)\n",
    "#for some reason kdeplot here does not have any effect with \"palette\" but \"cmap\" works\n",
    "g.map_upper(sns.kdeplot, cmap=\"BuPu\")\n",
    "g.map_upper(corrfunc)\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e6155",
   "metadata": {},
   "source": [
    "Thats all for today.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117e335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420b5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d9e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c1871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bfb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ed370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ec9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a438b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
