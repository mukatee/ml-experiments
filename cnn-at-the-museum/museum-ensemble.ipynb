{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "model_img_size = 299 #TODO: 224\n",
    "tta = True\n",
    "best_thr = 0.085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers, applications\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import backend as K \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../input/imet-2019-fgvc6/train/\"\n",
    "test_path = \"../input/imet-2019-fgvc6/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_id</th>\n",
       "      <th>attribute_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>culture::abruzzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>culture::achaemenid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>culture::aegean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>culture::afghan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>culture::after british</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attribute_id          attribute_name\n",
       "0             0        culture::abruzzi\n",
       "1             1     culture::achaemenid\n",
       "2             2         culture::aegean\n",
       "3             3         culture::afghan\n",
       "4             4  culture::after british"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = pd.read_csv(\"../input/imet-2019-fgvc6/labels.csv\")\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = df_labels.shape[0]\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "def img_augment(img):\n",
    "    #fifty_chance = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    import random\n",
    "    if random.randint(1,101) > 90:\n",
    "        #print(\"original\")\n",
    "        #one in 10 return original image\n",
    "        return img\n",
    "    seq = iaa.SomeOf(3, [\n",
    "        iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "        #iaa.GaussianBlur(sigma=(0, 1.0)), # blur images with a sigma of 0 to 1.0 TODO: test for good values\n",
    "        iaa.Affine(\n",
    "            #scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "            scale=(0.8, 1.2),\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "        iaa.Affine(\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": 0}, # translate by -20 to +20 percent (per axis)\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "        iaa.Affine(\n",
    "            rotate=(-10, 10), # rotate by -10 to +10 degrees\n",
    "            #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "        iaa.Affine(\n",
    "            shear=(-5, 5), # shear by -5 to +5 degrees\n",
    "            #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=0, # if mode is constant, use a cval between 0 and 255\n",
    "            mode=\"constant\" # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ),\n",
    "    ])\n",
    "    img = seq.augment_image(img)\n",
    "    return img\n",
    "\n",
    "#https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up\n",
    "#https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "from keras.applications import inception_resnet_v2\n",
    "from keras.applications import inception_v3\n",
    "import random\n",
    "\n",
    "def img_pad_resize(filename, input_path, augment, preprocessor):\n",
    "    img = PIL.Image.open(f'{input_path}/{filename}')\n",
    "    img_w = img.size[0]\n",
    "    img_h = img.size[1]\n",
    "\n",
    "    cropped = False\n",
    "    if random.randint(1,101) > 50:\n",
    "        aspect_ratio = img_w/img_h\n",
    "        if aspect_ratio > 2 or aspect_ratio < 0.5:\n",
    "            diff = abs(img_w-img_h)\n",
    "            if img_w > img_h:\n",
    "                crop_size = img_h\n",
    "                crop_y = 0\n",
    "                crop_x = random.randint(0, diff)\n",
    "            else:\n",
    "                #print(filename)\n",
    "                crop_size = img_w\n",
    "                crop_x = 0\n",
    "                crop_y = random.randint(0, diff)\n",
    "            img_cropped = img.crop((crop_x, crop_y, crop_x+crop_size, crop_y+crop_size))\n",
    "            img.close()\n",
    "            img = img_cropped\n",
    "            cropped = True\n",
    "            #print(\"cropped:\"+filename)\n",
    "       \n",
    "    w = img.size[0]\n",
    "    h = img.size[1]\n",
    "    pad_size = np.abs(h-w)\n",
    "    wm = hm = 1\n",
    "    pw = ph = 0\n",
    "    if w < h:\n",
    "        wm = h / w\n",
    "        pw = pad_size / 2\n",
    "    else:\n",
    "        hm = w / h\n",
    "        ph = pad_size / 2\n",
    "    w *= wm\n",
    "    h *= hm\n",
    "    h = int(h)\n",
    "    w = int(w)\n",
    "    pw = int(pw)\n",
    "    ph = int(ph)\n",
    "    padding = (pw, ph, pw, ph)\n",
    "    padded = ImageOps.expand(img, padding)\n",
    "    resized = padded.resize((model_img_size, model_img_size))\n",
    "    np_img = np.array(resized)\n",
    "    img.close()\n",
    "    padded.close()\n",
    "    del img\n",
    "    del padded\n",
    "    if augment:\n",
    "        np_img = img_augment(np_img)\n",
    "    np_img = preprocessor(np_img)\n",
    "    #np_img = resnet50.preprocess_input(np_img)\n",
    "    return np_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a.iloc[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras.utils import Sequence\n",
    "\n",
    "#https://github.com/sdcubber/Keras-Sequence-boilerplate/blob/master/Keras-Sequence.ipynb\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class MySequence(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, mode=\"train\", augment=True):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.max_idx = math.ceil(len(x_set)/batch_size)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx2 = idx % self.max_idx\n",
    "        #i guess the len method above stops the generator from being called too much\n",
    "        #print(f\"idx:{idx}, idx2:{idx2}\")\n",
    "        start = idx * self.batch_size\n",
    "        end = min(start + batch_size, len(self.x))\n",
    "        batch_x = self.x.iloc[start : end]\n",
    "        batch_y = self.y[start : end]\n",
    "        \n",
    "        next_batch = []\n",
    "        for index, row in batch_x.iterrows():\n",
    "            file_name = row[\"filename\"]\n",
    "            file_path = row[\"path\"]\n",
    "            padded = img_pad_resize(file_name, file_path, self.augment)\n",
    "            if padded.shape != (299, 299, 3):\n",
    "                print(f\"shape mismatch {file_name}, {file_path}, {padded.shape}\")\n",
    "                print()\n",
    "                #the image has alpha channel, drop it\n",
    "                padded = padded[ :, :, :3]\n",
    "            \n",
    "            next_batch.append(padded)\n",
    "        np_y = np.array(batch_y)\n",
    "        result = np.array(next_batch), np_y\n",
    "\n",
    "        #print(result[0].shape)\n",
    "        return result\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.x, self.y = unison_shuffled_copies(self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "                             \n",
    "checkpoint = ModelCheckpoint('../working/Resnet50_best_{epoch:03d}_{val_loss:.2f}.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3,\n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=7)\n",
    "\n",
    "csv_logger = CSVLogger(filename='../working/training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen_batch(idx):\n",
    "    # configure batch size and retrieve one batch of images\n",
    "    plt.clf() #clears matplotlib data and axes\n",
    "    #for batch in train_generator:\n",
    "    rows = batch_size / 3\n",
    "    plt.figure(figsize=[30,10*rows])\n",
    "    batch = train_gen.__getitem__(idx)\n",
    "    for x in range(0,batch_size-1):\n",
    "    #    print(train_generator.filenames[x])\n",
    "        plt.subplot(rows, 3, x+1)\n",
    "        plt.imshow(batch[0][x], interpolation='nearest')\n",
    "\n",
    "        item_labels = np.argwhere(batch[1][x] > 0)\n",
    "        title_val = []\n",
    "        info_str = str(item_labels)\n",
    "        for tag_id in item_labels:\n",
    "            att_name = df_labels[df_labels['attribute_id'] == tag_id[0]]\n",
    "            info_str += \" \"+str(att_name[\"attribute_name\"]) #att_name is a dataframe\n",
    "            title_val.append(att_name['attribute_name'].iloc[0]) #i\n",
    "        #print(info_str)\n",
    "        plt.title(title_val)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_applications import imagenet_utils as utils\n",
    "\n",
    "def ResNet(stack_fn,\n",
    "           preact,\n",
    "           use_bias,\n",
    "           model_name='resnet',\n",
    "           include_top=True,\n",
    "           weights='imagenet',\n",
    "           input_tensor=None,\n",
    "           input_shape=None,\n",
    "           pooling=None,\n",
    "           classes=1000,\n",
    "           **kwargs):\n",
    "    \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        stack_fn: a function that returns output tensor for the\n",
    "            stacked residual blocks.\n",
    "        preact: whether to use pre-activation or not\n",
    "            (True for ResNetV2, False for ResNet and ResNeXt).\n",
    "        use_bias: whether to use biases for convolutional layers or not\n",
    "            (True for ResNet and ResNetV2, False for ResNeXt).\n",
    "        model_name: string, model name.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels.\n",
    "        pooling: optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils\n",
    "    # backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "    backend, layers, models, keras_utils = keras.backend, keras.layers, keras.models, keras.utils\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = utils._obtain_input_shape(input_shape,\n",
    "                                          default_size=224,\n",
    "                                          min_size=32,\n",
    "                                          data_format=backend.image_data_format(),\n",
    "                                          require_flatten=include_top,\n",
    "                                          weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv')(x)\n",
    "\n",
    "    if preact is False:\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                      name='conv1_bn')(x)\n",
    "        x = layers.Activation('relu', name='conv1_relu')(x)\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='pool1_pad')(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, name='pool1_pool')(x)\n",
    "\n",
    "    x = stack_fn(x)\n",
    "\n",
    "    if preact is True:\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                      name='post_bn')(x)\n",
    "        x = layers.Activation('relu', name='post_relu')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='probs')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name=model_name)\n",
    "\n",
    "    # Load weights.\n",
    "    if (weights == 'imagenet') and (model_name in WEIGHTS_HASHES):\n",
    "        if include_top:\n",
    "            file_name = model_name + '_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "            file_hash = WEIGHTS_HASHES[model_name][0]\n",
    "        else:\n",
    "            file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "            file_hash = WEIGHTS_HASHES[model_name][1]\n",
    "        weights_path = keras_utils.get_file(file_name,\n",
    "                                            BASE_WEIGHTS_PATH + file_name,\n",
    "                                            cache_subdir='models',\n",
    "                                            file_hash=file_hash)\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "def block2(x, filters, kernel_size=3, stride=1,\n",
    "           conv_shortcut=False, name=None):\n",
    "    \"\"\"A residual block.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer.\n",
    "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
    "        stride: default 1, stride of the first layer.\n",
    "        conv_shortcut: default False, use convolution shortcut if True,\n",
    "            otherwise identity shortcut.\n",
    "        name: string, block label.\n",
    "    # Returns\n",
    "        Output tensor for the residual block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    preact = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                       name=name + '_preact_bn')(x)\n",
    "    preact = layers.Activation('relu', name=name + '_preact_relu')(preact)\n",
    "\n",
    "    if conv_shortcut is True:\n",
    "        shortcut = layers.Conv2D(4 * filters, 1, strides=stride,\n",
    "                                 name=name + '_0_conv')(preact)\n",
    "    else:\n",
    "        shortcut = layers.MaxPooling2D(1, strides=stride)(x) if stride > 1 else x\n",
    "\n",
    "    x = layers.Conv2D(filters, 1, strides=1, use_bias=False,\n",
    "                      name=name + '_1_conv')(preact)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                  name=name + '_1_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_1_relu')(x)\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride,\n",
    "                      use_bias=False, name=name + '_2_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                  name=name + '_2_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_2_relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\n",
    "    x = layers.Add(name=name + '_out')([shortcut, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def stack2(x, filters, blocks, stride1=2, name=None):\n",
    "    \"\"\"A set of stacked residual blocks.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer in a block.\n",
    "        blocks: integer, blocks in the stacked blocks.\n",
    "        stride1: default 2, stride of the first layer in the first block.\n",
    "        name: string, stack label.\n",
    "    # Returns\n",
    "        Output tensor for the stacked blocks.\n",
    "    \"\"\"\n",
    "    x = block2(x, filters, conv_shortcut=True, name=name + '_block1')\n",
    "    for i in range(2, blocks):\n",
    "        x = block2(x, filters, name=name + '_block' + str(i))\n",
    "    x = block2(x, filters, stride=stride1, name=name + '_block' + str(blocks))\n",
    "    return x\n",
    "\n",
    "def ResNet50V2(include_top=True,\n",
    "               weights='imagenet',\n",
    "               input_tensor=None,\n",
    "               input_shape=None,\n",
    "               pooling=None,\n",
    "               classes=1000,\n",
    "               **kwargs):\n",
    "    def stack_fn(x):\n",
    "        x = stack2(x, 64, 3, name='conv2')\n",
    "        x = stack2(x, 128, 4, name='conv3')\n",
    "        x = stack2(x, 256, 6, name='conv4')\n",
    "        x = stack2(x, 512, 3, stride1=1, name='conv5')\n",
    "        return x\n",
    "    return ResNet(stack_fn, True, True, 'resnet50v2',\n",
    "                  include_top, weights,\n",
    "                  input_tensor, input_shape,\n",
    "                  pooling, classes,\n",
    "                  **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "def create_model_resnet(trainable_layer_count):\n",
    "    input_tensor = Input(shape=(model_img_size, model_img_size, 3))\n",
    "    base_model = ResNet50V2(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/keras-pretrain-model-weights/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "#    base_model = ResNet50(include_top=False,\n",
    "#                          #the weights value can apparently also be a file path..\n",
    "#                   weights=None, #loading weights from dataset, avoiding need for internet conn\n",
    "#                   input_tensor=input_tensor)\n",
    "#    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    if trainable_layer_count == \"all\":\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-trainable_layer_count:]:\n",
    "            layer.trainable = True\n",
    "    print(\"base model has {} layers\".format(len(base_model.layers)))\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu')(base_model.output)\n",
    "#     x = Flatten()(x)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(5e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #predict individual probability of each category\n",
    "    final_output = Dense(n_classes, activation='sigmoid', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "def create_model_inception_resnet(trainable_layer_count):\n",
    "    input_tensor = Input(shape=(model_img_size, model_img_size, 3))\n",
    "    base_model = InceptionResNetV2(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "#    base_model = ResNet50(include_top=False,\n",
    "#                          #the weights value can apparently also be a file path..\n",
    "#                   weights=None, #loading weights from dataset, avoiding need for internet conn\n",
    "#                   input_tensor=input_tensor)\n",
    "#    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    if trainable_layer_count == \"all\":\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-trainable_layer_count:]:\n",
    "            layer.trainable = True\n",
    "    print(\"base model has {} layers\".format(len(base_model.layers)))\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu')(base_model.output)\n",
    "#     x = Flatten()(x)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(5e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #predict individual probability of each category\n",
    "    final_output = Dense(n_classes, activation='sigmoid', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "def create_model_inception(trainable_layer_count):\n",
    "    input_tensor = Input(shape=(model_img_size, model_img_size, 3))\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "#    base_model = ResNet50(include_top=False,\n",
    "#                          #the weights value can apparently also be a file path..\n",
    "#                   weights=None, #loading weights from dataset, avoiding need for internet conn\n",
    "#                   input_tensor=input_tensor)\n",
    "#    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    if trainable_layer_count == \"all\":\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-trainable_layer_count:]:\n",
    "            layer.trainable = True\n",
    "    print(\"base model has {} layers\".format(len(base_model.layers)))\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu')(base_model.output)\n",
    "#     x = Flatten()(x)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(5e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #predict individual probability of each category\n",
    "    final_output = Dense(n_classes, activation='sigmoid', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2.0\n",
    "epsilon = K.epsilon()\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n",
    "    pt = K.clip(pt, epsilon, 1-epsilon)\n",
    "    CE = -K.log(pt)\n",
    "    FL = K.pow(1-pt, gamma) * CE\n",
    "    loss = K.sum(FL, axis=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    print(f\"y_true=\"+str(y_true[0]))\n",
    "    print(f\"y_pred={y_pred}\")\n",
    "    beta = 2\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=1)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=1)\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    result = K.mean(((1+beta**2)*precision*recall) / ((beta**2)*precision+recall+K.epsilon()))\n",
    "    print(result)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_f2=2\n",
    "\n",
    "def my_f2(y_true, y_pred):\n",
    "    print(f\"y_true=\"+str(y_true[0]))\n",
    "    print(f\"y_pred={y_pred}\")\n",
    "#    assert y_true.shape[0] == y_pred.shape[0]\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1), axis=1)\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0), axis=1)\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1), axis=1)\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f2 = (1+beta_f2**2)*p*r / (p*beta_f2**2 + r + 1e-15)\n",
    "\n",
    "    return np.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "#https://www.kaggle.com/anokas/fixed-f2-score-in-python\n",
    "def my_my_f2(y_true, y_pred):\n",
    "    # fbeta_score throws a confusing error if inputs are not numpy arrays\n",
    "    y_true, y_pred, = np.array(y_true), np.array(y_pred)\n",
    "    # We need to use average='samples' here, any other average method will generate bogus results\n",
    "    return fbeta_score(y_true, y_pred, beta=2, average='samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "base model has 190 layers\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "y_true=Tensor(\"metrics/f2_score/strided_slice:0\", shape=(?,), dtype=float32)\n",
      "y_pred=Tensor(\"final_output/Sigmoid:0\", shape=(?, 1103), dtype=float32)\n",
      "Tensor(\"metrics/f2_score/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "#f2_score = my_f2\n",
    "#https://datascience.stackexchange.com/questions/26112/decay-parameter-in-keras-optimizers\n",
    "model_resnet = create_model_resnet(\"all\")\n",
    "model_resnet.compile(optimizer=Adam(lr=0.0001, decay=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\", f2_score]) #loss=focal_loss\n",
    "model_resnet.load_weights(\"../input/imetresnet50v2trainedweights/Resnet50_best_013_0.01.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model has 780 layers\n",
      "y_true=Tensor(\"metrics_1/f2_score/strided_slice:0\", shape=(?,), dtype=float32)\n",
      "y_pred=Tensor(\"final_output_1/Sigmoid:0\", shape=(?, 1103), dtype=float32)\n",
      "Tensor(\"metrics_1/f2_score/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_inception_resnet = create_model_inception_resnet(\"all\")\n",
    "model_inception_resnet.compile(optimizer=Adam(lr=0.0001, decay=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\", f2_score]) #loss=focal_loss\n",
    "model_inception_resnet.load_weights(\"../input/imetresnet50v2trainedweights/inception_resnet_best_004_0.01.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model has 311 layers\n",
      "y_true=Tensor(\"metrics_2/f2_score/strided_slice:0\", shape=(?,), dtype=float32)\n",
      "y_pred=Tensor(\"final_output_2/Sigmoid:0\", shape=(?, 1103), dtype=float32)\n",
      "Tensor(\"metrics_2/f2_score/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_inception = create_model_inception(\"all\")\n",
    "model_inception.compile(optimizer=Adam(lr=0.0001, decay=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\", f2_score]) #loss=focal_loss\n",
    "model_inception.load_weights(\"../input/imetresnet50v2trainedweights/Inception_best_008_0.01.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793d9ca2f10b4e72b3c2231ffa41ab87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7443), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "probabilities_resnet = []\n",
    "submit_resnet = pd.read_csv('../input/imet-2019-fgvc6/sample_submission.csv')\n",
    "\n",
    "for name in tqdm(submit_resnet['id']):\n",
    "    np_img = img_pad_resize(name+\".png\", test_path, False, resnet50.preprocess_input)\n",
    "    if tta:\n",
    "        img_pred = np.zeros(n_classes)\n",
    "        for x in range(10):\n",
    "            aug_img = img_augment(np_img)\n",
    "            score_predict = model_resnet.predict(aug_img[np.newaxis])\n",
    "            img_pred += np.squeeze(score_predict)\n",
    "        img_pred /= 10\n",
    "        score_predict = img_pred\n",
    "    else:\n",
    "        score_predict = model_resnet.predict(np_img[np.newaxis])\n",
    "    probabilities_resnet.append(score_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f196d4881e840d28e8fcbe5cb92e07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7443), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "probabilities_inception_resnet = []\n",
    "submit_inception_resnet = pd.read_csv('../input/imet-2019-fgvc6/sample_submission.csv')\n",
    "\n",
    "for name in tqdm(submit_inception_resnet['id']):\n",
    "    np_img = img_pad_resize(name+\".png\", test_path, False, inception_resnet_v2.preprocess_input)\n",
    "    if tta:\n",
    "        img_pred = np.zeros(n_classes)\n",
    "        for x in range(6):\n",
    "            aug_img = img_augment(np_img)\n",
    "            score_predict = model_inception_resnet.predict(aug_img[np.newaxis])\n",
    "            img_pred += np.squeeze(score_predict)\n",
    "        img_pred /= 6\n",
    "        score_predict = img_pred\n",
    "    else:\n",
    "        score_predict = model_inception_resnet.predict(np_img[np.newaxis])\n",
    "    probabilities_inception_resnet.append(score_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f2cb45ab7948d8b0bffee43939da13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7443), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "probabilities_inception = []\n",
    "submit_inception = pd.read_csv('../input/imet-2019-fgvc6/sample_submission.csv')\n",
    "\n",
    "for name in tqdm(submit_inception['id']):\n",
    "    np_img = img_pad_resize(name+\".png\", test_path, False, inception_v3.preprocess_input)\n",
    "    if tta:\n",
    "        img_pred = np.zeros(n_classes)\n",
    "        for x in range(8):\n",
    "            aug_img = img_augment(np_img)\n",
    "            score_predict = model_inception.predict(aug_img[np.newaxis])\n",
    "            img_pred += np.squeeze(score_predict)\n",
    "        img_pred /= 8\n",
    "        score_predict = img_pred\n",
    "    else:\n",
    "        score_predict = model_inception.predict(np_img[np.newaxis])\n",
    "    probabilities_inception.append(score_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probabilities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-1de46964f328>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnp_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprobabilities\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'probabilities' is not defined"
     ]
    }
   ],
   "source": [
    "np_pred = np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-28-198766056a07>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_printoptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msuppress\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mnp_pred\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'np_pred' is not defined"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np_pred[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00002833, 0.00006745, 0.00033819, ..., 0.00003007, 0.00015867,\n",
       "        0.00007873]),\n",
       " array([0.00000085, 0.00006013, 0.00025669, ..., 0.00001119, 0.0000319 ,\n",
       "        0.00003241])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_inception[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_prob_inception = np.array(probabilities_inception)\n",
    "np_prob_inception_resnet = np.array(probabilities_inception_resnet)\n",
    "np_prob_resnet = np.array(probabilities_resnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00010678, 0.00051504, 0.00125169, ..., 0.00017352, 0.00107907,\n",
       "        0.00028112],\n",
       "       [0.00029389, 0.00278867, 0.0004789 , ..., 0.00003994, 0.00078088,\n",
       "        0.00017206]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_prob_sum = np_prob_inception + np_prob_inception_resnet + np_prob_resnet\n",
    "#np_prob_sum = np_prob_inception + np_prob_resnet\n",
    "np_prob_sum[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00003559, 0.00017168, 0.00041723, ..., 0.00005784, 0.00035969,\n",
       "        0.00009371],\n",
       "       [0.00009796, 0.00092956, 0.00015963, ..., 0.00001331, 0.00026029,\n",
       "        0.00005735]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_prob_avg = np_prob_sum/3\n",
    "np_prob_avg[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_pred_inception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict = np_prob_avg>=best_thr\n",
    "label_predict = label_predict.astype(int)\n",
    "label_predict[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argwhere(label_predict[0] == 1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['369 586 587 766 1039 1059', '188 231 369 1039', '79 121 415 498 961']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numeroiksi nuo binary ennusteet\n",
    "indices = []\n",
    "for row in label_predict:\n",
    "    new_row = np.argwhere(row == 1).flatten()\n",
    "    str_predict_label = ' '.join(str(l) for l in new_row)\n",
    "    indices.append(str_predict_label)\n",
    "indices[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#majority vote\n",
    "label_predict_inception = np_prob_inception>=best_thr\n",
    "label_predict_inception = label_predict_inception.astype(int)\n",
    "\n",
    "label_predict_inception_resnet = np_prob_inception_resnet>=best_thr\n",
    "label_predict_inception_resnet = label_predict_inception_resnet.astype(int)\n",
    "\n",
    "label_predict_resnet = np_prob_resnet>=best_thr\n",
    "label_predict_resnet = label_predict_resnet.astype(int)\n",
    "\n",
    "label_pred_sum = label_predict_inception + label_predict_inception_resnet + label_predict_resnet\n",
    "#label_pred_sum = label_predict_inception + label_predict_resnet\n",
    "label_pred_sum[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['369', '', '79']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numeroiksi nuo binary ennusteet\n",
    "indices_major = []\n",
    "for row in label_pred_sum:\n",
    "    new_row = np.argwhere(row >= 3).flatten()\n",
    "    str_predict_label = ' '.join(str(l) for l in new_row)\n",
    "    indices_major.append(str_predict_label)\n",
    "indices_major[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argwhere(label_pred_sum[0] == 1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argwhere(label_pred_sum[0] >= 2).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../input/imet-2019-fgvc6/sample_submission.csv')\n",
    "submit['attribute_ids'] = indices\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attribute_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10023b2cc4ed5f68</td>\n",
       "      <td>369 586 587 766 1039 1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100fbe75ed8fd887</td>\n",
       "      <td>188 231 369 1039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101b627524a04f19</td>\n",
       "      <td>79 121 415 498 961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10234480c41284c6</td>\n",
       "      <td>13 51 111 147 480 483 737 738 776 813 830 923 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023b0e2636dcea8</td>\n",
       "      <td>51 147 156 227 322 477 489 584 612 671 738 813...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                      attribute_ids\n",
       "0  10023b2cc4ed5f68                          369 586 587 766 1039 1059\n",
       "1  100fbe75ed8fd887                                   188 231 369 1039\n",
       "2  101b627524a04f19                                 79 121 415 498 961\n",
       "3  10234480c41284c6  13 51 111 147 480 483 737 738 776 813 830 923 ...\n",
       "4  1023b0e2636dcea8  51 147 156 227 322 477 489 584 612 671 738 813..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../input/imet-2019-fgvc6/sample_submission.csv')\n",
    "submit['attribute_ids'] = indices_major\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attribute_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10023b2cc4ed5f68</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100fbe75ed8fd887</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101b627524a04f19</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10234480c41284c6</td>\n",
       "      <td>13 147 480 483 738 776 830 1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023b0e2636dcea8</td>\n",
       "      <td>147 322 584 612 738 813 954 1046 1092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                          attribute_ids\n",
       "0  10023b2cc4ed5f68                                    369\n",
       "1  100fbe75ed8fd887                                       \n",
       "2  101b627524a04f19                                     79\n",
       "3  10234480c41284c6        13 147 480 483 738 776 830 1046\n",
       "4  1023b0e2636dcea8  147 322 584 612 738 813 954 1046 1092"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}