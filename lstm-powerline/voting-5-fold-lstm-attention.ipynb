{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e154a47bf09b8770980486e87786317a1b3038e1"
   },
   "source": [
    "### Meeting a Sayed Athar's request, I'm using the Kernel altered by Khoi Nguyen to explain how the whole code works.\n",
    "### If any part is not clear, please comment.  \n",
    "### Please upvote if it was helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
    "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
    "from keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6e6379386e44afc69bee8895a52da22199e888fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[647414, 878484, 343934, 79089, 919288, 126841, 507633, 969011, 9480, 535815, 758538, 680173, 954791, 804682, 565683, 265649, 566252, 349049, 968982, 469070]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "seeds = []\n",
    "for x in range(0, 20):\n",
    "    seeds.append(randint(0, 999999))\n",
    "print(seeds)\n",
    "# select how many folds will be created\n",
    "N_SPLITS = 5\n",
    "# it is just a constant with the measurements data size\n",
    "sample_size = 800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c3340ee96becb5ca8f075d9c44b7df383ddba5ee"
   },
   "outputs": [],
   "source": [
    "# It is the official metric used in this competition\n",
    "# below is the declaration of a function used inside the keras model, calculation with K (keras backend / thensorflow)\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "eda7ea366117d1ce8e5fce69e5bba333821d8b48"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just load train data\n",
    "df_train = pd.read_csv('../input/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "df_train = df_train.set_index(['id_measurement', 'phase'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "26df6c7fbfecd537404866faec13d1238ae3ebc6"
   },
   "outputs": [],
   "source": [
    "# in other notebook I have extracted the min and max values from the train data, the measurements\n",
    "max_num = 127\n",
    "min_num = -128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7b0717b14bcfcba1f48d33c8161ae51c778687af"
   },
   "outputs": [],
   "source": [
    "# This function standardize the data from (-128 to 127) to (-1 to 1)\n",
    "# Theoretically it helps in the NN Model training, but I didn't tested without it\n",
    "def min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n",
    "    if min_data < 0:\n",
    "        ts_std = (ts + abs(min_data)) / (max_data + abs(min_data))\n",
    "    else:\n",
    "        ts_std = (ts - min_data) / (max_data - min_data)\n",
    "    if range_needed[0] < 0:    \n",
    "        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n",
    "    else:\n",
    "        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "785c0d90dd5f2a7dded1a9db7a684b133c6fcdd3"
   },
   "outputs": [],
   "source": [
    "id_measurement = 0\n",
    "phase = 0\n",
    "praq_train = pq.read_pandas('../input/train.parquet', columns=[\"0\", \"1\", \"2\"]).to_pandas()\n",
    "signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "data = praq_train[str(signal_id)]\n",
    "ts_std = min_max_transf(data, min_data=min_num, max_data=max_num)\n",
    "ts_std = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "f75049244acca7a188096ce7b7e1a51f834977e1"
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "def mad(x, axis=None):\n",
    "    return np.mean(np.abs(x - np.mean(x, axis)), axis)\n",
    "\n",
    "def wavelet_denoise(x, wavelet='db1', mode='hard'):\n",
    "\n",
    "    # Extract approximate and detailed coefficients\n",
    "    c_a, c_d = pywt.dwt(x, wavelet)\n",
    "\n",
    "    # Determine the threshold\n",
    "    sigma = 1 / 0.6745 * mad(np.abs(c_d))\n",
    "    threshold = sigma * math.sqrt(2 * math.log(len(x)))\n",
    "\n",
    "    # Filter the detail coefficients\n",
    "    c_d_t = pywt.threshold(c_d, threshold, mode=mode)\n",
    "\n",
    "    # Reconstruct the signal\n",
    "    y = pywt.idwt(np.zeros_like(c_a), c_d_t, wavelet)\n",
    "\n",
    "    return y\n",
    "\n",
    "def signal_entropy(x):\n",
    "\n",
    "    y = wavelet_denoise(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        max_pos = y.argmax()\n",
    "        y[max_pos - 1000:max_pos + 1000] = 0.\n",
    "\n",
    "    return stats.entropy(np.histogram(y, 15)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "a5086b7d82c63bd1f310e6f5d16b81baebf49a77"
   },
   "outputs": [],
   "source": [
    "def teemu_trans(ts):\n",
    "    n_dim=160\n",
    "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, sample_size, bucket_size):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts_std[i:i + bucket_size]\n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        entropy = signal_entropy(ts_range)\n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "        # now, we just add all the features to new_ts and convert it to np.array\n",
    "        new_ts.append(np.concatenate([np.asarray([mean, std, max_range, entropy]),percentil_calc, relative_percentile]))\n",
    "#        new_ts.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range]),percentil_calc, relative_percentile]))\n",
    "    return new_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "00a9081b9b7dfcead47748cb774033d5528da97a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "c825ca19c2a6bd81ceda519c2b96ed06d33b2199"
   },
   "outputs": [],
   "source": [
    "praq_train = pq.read_pandas('../input/train.parquet', columns=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]).to_pandas()\n",
    "X = []\n",
    "y = []\n",
    "# using tdqm to evaluate processing time\n",
    "# takes each index from df_train and iteract it from start to end\n",
    "# it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "for id_measurement in [0,1]:\n",
    "    X_signal = []\n",
    "    # for each phase of the signal\n",
    "    for phase in [0,1,2]:\n",
    "        # extract from df_train both signal_id and target to compose the new data sets\n",
    "        signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "        # but just append the target one time, to not triplicate it\n",
    "        if phase == 0:\n",
    "            y.append(target)\n",
    "        # extract and transform data into sets of features\n",
    "        X_signal.append(teemu_trans(praq_train[str(signal_id)]))\n",
    "    # concatenate all the 3 phases in one matrix\n",
    "    X_signal = np.concatenate(X_signal, axis=1)\n",
    "    # add the data to X\n",
    "    X.append(X_signal)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "d897a1facf38e99ccdbc23d4b1edba06291f9f99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 160, 54)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "2501e8f6e74a48e806f6ec6e690ab288484a4363"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14322353,  0.00713529,  0.04705882,  0.        ,  0.12156863,\n",
       "        0.12941176,  0.1372549 ,  0.14509804,  0.14509804,  0.16078431,\n",
       "        0.16862745, -0.0216549 , -0.01381176, -0.00596863,  0.00187451,\n",
       "        0.00187451,  0.01756078,  0.02540392,  0.0085051 ,  0.00690299,\n",
       "        0.05490196,  0.00705892, -0.01960784, -0.00392157,  0.00392157,\n",
       "        0.01176471,  0.01176471,  0.01960784,  0.03529412, -0.02811294,\n",
       "       -0.01242667, -0.00458353,  0.00325961,  0.00325961,  0.01110275,\n",
       "        0.02678902, -0.15020235,  0.00815639,  0.07058824,  0.02234399,\n",
       "       -0.18431373, -0.16862745, -0.15294118, -0.15294118, -0.14509804,\n",
       "       -0.12941176, -0.11372549, -0.03411137, -0.0184251 , -0.00273882,\n",
       "       -0.00273882,  0.00510431,  0.02079059,  0.03647686])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f2d36c594a45756cabe14c1519248f76509df87f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14322353,  0.00713529,  0.04705882,  0.        ,  0.12156863,\n",
       "        0.12941176,  0.1372549 ,  0.14509804,  0.14509804,  0.16078431,\n",
       "        0.16862745, -0.0216549 , -0.01381176, -0.00596863,  0.00187451,\n",
       "        0.00187451,  0.01756078,  0.02540392,  0.0085051 ,  0.00690299,\n",
       "        0.05490196,  0.00705892, -0.01960784, -0.00392157,  0.00392157,\n",
       "        0.01176471,  0.01176471,  0.01960784,  0.03529412, -0.02811294,\n",
       "       -0.01242667, -0.00458353,  0.00325961,  0.00325961,  0.01110275,\n",
       "        0.02678902, -0.15020235,  0.00815639,  0.07058824,  0.02234399,\n",
       "       -0.18431373, -0.16862745, -0.15294118, -0.15294118, -0.14509804,\n",
       "       -0.12941176, -0.11372549, -0.03411137, -0.0184251 , -0.00273882,\n",
       "       -0.00273882,  0.00510431,  0.02079059,  0.03647686])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c6137bbbe75c3a1509a5f98e08805dbbd492aa37"
   },
   "outputs": [],
   "source": [
    "# This is one of the most important peace of code of this Kernel\n",
    "# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data \n",
    "# It would be praticaly impossible to build a NN with an input of that size\n",
    "# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features\n",
    "# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.\n",
    "def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n",
    "    lag = np.diff(ts)\n",
    "    lag = np.abs(lag)\n",
    "    #np.clip(lag, 0, 127, lag)\n",
    "    lag_std = lag.std()\n",
    "    lag_avg = lag.mean()\n",
    "    lag_max = lag.max()\n",
    "    lag_min = lag.min()\n",
    "    lag_avg_sum = lag.sum()/n_dim\n",
    "    \n",
    "    # convert data into -1 to 1\n",
    "#    ts_std = ts\n",
    "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, sample_size, bucket_size):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts_std[i:i + bucket_size]\n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        entropy = signal_entropy(ts_range)\n",
    "\n",
    "        outliers = []\n",
    "        for r in range(1, 7):\n",
    "            t = lag_std * r\n",
    "            o_count = np.sum(np.abs(lag_std-lag_avg) >= t)\n",
    "            o_count /= lag_avg_sum\n",
    "            outliers.append(o_count)\n",
    "        outliers = np.asarray(outliers) #7\n",
    "                \n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "        # now, we just add all the features to new_ts and convert it to np.array\n",
    "        new_ts.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range, entropy]),percentil_calc, relative_percentile]))#, outliers]))\n",
    "#        new_ts.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range]),percentil_calc, relative_percentile, outliers]))\n",
    "    return np.asarray(new_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_train = pq.read_pandas('../input/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    X = []\n",
    "    y = []\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            if phase == 0:\n",
    "                y.append(target)\n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(praq_train[str(signal_id)]))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "bab8923145dd26cf9082c896ea5d0e57ff92e233"
   },
   "outputs": [],
   "source": [
    "#X = np.load(\"X.npy\")\n",
    "#y = np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "52dc826ab9ee1dd56c9fb29bd5c1b2d26b5928bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1452/1452 [15:46<00:00,  1.52it/s]\n",
      "100%|██████████| 1452/1452 [15:43<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# this code is very simple, divide the total size of the df_train into two sets and process it\n",
    "X = []\n",
    "y = []\n",
    "def load_all():\n",
    "    total_size = len(df_train)\n",
    "    for ini, end in [(0, int(total_size/2)), (int(total_size/2), total_size)]:\n",
    "        X_temp, y_temp = prep_data(ini, end)\n",
    "        X.append(X_temp)\n",
    "        y.append(y_temp)\n",
    "load_all()\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "158873f518693d9f9f5c9c366e4e7e50c951c124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2904, 160, 60)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3b36a7a989f0f6bf3633285aa3e7c3c67b11f3c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10865098,  0.01217601,  0.12082699,  0.09647497,  0.08627451,\n",
       "        0.        ,  0.06666667,  0.08235294,  0.09803922,  0.10588235,\n",
       "        0.11372549,  0.1372549 ,  0.15294118, -0.04198431, -0.02629804,\n",
       "       -0.01061176, -0.00276863,  0.00507451,  0.02860392,  0.0442902 ,\n",
       "        0.03164235,  0.01094787,  0.04259022,  0.02069448,  0.09411765,\n",
       "        0.        , -0.01176471,  0.01176471,  0.02745098,  0.03529412,\n",
       "        0.03529412,  0.05882353,  0.08235294, -0.04340706, -0.01987765,\n",
       "       -0.00419137,  0.00365176,  0.00365176,  0.02718118,  0.05071059,\n",
       "       -0.15415686,  0.00882215, -0.14533472, -0.16297901,  0.05490196,\n",
       "        0.        , -0.18431373, -0.17647059, -0.16078431, -0.15294118,\n",
       "       -0.14509804, -0.1372549 , -0.12941176, -0.03015686, -0.02231373,\n",
       "       -0.00662745,  0.00121569,  0.00905882,  0.01690196,  0.0247451 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1500][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "51ad0e25b00536de6170168499923d82ae1d735f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2904, 160, 60) (2904,)\n"
     ]
    }
   ],
   "source": [
    "# The X shape here is very important. It is also important undertand a little how a LSTM works\n",
    "# X.shape[0] is the number of id_measuremts contained in train data\n",
    "# X.shape[1] is the number of chunks resultant of the transformation, each of this date enters in the LSTM serialized\n",
    "# This way the LSTM can understand the position of a data relative with other and activate a signal that needs\n",
    "# a serie of inputs in a specifc order.\n",
    "# X.shape[3] is the number of features multiplied by the number of phases (3)\n",
    "print(X.shape, y.shape)\n",
    "# save data into file, a numpy specific format\n",
    "np.save(\"X.npy\",X)\n",
    "np.save(\"y.npy\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "289bc7d1ab8048a60025801b457f8df1d848acbc"
   },
   "outputs": [],
   "source": [
    "# This is NN LSTM Model creation\n",
    "def model_lstm(input_shape):\n",
    "    # The shape was explained above, must have this order\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],))\n",
    "    # This is the LSTM layer\n",
    "    # Bidirecional implies that the 160 chunks are calculated in both ways, 0 to 159 and 159 to zero\n",
    "    # although it appear that just 0 to 159 way matter, I have tested with and without, and tha later worked best\n",
    "    # 128 and 64 are the number of cells used, too many can overfit and too few can underfit\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n",
    "    # The second LSTM can give more fire power to the model, but can overfit it too\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    # Attention is a new tecnology that can be applyed to a Recurrent NN to give more meanings to a signal found in the middle\n",
    "    # of the data, it helps more in longs chains of data. A normal RNN give all the responsibility of detect the signal\n",
    "    # to the last cell. Google RNN Attention for more information :)\n",
    "    x = Attention(input_shape[1])(x)\n",
    "    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    # A binnary classification as this must finish with shape (1,)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    # Pay attention in the addition of matthews_correlation metric in the compilation, it is a success factor key\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "0671240c20c9f577918aea47d66eb1a979f8f206"
   },
   "outputs": [],
   "source": [
    "# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n",
    "# So, find the best threshold to convert float to binary is crucial to the result\n",
    "# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n",
    "        if score > best_score:\n",
    "            print(\"found better score:\"+str(score)+\", th=\"+str(threshold))\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "8d6f4ca319c383b1b4f671a37c5a324136e7a466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 2322 samples, validate on 582 samples\n",
      "Epoch 1/50\n",
      "2322/2322 [==============================] - 9s 4ms/step - loss: 0.3417 - matthews_correlation: 0.0048 - val_loss: 0.2296 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_647414_0.h5\n",
      "Epoch 2/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.2270 - matthews_correlation: 0.0000e+00 - val_loss: 0.2186 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.2091 - matthews_correlation: 0.0146 - val_loss: 0.2006 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1945 - matthews_correlation: 0.0000e+00 - val_loss: 0.2127 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.2016 - matthews_correlation: 0.0000e+00 - val_loss: 0.1613 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1658 - matthews_correlation: 0.3280 - val_loss: 0.1473 - val_matthews_correlation: 0.2336\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.00000 to 0.23362, saving model to weights_647414_0.h5\n",
      "Epoch 7/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1315 - matthews_correlation: 0.4773 - val_loss: 0.1177 - val_matthews_correlation: 0.4690\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.23362 to 0.46903, saving model to weights_647414_0.h5\n",
      "Epoch 8/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1294 - matthews_correlation: 0.5151 - val_loss: 0.0930 - val_matthews_correlation: 0.7056\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.46903 to 0.70558, saving model to weights_647414_0.h5\n",
      "Epoch 9/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1160 - matthews_correlation: 0.6489 - val_loss: 0.0851 - val_matthews_correlation: 0.7426\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.70558 to 0.74259, saving model to weights_647414_0.h5\n",
      "Epoch 10/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1119 - matthews_correlation: 0.6115 - val_loss: 0.0802 - val_matthews_correlation: 0.7700\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.74259 to 0.77001, saving model to weights_647414_0.h5\n",
      "Epoch 11/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1084 - matthews_correlation: 0.6395 - val_loss: 0.0961 - val_matthews_correlation: 0.7080\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.77001\n",
      "Epoch 12/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1105 - matthews_correlation: 0.6175 - val_loss: 0.0901 - val_matthews_correlation: 0.7123\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.77001\n",
      "Epoch 13/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1061 - matthews_correlation: 0.6536 - val_loss: 0.0919 - val_matthews_correlation: 0.7008\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.77001\n",
      "Epoch 14/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1068 - matthews_correlation: 0.6383 - val_loss: 0.1091 - val_matthews_correlation: 0.7105\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.77001\n",
      "Epoch 15/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1156 - matthews_correlation: 0.5267 - val_loss: 0.0828 - val_matthews_correlation: 0.7536\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.77001\n",
      "Epoch 16/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0973 - matthews_correlation: 0.6722 - val_loss: 0.0755 - val_matthews_correlation: 0.7516\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.77001\n",
      "Epoch 17/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0989 - matthews_correlation: 0.6266 - val_loss: 0.0802 - val_matthews_correlation: 0.7062\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.77001\n",
      "Epoch 18/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1098 - matthews_correlation: 0.5747 - val_loss: 0.0845 - val_matthews_correlation: 0.7730\n",
      "\n",
      "Epoch 00018: val_matthews_correlation improved from 0.77001 to 0.77304, saving model to weights_647414_0.h5\n",
      "Epoch 19/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1037 - matthews_correlation: 0.6178 - val_loss: 0.0984 - val_matthews_correlation: 0.6945\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.77304\n",
      "Epoch 20/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1111 - matthews_correlation: 0.6579 - val_loss: 0.0924 - val_matthews_correlation: 0.6231\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.77304\n",
      "Epoch 21/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1048 - matthews_correlation: 0.6233 - val_loss: 0.0732 - val_matthews_correlation: 0.8296\n",
      "\n",
      "Epoch 00021: val_matthews_correlation improved from 0.77304 to 0.82962, saving model to weights_647414_0.h5\n",
      "Epoch 22/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0965 - matthews_correlation: 0.6759 - val_loss: 0.0861 - val_matthews_correlation: 0.7635\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.82962\n",
      "Epoch 23/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0985 - matthews_correlation: 0.6386 - val_loss: 0.0840 - val_matthews_correlation: 0.7759\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.82962\n",
      "Epoch 24/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0954 - matthews_correlation: 0.6736 - val_loss: 0.0779 - val_matthews_correlation: 0.7062\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.82962\n",
      "Epoch 25/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1123 - matthews_correlation: 0.5627 - val_loss: 0.0830 - val_matthews_correlation: 0.6786\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.82962\n",
      "Epoch 26/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0996 - matthews_correlation: 0.7127 - val_loss: 0.0797 - val_matthews_correlation: 0.7742\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.82962\n",
      "Epoch 27/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0897 - matthews_correlation: 0.7241 - val_loss: 0.0720 - val_matthews_correlation: 0.8437\n",
      "\n",
      "Epoch 00027: val_matthews_correlation improved from 0.82962 to 0.84369, saving model to weights_647414_0.h5\n",
      "Epoch 28/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0926 - matthews_correlation: 0.6861 - val_loss: 0.0993 - val_matthews_correlation: 0.7509\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 29/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0948 - matthews_correlation: 0.7099 - val_loss: 0.0788 - val_matthews_correlation: 0.7870\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 30/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0950 - matthews_correlation: 0.6559 - val_loss: 0.0817 - val_matthews_correlation: 0.7955\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 31/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0892 - matthews_correlation: 0.7137 - val_loss: 0.0908 - val_matthews_correlation: 0.7325\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 32/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0867 - matthews_correlation: 0.7082 - val_loss: 0.0762 - val_matthews_correlation: 0.7495\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 33/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0957 - matthews_correlation: 0.6502 - val_loss: 0.0929 - val_matthews_correlation: 0.6549\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 34/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0988 - matthews_correlation: 0.5968 - val_loss: 0.0916 - val_matthews_correlation: 0.5647\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 35/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0898 - matthews_correlation: 0.7103 - val_loss: 0.0763 - val_matthews_correlation: 0.7516\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 36/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0886 - matthews_correlation: 0.6951 - val_loss: 0.0766 - val_matthews_correlation: 0.7399\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 37/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0863 - matthews_correlation: 0.6699 - val_loss: 0.0703 - val_matthews_correlation: 0.7304\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 38/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0857 - matthews_correlation: 0.7124 - val_loss: 0.0815 - val_matthews_correlation: 0.7504\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 39/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0838 - matthews_correlation: 0.7181 - val_loss: 0.0803 - val_matthews_correlation: 0.7584\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 40/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0854 - matthews_correlation: 0.7038 - val_loss: 0.0781 - val_matthews_correlation: 0.7129\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 41/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0804 - matthews_correlation: 0.6985 - val_loss: 0.0801 - val_matthews_correlation: 0.7288\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 42/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.1004 - matthews_correlation: 0.6375 - val_loss: 0.0684 - val_matthews_correlation: 0.7942\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 43/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0854 - matthews_correlation: 0.7002 - val_loss: 0.0701 - val_matthews_correlation: 0.7612\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 44/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0857 - matthews_correlation: 0.6849 - val_loss: 0.0711 - val_matthews_correlation: 0.7948\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 45/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0798 - matthews_correlation: 0.6904 - val_loss: 0.0710 - val_matthews_correlation: 0.8167\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 46/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0948 - matthews_correlation: 0.6190 - val_loss: 0.0887 - val_matthews_correlation: 0.7291\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 47/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0816 - matthews_correlation: 0.7093 - val_loss: 0.0762 - val_matthews_correlation: 0.7893\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 48/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0803 - matthews_correlation: 0.7116 - val_loss: 0.0702 - val_matthews_correlation: 0.7870\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.84369\n",
      "Epoch 49/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0910 - matthews_correlation: 0.7158 - val_loss: 0.0741 - val_matthews_correlation: 0.8725\n",
      "\n",
      "Epoch 00049: val_matthews_correlation improved from 0.84369 to 0.87252, saving model to weights_647414_0.h5\n",
      "Epoch 50/50\n",
      "2322/2322 [==============================] - 3s 1ms/step - loss: 0.0783 - matthews_correlation: 0.7371 - val_loss: 0.0828 - val_matthews_correlation: 0.7595\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.87252\n",
      "Beginning fold 2\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 4s 2ms/step - loss: 0.3645 - matthews_correlation: 0.0000e+00 - val_loss: 0.2369 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_647414_1.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.2320 - matthews_correlation: 0.0000e+00 - val_loss: 0.2292 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.2184 - matthews_correlation: 0.0000e+00 - val_loss: 0.2170 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.2066 - matthews_correlation: 0.0000e+00 - val_loss: 0.2259 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1905 - matthews_correlation: 0.0935 - val_loss: 0.2115 - val_matthews_correlation: 0.0806\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.00000 to 0.08057, saving model to weights_647414_1.h5\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1834 - matthews_correlation: 0.2100 - val_loss: 0.1800 - val_matthews_correlation: 0.1045\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.08057 to 0.10453, saving model to weights_647414_1.h5\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1585 - matthews_correlation: 0.3293 - val_loss: 0.1627 - val_matthews_correlation: 0.2908\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.10453 to 0.29075, saving model to weights_647414_1.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1298 - matthews_correlation: 0.4497 - val_loss: 0.1302 - val_matthews_correlation: 0.2610\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.29075\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1549 - matthews_correlation: 0.5135 - val_loss: 0.1781 - val_matthews_correlation: 0.1137\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.29075\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1733 - matthews_correlation: 0.0000e+00 - val_loss: 0.1611 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.29075\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1293 - matthews_correlation: 0.2482 - val_loss: 0.1310 - val_matthews_correlation: 0.5038\n",
      "\n",
      "Epoch 00011: val_matthews_correlation improved from 0.29075 to 0.50377, saving model to weights_647414_1.h5\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1153 - matthews_correlation: 0.6237 - val_loss: 0.1474 - val_matthews_correlation: 0.5771\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.50377 to 0.57708, saving model to weights_647414_1.h5\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1107 - matthews_correlation: 0.5513 - val_loss: 0.1163 - val_matthews_correlation: 0.6208\n",
      "\n",
      "Epoch 00013: val_matthews_correlation improved from 0.57708 to 0.62084, saving model to weights_647414_1.h5\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1071 - matthews_correlation: 0.5922 - val_loss: 0.1204 - val_matthews_correlation: 0.5457\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.62084\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1055 - matthews_correlation: 0.6181 - val_loss: 0.1195 - val_matthews_correlation: 0.6456\n",
      "\n",
      "Epoch 00015: val_matthews_correlation improved from 0.62084 to 0.64561, saving model to weights_647414_1.h5\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0997 - matthews_correlation: 0.6792 - val_loss: 0.1115 - val_matthews_correlation: 0.6576\n",
      "\n",
      "Epoch 00016: val_matthews_correlation improved from 0.64561 to 0.65760, saving model to weights_647414_1.h5\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0980 - matthews_correlation: 0.6740 - val_loss: 0.1136 - val_matthews_correlation: 0.6519\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.65760\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0963 - matthews_correlation: 0.6857 - val_loss: 0.1118 - val_matthews_correlation: 0.6281\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.65760\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0947 - matthews_correlation: 0.6850 - val_loss: 0.1076 - val_matthews_correlation: 0.6584\n",
      "\n",
      "Epoch 00019: val_matthews_correlation improved from 0.65760 to 0.65844, saving model to weights_647414_1.h5\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0948 - matthews_correlation: 0.6577 - val_loss: 0.1119 - val_matthews_correlation: 0.6710\n",
      "\n",
      "Epoch 00020: val_matthews_correlation improved from 0.65844 to 0.67099, saving model to weights_647414_1.h5\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0978 - matthews_correlation: 0.6808 - val_loss: 0.1132 - val_matthews_correlation: 0.6812\n",
      "\n",
      "Epoch 00021: val_matthews_correlation improved from 0.67099 to 0.68121, saving model to weights_647414_1.h5\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0971 - matthews_correlation: 0.6596 - val_loss: 0.1111 - val_matthews_correlation: 0.6953\n",
      "\n",
      "Epoch 00022: val_matthews_correlation improved from 0.68121 to 0.69527, saving model to weights_647414_1.h5\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0967 - matthews_correlation: 0.6775 - val_loss: 0.1183 - val_matthews_correlation: 0.5948\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0960 - matthews_correlation: 0.6583 - val_loss: 0.1182 - val_matthews_correlation: 0.5948\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0923 - matthews_correlation: 0.6734 - val_loss: 0.1103 - val_matthews_correlation: 0.6660\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0900 - matthews_correlation: 0.6895 - val_loss: 0.1187 - val_matthews_correlation: 0.6500\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0886 - matthews_correlation: 0.6842 - val_loss: 0.1064 - val_matthews_correlation: 0.6247\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0909 - matthews_correlation: 0.6882 - val_loss: 0.1177 - val_matthews_correlation: 0.5974\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0916 - matthews_correlation: 0.7013 - val_loss: 0.1146 - val_matthews_correlation: 0.6175\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0915 - matthews_correlation: 0.6991 - val_loss: 0.1151 - val_matthews_correlation: 0.6584\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0924 - matthews_correlation: 0.6807 - val_loss: 0.1068 - val_matthews_correlation: 0.6417\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0938 - matthews_correlation: 0.7061 - val_loss: 0.1208 - val_matthews_correlation: 0.6525\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0948 - matthews_correlation: 0.6763 - val_loss: 0.1036 - val_matthews_correlation: 0.6525\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0893 - matthews_correlation: 0.7185 - val_loss: 0.1050 - val_matthews_correlation: 0.6215\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0889 - matthews_correlation: 0.6689 - val_loss: 0.1203 - val_matthews_correlation: 0.6370\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0891 - matthews_correlation: 0.7209 - val_loss: 0.1178 - val_matthews_correlation: 0.6589\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0925 - matthews_correlation: 0.6667 - val_loss: 0.1123 - val_matthews_correlation: 0.6525\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0865 - matthews_correlation: 0.6966 - val_loss: 0.1126 - val_matthews_correlation: 0.6460\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0853 - matthews_correlation: 0.7130 - val_loss: 0.1164 - val_matthews_correlation: 0.6169\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0878 - matthews_correlation: 0.7128 - val_loss: 0.1121 - val_matthews_correlation: 0.6436\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0912 - matthews_correlation: 0.7019 - val_loss: 0.1036 - val_matthews_correlation: 0.6047\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0899 - matthews_correlation: 0.6920 - val_loss: 0.1223 - val_matthews_correlation: 0.6525\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0885 - matthews_correlation: 0.7006 - val_loss: 0.1023 - val_matthews_correlation: 0.6525\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0923 - matthews_correlation: 0.7126 - val_loss: 0.1008 - val_matthews_correlation: 0.6220\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0970 - matthews_correlation: 0.6466 - val_loss: 0.1412 - val_matthews_correlation: 0.5831\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0993 - matthews_correlation: 0.6873 - val_loss: 0.1100 - val_matthews_correlation: 0.6047\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0877 - matthews_correlation: 0.7089 - val_loss: 0.1001 - val_matthews_correlation: 0.6525\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0861 - matthews_correlation: 0.6749 - val_loss: 0.1169 - val_matthews_correlation: 0.6175\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0848 - matthews_correlation: 0.7219 - val_loss: 0.1045 - val_matthews_correlation: 0.5916\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.69527\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0859 - matthews_correlation: 0.7135 - val_loss: 0.1076 - val_matthews_correlation: 0.6282\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.69527\n",
      "Beginning fold 3\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 4s 2ms/step - loss: 0.3634 - matthews_correlation: -0.0038 - val_loss: 0.2400 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_647414_2.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.2280 - matthews_correlation: 0.0000e+00 - val_loss: 0.2204 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.2178 - matthews_correlation: 0.0000e+00 - val_loss: 0.2056 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.2101 - matthews_correlation: 0.0000e+00 - val_loss: 0.1979 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1996 - matthews_correlation: 0.0323 - val_loss: 0.1916 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1777 - matthews_correlation: 0.0101 - val_loss: 0.1703 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1428 - matthews_correlation: 0.3764 - val_loss: 0.1765 - val_matthews_correlation: 0.1922\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.00000 to 0.19216, saving model to weights_647414_2.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1229 - matthews_correlation: 0.4176 - val_loss: 0.1163 - val_matthews_correlation: 0.5998\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.19216 to 0.59979, saving model to weights_647414_2.h5\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1155 - matthews_correlation: 0.5563 - val_loss: 0.1174 - val_matthews_correlation: 0.5931\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.59979\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1069 - matthews_correlation: 0.5877 - val_loss: 0.1293 - val_matthews_correlation: 0.6227\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.59979 to 0.62268, saving model to weights_647414_2.h5\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1060 - matthews_correlation: 0.5830 - val_loss: 0.1187 - val_matthews_correlation: 0.4239\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.62268\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0962 - matthews_correlation: 0.6415 - val_loss: 0.1089 - val_matthews_correlation: 0.6750\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.62268 to 0.67502, saving model to weights_647414_2.h5\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0936 - matthews_correlation: 0.6763 - val_loss: 0.1245 - val_matthews_correlation: 0.6183\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1043 - matthews_correlation: 0.6809 - val_loss: 0.1140 - val_matthews_correlation: 0.6246\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0943 - matthews_correlation: 0.6153 - val_loss: 0.1108 - val_matthews_correlation: 0.5256\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0922 - matthews_correlation: 0.6454 - val_loss: 0.1104 - val_matthews_correlation: 0.5188\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1011 - matthews_correlation: 0.6301 - val_loss: 0.1066 - val_matthews_correlation: 0.6242\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0958 - matthews_correlation: 0.6573 - val_loss: 0.1143 - val_matthews_correlation: 0.5344\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0893 - matthews_correlation: 0.6648 - val_loss: 0.1106 - val_matthews_correlation: 0.6675\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0929 - matthews_correlation: 0.6833 - val_loss: 0.1187 - val_matthews_correlation: 0.5625\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0892 - matthews_correlation: 0.6763 - val_loss: 0.1088 - val_matthews_correlation: 0.6588\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0858 - matthews_correlation: 0.6752 - val_loss: 0.1330 - val_matthews_correlation: 0.5310\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0893 - matthews_correlation: 0.6099 - val_loss: 0.1114 - val_matthews_correlation: 0.6413\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0850 - matthews_correlation: 0.7105 - val_loss: 0.1199 - val_matthews_correlation: 0.5531\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0837 - matthews_correlation: 0.7048 - val_loss: 0.1079 - val_matthews_correlation: 0.6675\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0829 - matthews_correlation: 0.7124 - val_loss: 0.1209 - val_matthews_correlation: 0.5751\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0879 - matthews_correlation: 0.7228 - val_loss: 0.1180 - val_matthews_correlation: 0.5601\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0858 - matthews_correlation: 0.7082 - val_loss: 0.1115 - val_matthews_correlation: 0.5634\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.67502\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0852 - matthews_correlation: 0.7337 - val_loss: 0.1199 - val_matthews_correlation: 0.6770\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.67502 to 0.67705, saving model to weights_647414_2.h5\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1033 - matthews_correlation: 0.6340 - val_loss: 0.1101 - val_matthews_correlation: 0.5972\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0953 - matthews_correlation: 0.6592 - val_loss: 0.1244 - val_matthews_correlation: 0.4082\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0901 - matthews_correlation: 0.6583 - val_loss: 0.1104 - val_matthews_correlation: 0.6014\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0836 - matthews_correlation: 0.7518 - val_loss: 0.1185 - val_matthews_correlation: 0.6214\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0844 - matthews_correlation: 0.6880 - val_loss: 0.1117 - val_matthews_correlation: 0.5758\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0821 - matthews_correlation: 0.7186 - val_loss: 0.1186 - val_matthews_correlation: 0.5893\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0871 - matthews_correlation: 0.7285 - val_loss: 0.1132 - val_matthews_correlation: 0.5244\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0870 - matthews_correlation: 0.6613 - val_loss: 0.1092 - val_matthews_correlation: 0.6637\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0815 - matthews_correlation: 0.7158 - val_loss: 0.1158 - val_matthews_correlation: 0.5912\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0818 - matthews_correlation: 0.7448 - val_loss: 0.1130 - val_matthews_correlation: 0.6185\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0758 - matthews_correlation: 0.7469 - val_loss: 0.1170 - val_matthews_correlation: 0.5921\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0746 - matthews_correlation: 0.7686 - val_loss: 0.1156 - val_matthews_correlation: 0.5921\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0876 - matthews_correlation: 0.7195 - val_loss: 0.1215 - val_matthews_correlation: 0.5820\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0815 - matthews_correlation: 0.7640 - val_loss: 0.1358 - val_matthews_correlation: 0.5878\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0809 - matthews_correlation: 0.7112 - val_loss: 0.1294 - val_matthews_correlation: 0.5754\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0758 - matthews_correlation: 0.7292 - val_loss: 0.1197 - val_matthews_correlation: 0.5924\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0757 - matthews_correlation: 0.7373 - val_loss: 0.1232 - val_matthews_correlation: 0.5791\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0704 - matthews_correlation: 0.7766 - val_loss: 0.1265 - val_matthews_correlation: 0.5754\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0773 - matthews_correlation: 0.7138 - val_loss: 0.1153 - val_matthews_correlation: 0.6300\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0837 - matthews_correlation: 0.6825 - val_loss: 0.1164 - val_matthews_correlation: 0.6402\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.67705\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 3s 1ms/step - loss: 0.0761 - matthews_correlation: 0.7222 - val_loss: 0.1217 - val_matthews_correlation: 0.5824\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.67705\n",
      "Beginning fold 4\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 4s 2ms/step - loss: 0.3536 - matthews_correlation: 0.0015 - val_loss: 0.2365 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_647414_3.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.2304 - matthews_correlation: 0.0000e+00 - val_loss: 0.2265 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.2184 - matthews_correlation: 0.0000e+00 - val_loss: 0.1978 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1991 - matthews_correlation: 0.1418 - val_loss: 0.1790 - val_matthews_correlation: 0.2530\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.00000 to 0.25301, saving model to weights_647414_3.h5\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1910 - matthews_correlation: 0.2037 - val_loss: 0.1714 - val_matthews_correlation: 0.2741\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.25301 to 0.27415, saving model to weights_647414_3.h5\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1858 - matthews_correlation: 0.1914 - val_loss: 0.1751 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.27415\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1624 - matthews_correlation: 0.2350 - val_loss: 0.1487 - val_matthews_correlation: 0.4351\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.27415 to 0.43513, saving model to weights_647414_3.h5\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1265 - matthews_correlation: 0.6260 - val_loss: 0.1047 - val_matthews_correlation: 0.6950\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.43513 to 0.69502, saving model to weights_647414_3.h5\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1126 - matthews_correlation: 0.6089 - val_loss: 0.1260 - val_matthews_correlation: 0.5422\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.69502\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1112 - matthews_correlation: 0.6009 - val_loss: 0.1056 - val_matthews_correlation: 0.5887\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.69502\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1062 - matthews_correlation: 0.6339 - val_loss: 0.1093 - val_matthews_correlation: 0.5377\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.69502\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1096 - matthews_correlation: 0.6195 - val_loss: 0.1111 - val_matthews_correlation: 0.7008\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.69502 to 0.70081, saving model to weights_647414_3.h5\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1016 - matthews_correlation: 0.6544 - val_loss: 0.1077 - val_matthews_correlation: 0.5460\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0934 - matthews_correlation: 0.6704 - val_loss: 0.1318 - val_matthews_correlation: 0.5009\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0991 - matthews_correlation: 0.6351 - val_loss: 0.1043 - val_matthews_correlation: 0.6348\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1025 - matthews_correlation: 0.6246 - val_loss: 0.1016 - val_matthews_correlation: 0.6289\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0964 - matthews_correlation: 0.6269 - val_loss: 0.1055 - val_matthews_correlation: 0.6669\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0981 - matthews_correlation: 0.6725 - val_loss: 0.1317 - val_matthews_correlation: 0.4666\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0924 - matthews_correlation: 0.6362 - val_loss: 0.1210 - val_matthews_correlation: 0.5135\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0986 - matthews_correlation: 0.5885 - val_loss: 0.1044 - val_matthews_correlation: 0.5014\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0928 - matthews_correlation: 0.7106 - val_loss: 0.1028 - val_matthews_correlation: 0.6278\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0926 - matthews_correlation: 0.5979 - val_loss: 0.0990 - val_matthews_correlation: 0.6478\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0877 - matthews_correlation: 0.6586 - val_loss: 0.1121 - val_matthews_correlation: 0.5126\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0978 - matthews_correlation: 0.6794 - val_loss: 0.0993 - val_matthews_correlation: 0.5953\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0914 - matthews_correlation: 0.6578 - val_loss: 0.0994 - val_matthews_correlation: 0.6471\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0904 - matthews_correlation: 0.6653 - val_loss: 0.1183 - val_matthews_correlation: 0.5463\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0926 - matthews_correlation: 0.6412 - val_loss: 0.1120 - val_matthews_correlation: 0.5014\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0891 - matthews_correlation: 0.6921 - val_loss: 0.0984 - val_matthews_correlation: 0.6003\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0895 - matthews_correlation: 0.7200 - val_loss: 0.0920 - val_matthews_correlation: 0.6501\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0866 - matthews_correlation: 0.6820 - val_loss: 0.1039 - val_matthews_correlation: 0.5788\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.70081\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0877 - matthews_correlation: 0.6950 - val_loss: 0.0949 - val_matthews_correlation: 0.7287\n",
      "\n",
      "Epoch 00031: val_matthews_correlation improved from 0.70081 to 0.72869, saving model to weights_647414_3.h5\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0926 - matthews_correlation: 0.6793 - val_loss: 0.0977 - val_matthews_correlation: 0.4636\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0847 - matthews_correlation: 0.7046 - val_loss: 0.0959 - val_matthews_correlation: 0.5832\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0828 - matthews_correlation: 0.7350 - val_loss: 0.0948 - val_matthews_correlation: 0.6437\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0887 - matthews_correlation: 0.7022 - val_loss: 0.0999 - val_matthews_correlation: 0.5872\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0861 - matthews_correlation: 0.6684 - val_loss: 0.0996 - val_matthews_correlation: 0.6101\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0865 - matthews_correlation: 0.6863 - val_loss: 0.1005 - val_matthews_correlation: 0.6052\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0814 - matthews_correlation: 0.7422 - val_loss: 0.0995 - val_matthews_correlation: 0.5563\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0805 - matthews_correlation: 0.7430 - val_loss: 0.0977 - val_matthews_correlation: 0.6471\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0846 - matthews_correlation: 0.6831 - val_loss: 0.0935 - val_matthews_correlation: 0.5518\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0856 - matthews_correlation: 0.6945 - val_loss: 0.0987 - val_matthews_correlation: 0.5572\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0881 - matthews_correlation: 0.6906 - val_loss: 0.0962 - val_matthews_correlation: 0.6418\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0831 - matthews_correlation: 0.6872 - val_loss: 0.0972 - val_matthews_correlation: 0.6003\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0791 - matthews_correlation: 0.7558 - val_loss: 0.1010 - val_matthews_correlation: 0.5116\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0793 - matthews_correlation: 0.7407 - val_loss: 0.1070 - val_matthews_correlation: 0.4512\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0792 - matthews_correlation: 0.7382 - val_loss: 0.1047 - val_matthews_correlation: 0.5577\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0817 - matthews_correlation: 0.6837 - val_loss: 0.0968 - val_matthews_correlation: 0.5183\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0849 - matthews_correlation: 0.7032 - val_loss: 0.1083 - val_matthews_correlation: 0.6202\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0899 - matthews_correlation: 0.6950 - val_loss: 0.1208 - val_matthews_correlation: 0.5094\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.72869\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0826 - matthews_correlation: 0.7238 - val_loss: 0.1011 - val_matthews_correlation: 0.4811\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.72869\n",
      "Beginning fold 5\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 4s 2ms/step - loss: 0.3763 - matthews_correlation: 0.0000e+00 - val_loss: 0.2269 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_647414_4.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.2299 - matthews_correlation: 0.0000e+00 - val_loss: 0.2176 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.2183 - matthews_correlation: 0.0000e+00 - val_loss: 0.1995 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1991 - matthews_correlation: 0.0000e+00 - val_loss: 0.2051 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.2017 - matthews_correlation: 0.0000e+00 - val_loss: 0.1781 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1828 - matthews_correlation: 0.0000e+00 - val_loss: 0.1894 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.2131 - matthews_correlation: 0.0000e+00 - val_loss: 0.1929 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.2000 - matthews_correlation: 0.1034 - val_loss: 0.1772 - val_matthews_correlation: 0.4175\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.00000 to 0.41752, saving model to weights_647414_4.h5\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1720 - matthews_correlation: 0.2087 - val_loss: 0.1857 - val_matthews_correlation: 0.4175\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.41752\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1723 - matthews_correlation: 0.1738 - val_loss: 0.1533 - val_matthews_correlation: 0.4175\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.41752\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1451 - matthews_correlation: 0.2907 - val_loss: 0.1302 - val_matthews_correlation: 0.5460\n",
      "\n",
      "Epoch 00011: val_matthews_correlation improved from 0.41752 to 0.54602, saving model to weights_647414_4.h5\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1205 - matthews_correlation: 0.5451 - val_loss: 0.1489 - val_matthews_correlation: 0.5723\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.54602 to 0.57230, saving model to weights_647414_4.h5\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1200 - matthews_correlation: 0.5646 - val_loss: 0.1206 - val_matthews_correlation: 0.6042\n",
      "\n",
      "Epoch 00013: val_matthews_correlation improved from 0.57230 to 0.60420, saving model to weights_647414_4.h5\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1097 - matthews_correlation: 0.6535 - val_loss: 0.1145 - val_matthews_correlation: 0.6248\n",
      "\n",
      "Epoch 00014: val_matthews_correlation improved from 0.60420 to 0.62477, saving model to weights_647414_4.h5\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1011 - matthews_correlation: 0.6810 - val_loss: 0.1141 - val_matthews_correlation: 0.6382\n",
      "\n",
      "Epoch 00015: val_matthews_correlation improved from 0.62477 to 0.63823, saving model to weights_647414_4.h5\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1154 - matthews_correlation: 0.6555 - val_loss: 0.1280 - val_matthews_correlation: 0.6129\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.63823\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1041 - matthews_correlation: 0.6445 - val_loss: 0.1201 - val_matthews_correlation: 0.6428\n",
      "\n",
      "Epoch 00017: val_matthews_correlation improved from 0.63823 to 0.64278, saving model to weights_647414_4.h5\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1009 - matthews_correlation: 0.6483 - val_loss: 0.1256 - val_matthews_correlation: 0.5735\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.64278\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1040 - matthews_correlation: 0.6237 - val_loss: 0.1114 - val_matthews_correlation: 0.6428\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.64278\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0950 - matthews_correlation: 0.6848 - val_loss: 0.1115 - val_matthews_correlation: 0.6662\n",
      "\n",
      "Epoch 00020: val_matthews_correlation improved from 0.64278 to 0.66625, saving model to weights_647414_4.h5\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0931 - matthews_correlation: 0.6492 - val_loss: 0.1143 - val_matthews_correlation: 0.6227\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0976 - matthews_correlation: 0.6713 - val_loss: 0.1100 - val_matthews_correlation: 0.6428\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0926 - matthews_correlation: 0.6949 - val_loss: 0.1216 - val_matthews_correlation: 0.3969\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0941 - matthews_correlation: 0.6621 - val_loss: 0.1120 - val_matthews_correlation: 0.5872\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0922 - matthews_correlation: 0.7171 - val_loss: 0.1100 - val_matthews_correlation: 0.6201\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0937 - matthews_correlation: 0.6566 - val_loss: 0.1143 - val_matthews_correlation: 0.6428\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0925 - matthews_correlation: 0.6580 - val_loss: 0.1128 - val_matthews_correlation: 0.4452\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0925 - matthews_correlation: 0.6357 - val_loss: 0.1206 - val_matthews_correlation: 0.5759\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.66625\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0949 - matthews_correlation: 0.6674 - val_loss: 0.1060 - val_matthews_correlation: 0.6769\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.66625 to 0.67688, saving model to weights_647414_4.h5\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0999 - matthews_correlation: 0.6897 - val_loss: 0.1107 - val_matthews_correlation: 0.5979\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0944 - matthews_correlation: 0.6736 - val_loss: 0.1226 - val_matthews_correlation: 0.5592\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0951 - matthews_correlation: 0.6866 - val_loss: 0.1059 - val_matthews_correlation: 0.6088\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0956 - matthews_correlation: 0.6637 - val_loss: 0.1101 - val_matthews_correlation: 0.4359\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0896 - matthews_correlation: 0.6534 - val_loss: 0.1060 - val_matthews_correlation: 0.6424\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0880 - matthews_correlation: 0.6999 - val_loss: 0.1140 - val_matthews_correlation: 0.4205\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0910 - matthews_correlation: 0.6294 - val_loss: 0.1134 - val_matthews_correlation: 0.5617\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0864 - matthews_correlation: 0.7228 - val_loss: 0.1151 - val_matthews_correlation: 0.5601\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0856 - matthews_correlation: 0.7146 - val_loss: 0.1120 - val_matthews_correlation: 0.5502\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0827 - matthews_correlation: 0.7333 - val_loss: 0.1113 - val_matthews_correlation: 0.5593\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0819 - matthews_correlation: 0.6808 - val_loss: 0.1167 - val_matthews_correlation: 0.4594\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0845 - matthews_correlation: 0.7090 - val_loss: 0.1029 - val_matthews_correlation: 0.6281\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0844 - matthews_correlation: 0.7413 - val_loss: 0.1053 - val_matthews_correlation: 0.6310\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0792 - matthews_correlation: 0.7494 - val_loss: 0.1130 - val_matthews_correlation: 0.4553\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0879 - matthews_correlation: 0.7035 - val_loss: 0.1181 - val_matthews_correlation: 0.6042\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0883 - matthews_correlation: 0.6779 - val_loss: 0.1202 - val_matthews_correlation: 0.4384\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0851 - matthews_correlation: 0.7295 - val_loss: 0.1052 - val_matthews_correlation: 0.6372\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0836 - matthews_correlation: 0.7000 - val_loss: 0.1079 - val_matthews_correlation: 0.6063\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0795 - matthews_correlation: 0.7255 - val_loss: 0.1076 - val_matthews_correlation: 0.6206\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0800 - matthews_correlation: 0.7380 - val_loss: 0.1105 - val_matthews_correlation: 0.4534\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 3s 1ms/step - loss: 0.0844 - matthews_correlation: 0.7068 - val_loss: 0.1061 - val_matthews_correlation: 0.6427\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.67688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647414\n",
      "(2904,) (2904,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'base_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6ab1cc9ab6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mbest_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'threshold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mseed_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ed602fb7947a>\u001b[0m in \u001b[0;36mthreshold_search\u001b[0;34m(y_true, y_proba)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatthews_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_proba\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"found better score:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\", th=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a35a1e4a083b>\u001b[0m in \u001b[0;36mmatthews_correlation\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mof\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0mproblems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     '''\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0my_pred_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my_pred_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(x, min_value, max_value)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m     \u001b[0mmin_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m     \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'base_dtype'"
     ]
    }
   ],
   "source": [
    "# Here is where the training happens\n",
    "\n",
    "seed_trains = []\n",
    "\n",
    "for seed in seeds:\n",
    "    #not using seed right now since tensorflow/keras combo impossible to make deterministic\n",
    "    # First, create a set of indexes of the 5 folds\n",
    "    splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "    preds_val = []\n",
    "    y_val = []\n",
    "    # Then, iteract with each fold\n",
    "    # If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\n",
    "    for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "        K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "        print(\"Beginning fold {}\".format(idx+1))\n",
    "        # use the indexes to extract the folds in the train and validation data\n",
    "        train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "        # instantiate the model for this fold\n",
    "        model = model_lstm(train_X.shape)\n",
    "        # This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an\n",
    "        # validation matthews_correlation greater than the last one.\n",
    "        ckpt = ModelCheckpoint('weights_{}_{}.h5'.format(seed, idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n",
    "        # Train, train, train\n",
    "        model.fit(train_X, train_y, batch_size=128, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt])\n",
    "        # loads the best weights saved by the checkpoint\n",
    "        model.load_weights('weights_{}_{}.h5'.format(seed, idx))\n",
    "        # Add the predictions of the validation to the list preds_val\n",
    "        preds_val.append(model.predict(val_X, batch_size=512))\n",
    "        # and the val true y\n",
    "        y_val.append(val_y)\n",
    "\n",
    "    # concatenates all and prints the shape    \n",
    "    preds_val = np.concatenate(preds_val)[...,0]\n",
    "    y_val = np.concatenate(y_val)\n",
    "    print(seed)\n",
    "    print(preds_val.shape, y_val.shape)\n",
    "    best_threshold = threshold_search(y_val, preds_val)['threshold']\n",
    "    seed_trains.append(best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "d28151fd0be9fd9762f3f55e307d82f89bfbd291"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "6fee7f722ed08bc1453a822a4371ed2d48e08abc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "ae9bd3fa9d8c0781c0708846bb7f2a9f9e6cbd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 11.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now load the test data\n",
    "# This first part is the meta data, not the main data, the measurements\n",
    "meta_test = pd.read_csv('../input/metadata_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "3eb186d032f79c99ffba05dd1a7fabb77e13cec5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>2904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>2904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8715</th>\n",
       "      <td>2905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>2905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_measurement  phase\n",
       "signal_id                       \n",
       "8712                 2904      0\n",
       "8713                 2904      1\n",
       "8714                 2904      2\n",
       "8715                 2905      0\n",
       "8716                 2905      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test = meta_test.set_index(['signal_id'])\n",
    "meta_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "9058a4a1da900b4801ad94984b771474f68dc1bc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'X_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a2cf86b8b1cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_test.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_test.npy'"
     ]
    }
   ],
   "source": [
    "X_test_input = np.load(\"X_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "6f8e94387f625bff0a9a6289e1ee038908bc5856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712 10 20337 2033 7 20337\n",
      "[[8712, 10745], [10745, 12778], [12778, 14811], [14811, 16844], [16844, 18877], [18877, 20910], [20910, 22943], [22943, 24976], [24976, 27009], [27009, 29042], [29042, 29049]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1716/2033 [05:56<01:06,  4.80it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# First we daclarete a series of parameters to initiate the loading of the main data\n",
    "# it is too large, it is impossible to load in one time, so we are doing it in dividing in 10 parts\n",
    "first_sig = meta_test.index[0]\n",
    "n_parts = 10\n",
    "max_line = len(meta_test)\n",
    "part_size = int(max_line / n_parts)\n",
    "last_part = max_line % n_parts\n",
    "print(first_sig, n_parts, max_line, part_size, last_part, n_parts * part_size + last_part)\n",
    "# Here we create a list of lists with start index and end index for each of the 10 parts and one for the last partial part\n",
    "start_end = [[x, x+part_size] for x in range(first_sig, max_line + first_sig, part_size)]\n",
    "start_end = start_end[:-1] + [[start_end[-1][0], start_end[-1][0] + last_part]]\n",
    "print(start_end)\n",
    "X_test = []\n",
    "# now, very like we did above with the train data, we convert the test data part by part\n",
    "# transforming the 3 phases 800000 measurement in matrix (160,57)\n",
    "for start, end in start_end:\n",
    "    subset_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    for i in tqdm(subset_test.columns):\n",
    "        id_measurement, phase = meta_test.loc[int(i)]\n",
    "        subset_test_col = subset_test[i]\n",
    "        subset_trans = transform_ts(subset_test_col)\n",
    "        X_test.append([i, id_measurement, phase, subset_trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "af9aa6b2b8f8a2beda1a02ff998e3072fcad8d06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6779, 160, 60)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_input = np.asarray([np.concatenate([X_test[i][3],X_test[i+1][3], X_test[i+2][3]], axis=1) for i in range(0,len(X_test), 3)])\n",
    "np.save(\"X_test.npy\",X_test_input)\n",
    "X_test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "cfd265d3e07c4cc1679d2c4d55fe7de631c813e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20337\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "print(len(submission))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "2f7342296138f6bfd3e9cedd029e1035de3b98fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6779/6779 [==============================] - 2s 290us/step\n",
      "6779/6779 [==============================] - 2s 272us/step\n",
      "6779/6779 [==============================] - 2s 269us/step\n",
      "6779/6779 [==============================] - 2s 269us/step\n",
      "6779/6779 [==============================] - 2s 271us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9f00225280e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mpred_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_scalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpreds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpreds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mpreds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mseed_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "seed_preds = []\n",
    "for seed in seeds:\n",
    "    preds_test = []\n",
    "    for i in range(N_SPLITS):\n",
    "        model.load_weights('weights_{}_{}.h5'.format(seed, i))\n",
    "        pred = model.predict(X_test_input, batch_size=300, verbose=1)\n",
    "        pred_3 = []\n",
    "        for pred_scalar in pred:\n",
    "            for i in range(3):\n",
    "                pred_3.append(pred_scalar)\n",
    "        preds_test.append(pred_3)\n",
    "    preds_test = (np.squeeze(np.mean(preds_test, axis=0)) > best_threshold).astype(np.int)\n",
    "    preds_test.shape\n",
    "    seed_preds.append(preds_test)\n",
    "    submission['target'] = preds_test\n",
    "    submission.to_csv('submission_{}.csv'.format(seed), index=False)\n",
    "    submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "5fb97a3276d143f82543d7c18d9db3d3cd2d6cd2"
   },
   "outputs": [],
   "source": [
    "for row in seed_preds:\n",
    "    print(sum(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "7a322ab533c83e806102963df1f6ddadacf3d559"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-847fac4cf853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseed_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "seed_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "9f76c471eaf983707d446c5081ab3d50c4e40ea5"
   },
   "outputs": [],
   "source": [
    "#preds_test = (np.squeeze(np.mean(preds_test, axis=0)) > best_threshold).astype(np.int)\n",
    "#preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "b55a630f0be93b8c730cab07a1520dfd990fdd7e"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-391123af1cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#convert list of predictions into set of columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#take most common value (0 or 1) or each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "# Ensemble with voting\n",
    "labels = np.array(seed_preds)\n",
    "#convert list of predictions into set of columns\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "#take most common value (0 or 1) or each row\n",
    "labels = scipy.stats.mode(labels, axis=-1)[0]\n",
    "labels = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "d1b93ac1fcf2281e586e2bbcf55d4b3abe380e51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "7c99773998433bc2d5e66035d4c2c6315f255045"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "5c215c5a184078b3cb134393f5f97e2a9ab4b2a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "b35723f85d494b4b6ec630dd7c79135a110a4062"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-371f070a0db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4001\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "submission['target'] = labels\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "d7600d0093a9880003240ef9ce0a1f1303e4d982"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
